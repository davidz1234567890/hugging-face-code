2
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.05it/s]
here is input text: What is 1+1=?
Tokenized Input: ['<|begin_of_text|>', 'What', 'Ġis', 'Ġ', '1', '+', '1', '=?']
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
Hidden States Shape (Last Layer): torch.Size([1, 8, 4096])
here is hidden_states[0]: tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,
           6.3419e-05,  1.1902e-03],
         [ 2.0752e-02, -1.2894e-03,  2.8229e-03,  ...,  2.1973e-02,
           3.1128e-03,  1.0681e-02],
         [-2.6093e-03,  7.7057e-04,  2.6131e-04,  ...,  1.1902e-02,
           4.6387e-03,  9.1553e-03],
         ...,
         [ 1.1230e-02, -7.5989e-03,  3.0518e-03,  ...,  1.4282e-02,
           1.0010e-02,  9.6436e-03],
         [ 3.8605e-03,  3.4485e-03,  2.6855e-03,  ...,  9.2773e-03,
           3.1738e-03, -2.7618e-03],
         [-4.9744e-03, -2.9449e-03,  1.3184e-02,  ...,  7.3242e-03,
           1.5182e-03, -9.8877e-03]]], dtype=torch.bfloat16,
       grad_fn=<EmbeddingBackward0>)

here is hidden_states[1]: tensor([[[ 0.0022,  0.0041, -0.0007,  ...,  0.0190, -0.0042, -0.0025],
         [ 0.0493,  0.0287,  0.0214,  ...,  0.0098,  0.0114,  0.0237],
         [ 0.0171,  0.0104,  0.0003,  ...,  0.0181, -0.0066,  0.0101],
         ...,
         [ 0.0138,  0.0042,  0.0156,  ..., -0.0074,  0.0166,  0.0115],
         [ 0.0058, -0.0222, -0.0038,  ..., -0.0080,  0.0109,  0.0004],
         [-0.0030,  0.0050,  0.0155,  ...,  0.0054, -0.0064, -0.0135]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[2]: tensor([[[-0.1040,  0.0718, -0.0723,  ...,  0.8398,  0.1826,  0.1167],
         [ 0.0569,  0.0366,  0.0175,  ..., -0.0042,  0.0090,  0.0352],
         [ 0.0364,  0.0144, -0.0022,  ..., -0.0194, -0.0464,  0.0190],
         ...,
         [ 0.0256, -0.0184,  0.0288,  ..., -0.0503,  0.0361,  0.0344],
         [ 0.0243, -0.0552, -0.0222,  ..., -0.0520,  0.0073,  0.0325],
         [ 0.0178, -0.0266,  0.0170,  ..., -0.0094, -0.0105, -0.0069]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[3]: tensor([[[-0.1108,  0.0767, -0.0669,  ...,  0.8555,  0.1963,  0.1123],
         [ 0.0525,  0.0295,  0.0366,  ..., -0.0048,  0.0092,  0.0503],
         [ 0.0237,  0.0366, -0.0247,  ..., -0.0206, -0.0557,  0.0359],
         ...,
         [ 0.0020, -0.0228,  0.0182,  ...,  0.0037,  0.0378,  0.0039],
         [ 0.0276, -0.0483, -0.0229,  ..., -0.0581,  0.0138, -0.0046],
         [ 0.0016, -0.0356,  0.0513,  ...,  0.0393, -0.0325, -0.0229]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[4]: tensor([[[-9.0332e-02,  8.9844e-02, -4.7119e-02,  ...,  9.0625e-01,
           1.9531e-01,  1.1328e-01],
         [ 6.9336e-02,  7.4219e-02,  3.5400e-02,  ..., -4.6387e-03,
           1.2207e-04,  3.9551e-02],
         [ 7.5684e-02,  3.1494e-02, -1.1963e-02,  ..., -3.1982e-02,
          -1.3672e-02,  6.4453e-02],
         ...,
         [-2.7588e-02,  2.5635e-03,  0.0000e+00,  ...,  6.7871e-02,
           4.4678e-02, -6.1279e-02],
         [-2.9297e-03, -8.3496e-02, -2.8931e-02,  ..., -1.4648e-03,
           7.8125e-03, -1.5015e-02],
         [-5.8105e-02, -7.5684e-02,  2.5635e-02,  ...,  6.3477e-02,
          -2.4048e-02, -6.1768e-02]]], dtype=torch.bfloat16,
       grad_fn=<AddBackward0>)

here is hidden_states[5]: tensor([[[-1.1377e-01,  8.9844e-02, -4.5654e-02,  ...,  8.8281e-01,
           2.2949e-01,  1.2354e-01],
         [ 8.2520e-02,  1.1133e-01,  2.3560e-02,  ...,  2.4048e-02,
           1.7090e-02,  7.0801e-02],
         [ 7.9102e-02,  1.4355e-01, -3.2471e-02,  ..., -1.0071e-02,
          -9.3994e-03,  1.1816e-01],
         ...,
         [ 4.8828e-04,  1.6724e-02, -3.3569e-03,  ...,  1.0059e-01,
          -1.0742e-02, -5.1514e-02],
         [ 7.2021e-03, -9.6680e-02, -1.1719e-02,  ...,  2.5513e-02,
           5.4321e-03,  8.5449e-03],
         [-6.6406e-02, -1.6113e-02,  7.9346e-03,  ...,  6.9824e-02,
          -4.0283e-02, -5.3955e-02]]], dtype=torch.bfloat16,
       grad_fn=<AddBackward0>)

here is hidden_states[6]: tensor([[[-1.0693e-01,  9.6191e-02, -3.1250e-02,  ...,  8.7500e-01,
           2.4609e-01,  1.3086e-01],
         [ 1.5234e-01,  9.3750e-02,  2.7344e-02,  ..., -2.1729e-02,
          -4.5166e-03,  1.2891e-01],
         [ 1.4746e-01,  2.0508e-01, -6.5430e-02,  ...,  9.0332e-03,
           7.2754e-02,  1.2207e-01],
         ...,
         [ 1.0352e-01,  5.6396e-02,  6.2500e-02,  ...,  6.0059e-02,
           8.1055e-02,  1.4648e-03],
         [ 4.0039e-02, -1.0254e-02,  6.9580e-03,  ...,  1.6602e-02,
           5.1025e-02,  1.7334e-02],
         [ 3.9062e-02,  4.5410e-02, -2.3499e-03,  ..., -3.1982e-02,
           6.3477e-02,  2.4414e-04]]], dtype=torch.bfloat16,
       grad_fn=<AddBackward0>)

here is hidden_states[7]: tensor([[[-0.1133,  0.1157, -0.0223,  ...,  0.8320,  0.2559,  0.1270],
         [ 0.1475,  0.1177, -0.0214,  ..., -0.0500, -0.0034,  0.1660],
         [ 0.1270,  0.0894, -0.0972,  ..., -0.0068, -0.0283,  0.0613],
         ...,
         [ 0.1309,  0.0361,  0.0781,  ...,  0.0845,  0.0815,  0.0186],
         [ 0.0942,  0.0610,  0.0684,  ..., -0.0303,  0.1357, -0.0115],
         [ 0.0242,  0.0688,  0.0284,  ..., -0.0039,  0.0938, -0.0299]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[8]: tensor([[[-0.0981,  0.1729, -0.0469,  ...,  0.7695,  0.2490,  0.1543],
         [ 0.2109,  0.1055, -0.0752,  ..., -0.1396, -0.1006,  0.1338],
         [ 0.1865,  0.1035, -0.0977,  ..., -0.0569, -0.0366,  0.0581],
         ...,
         [ 0.1738,  0.0275,  0.0601,  ...,  0.0864,  0.0752,  0.0293],
         [ 0.1758,  0.0835,  0.0908,  ..., -0.0089,  0.1494,  0.0129],
         [ 0.0354,  0.0298,  0.0303,  ...,  0.0752,  0.1748,  0.0576]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[9]: tensor([[[-0.1064,  0.2051, -0.0322,  ...,  0.7031,  0.2578,  0.1592],
         [ 0.1992,  0.1592, -0.0869,  ..., -0.1455, -0.1152,  0.1387],
         [ 0.1992,  0.1221, -0.0195,  ..., -0.0481, -0.0649, -0.0664],
         ...,
         [ 0.0757, -0.0205,  0.1143,  ...,  0.1660,  0.1699,  0.0334],
         [ 0.1289,  0.1357,  0.0874,  ...,  0.0703,  0.1533, -0.0109],
         [ 0.0737, -0.0009,  0.1602,  ...,  0.0859,  0.1533, -0.0236]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[10]: tensor([[[-0.0938,  0.2295, -0.0610,  ...,  0.6523,  0.2773,  0.1895],
         [ 0.1553,  0.1226, -0.0571,  ..., -0.1348, -0.0469,  0.1768],
         [ 0.1748,  0.0483, -0.0187,  ..., -0.0547, -0.0400, -0.0603],
         ...,
         [ 0.1177,  0.0547,  0.0225,  ...,  0.1641,  0.2295,  0.0422],
         [ 0.1079,  0.1738,  0.0835,  ...,  0.0400,  0.1338, -0.0598],
         [ 0.0549,  0.0449,  0.1514,  ...,  0.0967,  0.2217,  0.0410]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[11]: tensor([[[-0.0286,  0.3125, -0.0479,  ...,  0.5312,  0.3086,  0.2070],
         [ 0.1738,  0.0449, -0.0189,  ..., -0.1484, -0.0405,  0.2070],
         [ 0.1963,  0.0066,  0.0366,  ..., -0.0723, -0.0542,  0.0352],
         ...,
         [ 0.1924, -0.0244,  0.0144,  ...,  0.1670,  0.2441,  0.0144],
         [ 0.1953,  0.1367,  0.0361,  ..., -0.0093,  0.1025, -0.0630],
         [ 0.0991,  0.0378,  0.1182,  ...,  0.0767,  0.1738, -0.0264]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[12]: tensor([[[-0.0391,  0.3398, -0.0894,  ...,  0.5078,  0.3203,  0.2061],
         [ 0.1895,  0.0581,  0.0245,  ..., -0.1787, -0.0248,  0.1738],
         [ 0.1104,  0.0364, -0.0254,  ..., -0.0791, -0.0164, -0.1064],
         ...,
         [ 0.1719,  0.0393,  0.0272,  ...,  0.0967,  0.3184,  0.0083],
         [ 0.1133,  0.1797, -0.0093,  ..., -0.0237,  0.1211, -0.0684],
         [ 0.0718,  0.0378,  0.1123,  ...,  0.1108,  0.2471,  0.0205]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[13]: tensor([[[-0.0265,  0.3594, -0.0737,  ...,  0.4375,  0.3047,  0.1963],
         [ 0.1807,  0.0117,  0.0879,  ..., -0.2207, -0.0415,  0.2930],
         [ 0.1484, -0.0266, -0.0094,  ..., -0.1475, -0.0354, -0.0718],
         ...,
         [ 0.1934, -0.0889,  0.0564,  ...,  0.0986,  0.1953,  0.0376],
         [ 0.0488,  0.0566,  0.0742,  ..., -0.0496,  0.1279, -0.0820],
         [ 0.1426, -0.0859,  0.1475,  ...,  0.1260,  0.1758,  0.0176]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[14]: tensor([[[-0.0300,  0.3398, -0.1167,  ...,  0.3652,  0.2930,  0.2051],
         [ 0.0776, -0.0228,  0.0425,  ..., -0.2246, -0.0106,  0.2949],
         [ 0.0425, -0.0444,  0.0024,  ..., -0.1523,  0.0742, -0.0913],
         ...,
         [ 0.2773,  0.0630,  0.0396,  ...,  0.1934,  0.1426, -0.0017],
         [ 0.2295,  0.1475,  0.1641,  ...,  0.0439,  0.0928, -0.1504],
         [ 0.2656, -0.1157,  0.1992,  ...,  0.1729,  0.1875,  0.0544]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[15]: tensor([[[-0.0569,  0.2988, -0.1055,  ...,  0.3672,  0.3320,  0.1221],
         [ 0.0464, -0.0566,  0.0752,  ..., -0.2412,  0.0391,  0.2656],
         [ 0.0322, -0.1240,  0.0537,  ..., -0.2324,  0.0928, -0.0908],
         ...,
         [ 0.2969, -0.0413, -0.0552,  ...,  0.1670,  0.1982, -0.0137],
         [ 0.2109,  0.0608,  0.0459,  ...,  0.0170,  0.0513, -0.0981],
         [ 0.3320, -0.0996,  0.1602,  ...,  0.0952,  0.1543,  0.0227]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[16]: tensor([[[-0.1104,  0.1650, -0.0396,  ...,  0.2871,  0.3711,  0.0840],
         [-0.0591, -0.0332,  0.0508,  ..., -0.2812,  0.1172,  0.3242],
         [ 0.0588, -0.1631, -0.1543,  ..., -0.2930,  0.1079,  0.0151],
         ...,
         [ 0.2266, -0.0131, -0.0131,  ...,  0.1543,  0.1943, -0.0542],
         [ 0.2773,  0.1230, -0.0132,  ...,  0.0045,  0.0977, -0.0483],
         [ 0.2812, -0.1147,  0.2539,  ...,  0.1523,  0.0430,  0.0493]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[17]: tensor([[[-0.1230,  0.1641, -0.0303,  ...,  0.2734,  0.3926,  0.0688],
         [ 0.0776, -0.1455,  0.0688,  ..., -0.2012,  0.0186,  0.4043],
         [ 0.0129, -0.1865, -0.0908,  ..., -0.3281,  0.0933, -0.0232],
         ...,
         [ 0.1758,  0.0605, -0.0166,  ...,  0.2188,  0.1426, -0.0175],
         [ 0.2949,  0.1572,  0.1680,  ...,  0.0396,  0.0403, -0.0635],
         [ 0.3301, -0.0483,  0.3320,  ...,  0.1738,  0.0178,  0.0522]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[18]: tensor([[[-0.1543,  0.1250, -0.0259,  ...,  0.2637,  0.4863,  0.0469],
         [-0.0115, -0.1445,  0.1777,  ..., -0.2031, -0.0259,  0.4414],
         [ 0.0386, -0.1699, -0.0024,  ..., -0.3320,  0.1631, -0.1245],
         ...,
         [ 0.2412,  0.0518, -0.0149,  ...,  0.3379,  0.1187,  0.0493],
         [ 0.2637,  0.2168,  0.1309,  ...,  0.0630,  0.0146, -0.1895],
         [ 0.2520, -0.0110,  0.3066,  ...,  0.2305, -0.0552,  0.0708]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[19]: tensor([[[-0.1729,  0.1196, -0.0119,  ...,  0.2773,  0.4238,  0.0206],
         [-0.0527, -0.2773,  0.1201,  ..., -0.2207,  0.1006,  0.5078],
         [ 0.0703, -0.2344, -0.2773,  ..., -0.2188,  0.2891, -0.1035],
         ...,
         [ 0.2285,  0.0703,  0.1367,  ...,  0.3965,  0.1543,  0.1445],
         [ 0.2285,  0.2363,  0.1060,  ...,  0.0996,  0.0234, -0.0649],
         [ 0.2168,  0.0186,  0.2207,  ...,  0.2832,  0.0425,  0.1191]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[20]: tensor([[[-0.1807,  0.1250,  0.0145,  ...,  0.2832,  0.4199,  0.0723],
         [ 0.0059, -0.3984,  0.2246,  ..., -0.3809,  0.1602,  0.5625],
         [ 0.2207, -0.2344, -0.3086,  ..., -0.2832,  0.1523, -0.1108],
         ...,
         [ 0.3125,  0.0967,  0.2598,  ...,  0.5625,  0.1807,  0.2109],
         [ 0.2734,  0.0840,  0.1543,  ...,  0.0278, -0.0057, -0.1748],
         [ 0.2617,  0.0060,  0.3828,  ...,  0.2988,  0.0869,  0.0708]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[21]: tensor([[[-0.1738,  0.1030,  0.0120,  ...,  0.2695,  0.3711,  0.0850],
         [-0.1309, -0.4883,  0.2637,  ..., -0.4102,  0.2441,  0.6211],
         [ 0.0830, -0.4785, -0.4141,  ..., -0.2852,  0.2480, -0.0947],
         ...,
         [ 0.2578,  0.0610,  0.0850,  ...,  0.6289,  0.2119,  0.4336],
         [ 0.1895,  0.0410,  0.1318,  ...,  0.0928,  0.0547, -0.1299],
         [ 0.3047,  0.0022,  0.2656,  ...,  0.3066,  0.0688,  0.0854]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[22]: tensor([[[-0.1582,  0.0889,  0.0371,  ...,  0.2969,  0.3711,  0.0962],
         [-0.1699, -0.5234,  0.3125,  ..., -0.6562,  0.1279,  0.7109],
         [-0.1973, -0.4688, -0.4707,  ..., -0.2988,  0.1211, -0.2441],
         ...,
         [ 0.0586,  0.1465,  0.1011,  ...,  0.6719,  0.1729,  0.3945],
         [ 0.0645,  0.0214,  0.1621,  ...,  0.0425, -0.0178, -0.0684],
         [ 0.2012, -0.0242,  0.1787,  ...,  0.2500,  0.0176,  0.1152]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[23]: tensor([[[-0.1279,  0.0742,  0.0332,  ...,  0.3105,  0.3770,  0.0923],
         [-0.3438, -0.5938,  0.2930,  ..., -0.6367,  0.1689,  0.7734],
         [-0.1943, -0.3867, -0.5117,  ..., -0.4297,  0.1123, -0.2969],
         ...,
         [ 0.1660,  0.1719,  0.1084,  ...,  0.6289,  0.1621,  0.3828],
         [ 0.0605, -0.0096,  0.2344,  ..., -0.0303, -0.0284, -0.1016],
         [ 0.1309,  0.0996,  0.3594,  ...,  0.3164,  0.0232,  0.1191]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[24]: tensor([[[-0.1348,  0.0591, -0.0088,  ...,  0.2930,  0.3340,  0.0723],
         [-0.4141, -0.7383,  0.3027,  ..., -0.6680,  0.0659,  0.5781],
         [-0.1235, -0.4531, -0.3633,  ..., -0.4961,  0.0581, -0.3184],
         ...,
         [ 0.1328,  0.1689,  0.0249,  ...,  0.5117,  0.1328,  0.4258],
         [-0.0212,  0.0238,  0.2363,  ..., -0.0933, -0.0400, -0.0088],
         [ 0.0469,  0.0869,  0.3320,  ...,  0.2500, -0.0586,  0.2197]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[25]: tensor([[[-0.1235,  0.0508, -0.0026,  ...,  0.2871,  0.3242,  0.0728],
         [-0.4609, -0.5078,  0.3184,  ..., -0.6680,  0.1240,  0.5820],
         [-0.1797, -0.5430, -0.4375,  ..., -0.4863,  0.0718, -0.3789],
         ...,
         [ 0.0928,  0.1299, -0.0747,  ...,  0.4707,  0.2129,  0.5117],
         [ 0.0435,  0.1260,  0.2441,  ..., -0.0869, -0.1182,  0.0071],
         [ 0.0640,  0.2480,  0.2988,  ...,  0.3281, -0.0598,  0.1621]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[26]: tensor([[[-0.1270,  0.0280, -0.0356,  ...,  0.3008,  0.3086,  0.0767],
         [-0.5312, -0.5586,  0.3086,  ..., -0.6797,  0.3262,  0.6914],
         [-0.2832, -0.6680, -0.5195,  ..., -0.4785,  0.1992, -0.3359],
         ...,
         [ 0.0840,  0.0654, -0.0762,  ...,  0.4004,  0.1250,  0.6250],
         [-0.1387,  0.0603,  0.1436,  ..., -0.0262, -0.1875, -0.1108],
         [ 0.0947,  0.2129,  0.3789,  ...,  0.4199, -0.0762,  0.2383]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[27]: tensor([[[-0.1113,  0.0415, -0.0674,  ...,  0.2754,  0.3477,  0.0596],
         [-0.5547, -0.4590,  0.1279,  ..., -0.6875,  0.3223,  0.7539],
         [-0.2656, -0.7695, -0.5664,  ..., -0.4434,  0.3711, -0.3887],
         ...,
         [ 0.0786,  0.1465, -0.1758,  ...,  0.3301,  0.1924,  0.7734],
         [-0.3867,  0.0457,  0.2109,  ...,  0.0640, -0.2031, -0.1465],
         [-0.1064,  0.1699,  0.3633,  ...,  0.4629, -0.0413,  0.1475]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[28]: tensor([[[-0.1050,  0.0532, -0.0096,  ...,  0.3066,  0.3887,  0.0898],
         [-0.7734, -0.5859,  0.0625,  ..., -0.6953,  0.1689,  0.9844],
         [-0.2871, -0.8398, -0.5508,  ..., -0.4746,  0.3008, -0.5078],
         ...,
         [ 0.0190,  0.0933, -0.1230,  ...,  0.3066,  0.1992,  0.8203],
         [-0.5078,  0.0104,  0.3828,  ..., -0.0107, -0.2676, -0.2266],
         [-0.0791,  0.0332,  0.5312,  ...,  0.5625, -0.0898,  0.0806]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[29]: tensor([[[-0.0674,  0.1104, -0.0156,  ...,  0.3164,  0.3828,  0.1367],
         [-0.5898, -0.6289,  0.0669,  ..., -0.6094,  0.1328,  1.1562],
         [-0.5078, -0.7227, -0.5430,  ..., -0.7188,  0.2266, -0.2500],
         ...,
         [-0.1099,  0.1196, -0.0259,  ...,  0.2422,  0.0410,  0.8672],
         [-0.6562, -0.0160,  0.3789,  ..., -0.0928, -0.1826, -0.2402],
         [-0.1543, -0.1982,  0.5273,  ...,  0.6055,  0.1836,  0.0830]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[30]: tensor([[[ 5.2490e-02,  1.6992e-01,  9.8145e-02,  ...,  2.4609e-01,
           2.3438e-01,  1.0840e-01],
         [-9.4531e-01, -3.5742e-01, -1.0449e-01,  ..., -5.4297e-01,
          -5.3223e-02,  1.0469e+00],
         [-6.4062e-01, -7.9688e-01, -6.9531e-01,  ..., -7.3438e-01,
           9.7656e-04, -1.2500e-01],
         ...,
         [ 2.2070e-01,  8.9844e-02, -1.3281e-01,  ...,  4.6875e-01,
          -1.0498e-01,  1.0156e+00],
         [-2.5781e-01, -2.6367e-01,  3.5547e-01,  ..., -1.6895e-01,
          -1.3672e-01, -3.6719e-01],
         [-1.5625e-01, -5.2344e-01,  5.5859e-01,  ...,  7.8516e-01,
           2.8320e-01, -3.5156e-02]]], dtype=torch.bfloat16,
       grad_fn=<AddBackward0>)

here is hidden_states[31]: tensor([[[ 0.1582,  0.4141,  0.1699,  ...,  0.2871, -0.0967,  0.2793],
         [-0.7852, -0.8125,  0.0908,  ..., -0.2852, -0.0986,  0.8281],
         [-0.6602, -0.8047, -0.8867,  ..., -1.4844, -0.0425, -0.5352],
         ...,
         [ 0.2207, -0.2539, -0.1650,  ...,  0.1523,  0.0430,  0.6133],
         [-0.4453, -0.4922,  0.5156,  ..., -0.0586,  0.0107, -1.0156],
         [-0.1865, -0.6953,  0.9219,  ...,  0.8984,  0.6172, -0.9141]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[32]: tensor([[[ 1.3203,  2.7656,  1.6484,  ..., -1.9531,  2.7656,  1.8906],
         [-0.0486, -3.2188,  0.8750,  ..., -0.8789,  0.5391,  1.8672],
         [ 0.2969,  0.2139, -3.7344,  ..., -2.7969, -0.1729,  1.3672],
         ...,
         [ 1.2344,  0.2256,  0.6953,  ...,  0.3887,  2.5625,  1.0547],
         [-0.7578, -0.3105,  3.5781,  ..., -1.1562, -0.2637, -1.9141],
         [-0.3477, -1.6562,  3.4844,  ...,  3.0312,  3.0312, -1.6641]]],
       dtype=torch.bfloat16, grad_fn=<MulBackward0>)

Attention Shape (First Layer): torch.Size([1, 32, 8, 8])
here is attentions[0]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 3.9551e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.4375e-01, 1.4258e-01, 1.3672e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [5.1953e-01, 9.5703e-02, 1.2695e-01,  ..., 2.6611e-02,
           0.0000e+00, 0.0000e+00],
          [5.1953e-01, 9.0332e-02, 8.7891e-02,  ..., 5.3223e-02,
           1.3855e-02, 0.0000e+00],
          [3.5547e-01, 6.3965e-02, 5.6152e-02,  ..., 1.6309e-01,
           3.0151e-02, 1.7871e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.4048e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [6.2500e-02, 8.9062e-01, 4.7119e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [5.7422e-01, 3.7231e-03, 1.4832e-02,  ..., 1.5918e-01,
           0.0000e+00, 0.0000e+00],
          [7.2327e-03, 1.4454e-06, 1.2040e-05,  ..., 9.7656e-01,
           1.3062e-02, 0.0000e+00],
          [9.2188e-01, 3.2806e-03, 2.4567e-03,  ..., 6.2256e-03,
           8.6060e-03, 4.3213e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4141e-01, 5.8350e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.4551e-01, 6.7188e-01, 1.8164e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [3.9551e-02, 1.3638e-04, 5.0659e-03,  ..., 1.2598e-01,
           0.0000e+00, 0.0000e+00],
          [2.4414e-02, 3.1281e-03, 1.0681e-02,  ..., 2.7930e-01,
           8.5449e-02, 0.0000e+00],
          [2.6367e-02, 5.0545e-05, 3.2663e-05,  ..., 9.2285e-02,
           3.6523e-01, 5.1562e-01]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 6.8188e-05, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.2398e-04, 3.3617e-05,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [1.0000e+00, 3.0547e-07, 2.8729e-05,  ..., 2.8908e-06,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 1.0550e-05, 3.2997e-04,  ..., 3.1281e-04,
           4.2200e-05, 0.0000e+00],
          [9.8438e-01, 4.3511e-06, 9.4414e-05,  ..., 7.4387e-05,
           3.1233e-05, 5.0068e-06]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.3359e-01, 6.5430e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.8281e-01, 5.7068e-03, 1.1230e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.2188e-01, 3.2043e-04, 1.3504e-03,  ..., 4.8096e-02,
           0.0000e+00, 0.0000e+00],
          [9.1797e-01, 3.9978e-03, 9.5215e-03,  ..., 5.9204e-03,
           2.8564e-02, 0.0000e+00],
          [9.5312e-01, 4.1008e-04, 3.0823e-03,  ..., 1.1597e-03,
           6.4850e-04, 3.1494e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 2.6716e-11, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.2932e-12, 4.4529e-09,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [1.0000e+00, 1.0614e-13, 3.0923e-10,  ..., 5.2523e-11,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 4.0927e-12, 2.2847e-09,  ..., 3.0195e-10,
           5.2580e-12, 0.0000e+00],
          [1.0000e+00, 3.9435e-13, 3.7835e-10,  ..., 3.8881e-11,
           9.9476e-14, 1.3589e-13]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[1]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9805, 0.0181, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9336, 0.0214, 0.0461,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7344, 0.0337, 0.1787,  ..., 0.0083, 0.0000, 0.0000],
          [0.8125, 0.0171, 0.0947,  ..., 0.0208, 0.0027, 0.0000],
          [0.6406, 0.0271, 0.0894,  ..., 0.0574, 0.0275, 0.0544]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0221, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.8828, 0.0742, 0.0437,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8555, 0.0082, 0.0247,  ..., 0.0135, 0.0000, 0.0000],
          [0.8906, 0.0027, 0.0112,  ..., 0.0403, 0.0086, 0.0000],
          [0.8555, 0.0030, 0.0068,  ..., 0.0262, 0.0396, 0.0322]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9805, 0.0178, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9492, 0.0203, 0.0305,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9531, 0.0025, 0.0035,  ..., 0.0219, 0.0000, 0.0000],
          [0.9375, 0.0025, 0.0031,  ..., 0.0231, 0.0146, 0.0000],
          [0.8828, 0.0068, 0.0090,  ..., 0.0243, 0.0152, 0.0374]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9805, 0.0098, 0.0111,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9062, 0.0149, 0.0260,  ..., 0.0164, 0.0000, 0.0000],
          [0.8984, 0.0118, 0.0266,  ..., 0.0183, 0.0183, 0.0000],
          [0.8633, 0.0183, 0.0208,  ..., 0.0154, 0.0211, 0.0208]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0128, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9297, 0.0508, 0.0190,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8945, 0.0026, 0.0057,  ..., 0.0306, 0.0000, 0.0000],
          [0.8828, 0.0019, 0.0022,  ..., 0.0608, 0.0120, 0.0000],
          [0.8242, 0.0079, 0.0041,  ..., 0.0576, 0.0317, 0.0211]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9180, 0.0811, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9297, 0.0327, 0.0364,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8945, 0.0095, 0.0076,  ..., 0.0222, 0.0000, 0.0000],
          [0.8945, 0.0064, 0.0048,  ..., 0.0150, 0.0361, 0.0000],
          [0.8320, 0.0167, 0.0124,  ..., 0.0122, 0.0212, 0.0576]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

here is attentions[2]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 6.5002e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.2878e-02, 2.3041e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.7109e-01, 9.7656e-02, 9.6436e-03,  ..., 1.4465e-02,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 9.0332e-03, 6.9275e-03,  ..., 9.3384e-03,
           1.2207e-03, 0.0000e+00],
          [8.8672e-01, 3.5156e-02, 8.5449e-03,  ..., 1.9043e-02,
           7.6904e-03, 1.7090e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 4.6082e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 1.0529e-03, 2.2278e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8438e-01, 3.2997e-04, 8.9645e-04,  ..., 9.6436e-03,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 3.2425e-04, 9.3842e-04,  ..., 3.4790e-03,
           2.7161e-03, 0.0000e+00],
          [9.7656e-01, 5.5695e-04, 5.4169e-04,  ..., 5.1270e-03,
           2.8381e-03, 9.2773e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.2888e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 6.7749e-03, 1.8005e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.5312e-01, 8.8882e-04, 4.1809e-03,  ..., 2.3804e-02,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.4687e-04, 5.0049e-03,  ..., 8.5449e-03,
           4.6730e-04, 0.0000e+00],
          [6.4844e-01, 1.8066e-02, 9.8267e-03,  ..., 2.3926e-01,
           5.5420e-02, 1.8066e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.1963e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 7.3547e-03, 7.4768e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.5703e-01, 1.1902e-03, 1.5793e-03,  ..., 1.2024e-02,
           0.0000e+00, 0.0000e+00],
          [9.0234e-01, 1.4572e-03, 5.6763e-03,  ..., 1.8555e-02,
           2.2095e-02, 0.0000e+00],
          [7.9297e-01, 2.0630e-02, 1.5320e-02,  ..., 1.9043e-02,
           2.4658e-02, 1.1865e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.1057e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 1.4877e-03, 2.6855e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4922e-01, 1.0986e-03, 1.4221e-02,  ..., 6.2866e-03,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 2.6703e-03, 1.1230e-02,  ..., 1.3794e-02,
           4.7607e-03, 0.0000e+00],
          [9.4141e-01, 1.2665e-03, 1.2390e-02,  ..., 8.6670e-03,
           1.1108e-02, 1.6968e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 2.0630e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 2.1118e-02, 1.0254e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.1250e-01, 1.5015e-02, 7.8125e-02,  ..., 8.4229e-03,
           0.0000e+00, 0.0000e+00],
          [8.9844e-01, 1.1047e-02, 3.2715e-02,  ..., 1.1597e-02,
           2.5024e-03, 0.0000e+00],
          [9.0234e-01, 1.2878e-02, 2.4048e-02,  ..., 1.1536e-02,
           1.1902e-02, 1.5747e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[3]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.8076e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 5.3711e-03, 1.6724e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.5703e-01, 6.4087e-03, 9.4604e-03,  ..., 1.0559e-02,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 1.6708e-03, 1.8616e-03,  ..., 5.5237e-03,
           1.7090e-02, 0.0000e+00],
          [9.6875e-01, 1.7090e-03, 9.7275e-04,  ..., 1.7624e-03,
           3.8147e-03, 1.8066e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 9.5825e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 1.4343e-02, 1.7578e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.0234e-01, 1.2085e-02, 1.8677e-02,  ..., 2.2095e-02,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.4877e-03, 1.9150e-03,  ..., 7.7515e-03,
           2.6093e-03, 0.0000e+00],
          [9.4141e-01, 1.1902e-03, 2.5177e-03,  ..., 9.7656e-03,
           5.1270e-03, 3.4668e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 8.4839e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 8.9722e-03, 1.0254e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.2969e-01, 2.5391e-02, 2.0630e-02,  ..., 7.0496e-03,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 3.5095e-03, 5.7373e-03,  ..., 5.0049e-03,
           2.8687e-03, 0.0000e+00],
          [9.6094e-01, 5.7068e-03, 8.4229e-03,  ..., 4.0894e-03,
           7.8125e-03, 1.1169e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.8311e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 7.8125e-03, 6.1951e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.3750e-01, 1.0132e-02, 1.3367e-02,  ..., 1.5503e-02,
           0.0000e+00, 0.0000e+00],
          [9.1797e-01, 2.1973e-03, 4.7302e-03,  ..., 1.3306e-02,
           4.9561e-02, 0.0000e+00],
          [8.2812e-01, 5.4932e-03, 8.9111e-03,  ..., 3.3691e-02,
           5.1758e-02, 4.6631e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 6.4392e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 3.5706e-03, 7.1106e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.5156e-01, 6.1951e-03, 1.4648e-02,  ..., 5.5176e-02,
           0.0000e+00, 0.0000e+00],
          [9.1406e-01, 1.7166e-03, 4.0588e-03,  ..., 1.1719e-02,
           4.7852e-02, 0.0000e+00],
          [5.9375e-01, 2.6245e-03, 9.0942e-03,  ..., 1.0400e-01,
           1.8164e-01, 6.5430e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 9.5825e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 2.7344e-02, 1.4648e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.1406e-01, 7.0801e-03, 1.3062e-02,  ..., 2.4902e-02,
           0.0000e+00, 0.0000e+00],
          [9.1016e-01, 8.3618e-03, 8.9111e-03,  ..., 1.2451e-02,
           3.3447e-02, 0.0000e+00],
          [8.0078e-01, 3.6621e-03, 6.0120e-03,  ..., 4.4189e-02,
           7.4707e-02, 3.4912e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[4]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.5073e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 4.3640e-03, 7.8735e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4531e-01, 2.7771e-03, 6.3477e-03,  ..., 3.3691e-02,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 5.8746e-04, 1.6479e-03,  ..., 6.8665e-03,
           1.8677e-02, 0.0000e+00],
          [9.1016e-01, 3.0365e-03, 5.1575e-03,  ..., 1.5869e-02,
           1.2756e-02, 4.0771e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.4114e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 5.4016e-03, 1.0620e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.5703e-01, 4.7607e-03, 9.7046e-03,  ..., 1.4771e-02,
           0.0000e+00, 0.0000e+00],
          [9.2969e-01, 2.4109e-03, 6.2256e-03,  ..., 7.2021e-03,
           4.5654e-02, 0.0000e+00],
          [9.5703e-01, 2.9144e-03, 4.2725e-03,  ..., 3.7994e-03,
           1.2939e-02, 1.3733e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.8381e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 7.4768e-03, 2.8198e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.2969e-01, 4.5471e-03, 1.6846e-02,  ..., 2.7954e-02,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 7.7820e-04, 2.7618e-03,  ..., 1.0010e-02,
           1.4954e-02, 0.0000e+00],
          [8.6328e-01, 2.5787e-03, 1.3672e-02,  ..., 2.4292e-02,
           5.2979e-02, 3.4180e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 6.5918e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 1.2741e-03, 5.1270e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8438e-01, 3.8719e-04, 9.5749e-04,  ..., 5.8289e-03,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 2.0695e-04, 4.3869e-04,  ..., 3.9673e-03,
           1.9165e-02, 0.0000e+00],
          [8.8672e-01, 1.2665e-03, 1.3275e-03,  ..., 4.0588e-03,
           2.4414e-03, 1.0498e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.9346e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 2.4170e-02, 9.5215e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7656e-01, 2.8381e-03, 5.5542e-03,  ..., 3.6316e-03,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 9.5749e-04, 3.5553e-03,  ..., 6.6528e-03,
           7.0190e-03, 0.0000e+00],
          [9.6875e-01, 5.8594e-03, 5.1270e-03,  ..., 4.8828e-03,
           2.6398e-03, 8.7280e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.8289e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.4893e-02, 5.1575e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6484e-01, 4.1199e-03, 8.6060e-03,  ..., 3.9978e-03,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 5.3406e-04, 1.0681e-03,  ..., 3.1738e-03,
           7.5684e-03, 0.0000e+00],
          [9.5312e-01, 1.6479e-03, 2.7618e-03,  ..., 7.9346e-03,
           1.8677e-02, 1.0803e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[5]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.2268e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 1.5564e-02, 2.0264e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.7109e-01, 4.5776e-03, 6.3965e-02,  ..., 1.3428e-02,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 1.3351e-03, 2.6550e-03,  ..., 1.0071e-02,
           5.8899e-03, 0.0000e+00],
          [9.4922e-01, 6.1951e-03, 9.0332e-03,  ..., 5.1270e-03,
           1.8921e-03, 2.3926e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.6602e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.6113e-02, 7.2327e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9062e-01, 4.3945e-03, 2.6245e-03,  ..., 3.1250e-02,
           0.0000e+00, 0.0000e+00],
          [8.7500e-01, 5.1270e-03, 1.5869e-03,  ..., 2.4170e-02,
           6.1035e-02, 0.0000e+00],
          [9.4531e-01, 6.2866e-03, 1.9150e-03,  ..., 9.3079e-04,
           4.9744e-03, 3.4668e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 6.8970e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 9.3384e-03, 5.4321e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.8125e-01, 2.9053e-02, 1.0986e-01,  ..., 2.0020e-02,
           0.0000e+00, 0.0000e+00],
          [9.1016e-01, 1.3428e-02, 3.1250e-02,  ..., 1.8555e-02,
           6.4392e-03, 0.0000e+00],
          [8.7109e-01, 1.1169e-02, 5.9570e-02,  ..., 1.0864e-02,
           1.4648e-02, 2.5879e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 5.9204e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 1.6357e-02, 1.1169e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [6.0547e-01, 3.6377e-02, 9.9609e-02,  ..., 5.8105e-02,
           0.0000e+00, 0.0000e+00],
          [8.0078e-01, 8.6060e-03, 2.2949e-02,  ..., 7.2266e-02,
           3.6865e-02, 0.0000e+00],
          [7.3047e-01, 1.5869e-02, 9.8145e-02,  ..., 1.7822e-02,
           1.8433e-02, 9.6191e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.3101e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 5.6763e-03, 9.9487e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.3984e-01, 8.6060e-03, 1.5381e-02,  ..., 1.9165e-02,
           0.0000e+00, 0.0000e+00],
          [9.0625e-01, 7.8125e-03, 8.1177e-03,  ..., 1.2817e-02,
           3.7354e-02, 0.0000e+00],
          [3.7891e-01, 7.8735e-03, 1.1047e-02,  ..., 2.5195e-01,
           2.7734e-01, 1.9531e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 9.2773e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 5.0354e-03, 6.7749e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.0234e-01, 5.3711e-03, 9.5215e-03,  ..., 4.3945e-02,
           0.0000e+00, 0.0000e+00],
          [9.0234e-01, 2.7771e-03, 5.8899e-03,  ..., 2.5513e-02,
           5.2246e-02, 0.0000e+00],
          [7.8516e-01, 3.3112e-03, 1.3672e-02,  ..., 1.5991e-02,
           8.8379e-02, 8.3496e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[6]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.7373e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 5.0354e-03, 5.7068e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.3359e-01, 4.2725e-03, 1.5869e-02,  ..., 2.0020e-02,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 1.5411e-03, 5.1270e-03,  ..., 9.8267e-03,
           2.7344e-02, 0.0000e+00],
          [7.4609e-01, 1.3855e-02, 4.6387e-02,  ..., 3.4912e-02,
           6.1523e-02, 4.4434e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.5625e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 3.2959e-02, 9.8267e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.4609e-01, 3.0640e-02, 5.2490e-02,  ..., 3.3936e-02,
           0.0000e+00, 0.0000e+00],
          [8.1250e-01, 1.2634e-02, 2.2095e-02,  ..., 5.4199e-02,
           4.7119e-02, 0.0000e+00],
          [7.1094e-01, 3.6865e-02, 7.3730e-02,  ..., 2.9297e-02,
           2.0386e-02, 8.9355e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 3.7689e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 4.5166e-03, 3.6316e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6875e-01, 4.6997e-03, 3.0365e-03,  ..., 8.6670e-03,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 5.8289e-03, 1.4038e-03,  ..., 2.3956e-03,
           4.7607e-03, 0.0000e+00],
          [9.6094e-01, 4.0588e-03, 1.5869e-03,  ..., 4.8218e-03,
           3.6926e-03, 1.3733e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 4.5471e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 4.6387e-03, 6.2561e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.3359e-01, 5.1270e-03, 6.4697e-03,  ..., 4.1016e-02,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.1978e-03, 2.0447e-03,  ..., 4.9133e-03,
           1.1047e-02, 0.0000e+00],
          [7.4609e-01, 8.8501e-03, 1.6724e-02,  ..., 3.5156e-02,
           1.0059e-01, 6.5430e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.2339e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.0071e-02, 1.5015e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.2812e-01, 1.7578e-02, 7.1289e-02,  ..., 3.2959e-02,
           0.0000e+00, 0.0000e+00],
          [9.1406e-01, 3.5553e-03, 2.6367e-02,  ..., 2.1118e-02,
           1.5076e-02, 0.0000e+00],
          [8.4766e-01, 1.4038e-02, 3.9795e-02,  ..., 1.6235e-02,
           1.6235e-02, 5.0049e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 3.9368e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 1.6632e-03, 4.6692e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7266e-01, 1.7624e-03, 2.8419e-04,  ..., 2.0020e-02,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 7.7820e-04, 5.6839e-04,  ..., 4.6082e-03,
           1.0376e-02, 0.0000e+00],
          [8.0078e-01, 1.8692e-03, 4.1504e-03,  ..., 1.9775e-02,
           6.0791e-02, 9.8633e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[7]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [6.4062e-01, 3.5742e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [5.8594e-01, 1.2109e-01, 2.9297e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.4219e-01, 1.1536e-02, 1.9409e-02,  ..., 1.2793e-01,
           0.0000e+00, 0.0000e+00],
          [7.4219e-01, 3.7689e-03, 5.8289e-03,  ..., 7.4707e-02,
           1.3574e-01, 0.0000e+00],
          [5.9766e-01, 2.1118e-02, 5.1270e-02,  ..., 5.0293e-02,
           4.5898e-02, 1.6406e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.5024e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4141e-01, 4.3945e-02, 1.5442e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.8125e-01, 3.5400e-03, 1.7090e-02,  ..., 5.3467e-02,
           0.0000e+00, 0.0000e+00],
          [9.0625e-01, 8.5831e-04, 3.5553e-03,  ..., 2.0630e-02,
           5.1025e-02, 0.0000e+00],
          [6.7578e-01, 2.2583e-03, 1.1292e-02,  ..., 5.8838e-02,
           1.7383e-01, 6.5430e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 4.1992e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.7734e-01, 1.8359e-01, 3.8086e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [5.6641e-01, 2.4292e-02, 1.2793e-01,  ..., 7.4219e-02,
           0.0000e+00, 0.0000e+00],
          [5.3906e-01, 9.6436e-03, 2.8442e-02,  ..., 2.4805e-01,
           3.7598e-02, 0.0000e+00],
          [5.5469e-01, 2.7832e-02, 1.1670e-01,  ..., 8.5938e-02,
           2.3438e-02, 3.2715e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.4626e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 1.6235e-02, 1.2817e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.7109e-01, 8.2397e-03, 1.2817e-02,  ..., 2.4048e-02,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 5.0659e-03, 6.3782e-03,  ..., 8.8501e-03,
           2.3193e-02, 0.0000e+00],
          [6.3672e-01, 4.2114e-03, 6.6223e-03,  ..., 4.5410e-02,
           2.4902e-01, 5.1025e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 4.5166e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.4587e-02, 7.6904e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.0625e-01, 2.1118e-02, 2.4170e-02,  ..., 1.6724e-02,
           0.0000e+00, 0.0000e+00],
          [9.4141e-01, 7.3242e-03, 7.7820e-03,  ..., 1.8677e-02,
           1.1719e-02, 0.0000e+00],
          [7.9688e-01, 2.4658e-02, 2.4292e-02,  ..., 3.1738e-02,
           6.8359e-02, 2.8198e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.2939e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 2.2461e-02, 2.4414e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.2969e-01, 1.3245e-02, 2.4414e-02,  ..., 9.5215e-03,
           0.0000e+00, 0.0000e+00],
          [8.9453e-01, 1.3367e-02, 1.4893e-02,  ..., 2.8564e-02,
           2.5757e-02, 0.0000e+00],
          [8.9453e-01, 1.8433e-02, 2.1118e-02,  ..., 1.3123e-02,
           2.6978e-02, 1.7944e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[8]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.3711e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 6.1646e-03, 4.8828e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.3984e-01, 1.3367e-02, 2.4719e-03,  ..., 1.1719e-01,
           0.0000e+00, 0.0000e+00],
          [7.4609e-01, 9.0332e-03, 1.3733e-03,  ..., 1.7871e-01,
           4.2480e-02, 0.0000e+00],
          [8.6328e-01, 1.0742e-02, 3.1586e-03,  ..., 2.5391e-02,
           1.2573e-02, 6.1279e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 4.0588e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.3956e-03, 7.0953e-04,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6094e-01, 1.3580e-03, 1.5259e-05,  ..., 3.7598e-02,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 5.5313e-04, 5.0962e-06,  ..., 1.1902e-02,
           3.1738e-03, 0.0000e+00],
          [9.6094e-01, 2.8229e-03, 1.2779e-04,  ..., 8.6670e-03,
           8.0872e-04, 2.0874e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 3.7354e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 3.3936e-02, 1.6846e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.8281e-01, 2.7100e-02, 9.2163e-03,  ..., 3.9062e-02,
           0.0000e+00, 0.0000e+00],
          [8.2031e-01, 1.0498e-02, 5.3711e-03,  ..., 9.4727e-02,
           2.7832e-02, 0.0000e+00],
          [7.8906e-01, 2.4780e-02, 2.2583e-02,  ..., 3.4668e-02,
           1.3794e-02, 6.6895e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.4414e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 3.1250e-02, 1.7822e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.3828e-01, 4.5898e-02, 2.8320e-02,  ..., 4.5410e-02,
           0.0000e+00, 0.0000e+00],
          [7.6562e-01, 1.7944e-02, 1.1169e-02,  ..., 6.2012e-02,
           7.6172e-02, 0.0000e+00],
          [5.6641e-01, 7.1777e-02, 4.1504e-02,  ..., 7.3242e-02,
           7.0312e-02, 8.6426e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.2578e-01, 7.2754e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.8672e-01, 3.0518e-02, 8.1055e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.8906e-01, 3.6377e-02, 1.8433e-02,  ..., 5.6885e-02,
           0.0000e+00, 0.0000e+00],
          [7.9688e-01, 1.8555e-02, 8.4229e-03,  ..., 5.2490e-02,
           5.1758e-02, 0.0000e+00],
          [5.9766e-01, 3.0396e-02, 2.5269e-02,  ..., 7.7637e-02,
           7.3730e-02, 1.3379e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 3.5156e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.9688e-01, 5.7617e-02, 1.4746e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.3203e-01, 3.8086e-02, 1.5381e-02,  ..., 3.8818e-02,
           0.0000e+00, 0.0000e+00],
          [8.9062e-01, 1.3794e-02, 3.4180e-03,  ..., 3.4180e-02,
           2.8687e-02, 0.0000e+00],
          [6.9531e-01, 4.2236e-02, 1.6235e-02,  ..., 7.2266e-02,
           5.6152e-02, 7.6172e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[9]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.1406e-01, 8.6914e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.0234e-01, 6.2012e-02, 3.6865e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.4766e-01, 2.0630e-02, 6.0730e-03,  ..., 6.6895e-02,
           0.0000e+00, 0.0000e+00],
          [8.5156e-01, 1.2024e-02, 3.5858e-03,  ..., 7.4219e-02,
           2.1973e-02, 0.0000e+00],
          [7.5391e-01, 4.0039e-02, 1.3550e-02,  ..., 5.8105e-02,
           3.0884e-02, 6.3477e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 2.0752e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.1016e-01, 4.0771e-02, 4.9072e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.1797e-01, 7.5378e-03, 3.8452e-03,  ..., 3.2227e-02,
           0.0000e+00, 0.0000e+00],
          [8.7891e-01, 3.7384e-03, 3.9978e-03,  ..., 4.7607e-02,
           4.5898e-02, 0.0000e+00],
          [6.6016e-01, 1.3123e-02, 4.9438e-03,  ..., 6.8359e-02,
           1.7480e-01, 5.8105e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 4.8340e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 2.3682e-02, 6.1340e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.8672e-01, 2.0752e-02, 8.7891e-03,  ..., 4.5166e-02,
           0.0000e+00, 0.0000e+00],
          [7.3047e-01, 1.5747e-02, 6.0120e-03,  ..., 1.4258e-01,
           3.6133e-02, 0.0000e+00],
          [8.0859e-01, 2.8198e-02, 6.6833e-03,  ..., 4.0283e-02,
           2.7100e-02, 7.1289e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 2.5513e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 4.9561e-02, 3.6926e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9453e-01, 1.5259e-02, 2.5482e-03,  ..., 5.5908e-02,
           0.0000e+00, 0.0000e+00],
          [8.1641e-01, 1.3733e-02, 1.9989e-03,  ..., 8.3984e-02,
           4.0283e-02, 0.0000e+00],
          [9.0234e-01, 1.8433e-02, 4.2419e-03,  ..., 2.5635e-02,
           1.7212e-02, 1.9287e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 2.6245e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 3.7598e-02, 9.8877e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9453e-01, 2.0386e-02, 1.9775e-02,  ..., 2.9419e-02,
           0.0000e+00, 0.0000e+00],
          [8.8672e-01, 1.1536e-02, 8.6060e-03,  ..., 4.2725e-02,
           3.0396e-02, 0.0000e+00],
          [8.0469e-01, 1.9287e-02, 2.1484e-02,  ..., 3.8330e-02,
           5.7129e-02, 3.8818e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 5.0293e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 1.4709e-02, 2.2339e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.2188e-01, 6.1951e-03, 9.3460e-04,  ..., 4.8340e-02,
           0.0000e+00, 0.0000e+00],
          [9.1406e-01, 5.5847e-03, 1.4343e-03,  ..., 3.6133e-02,
           1.9897e-02, 0.0000e+00],
          [8.1641e-01, 4.0283e-03, 1.3962e-03,  ..., 9.3750e-02,
           4.4678e-02, 1.9531e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[10]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.4375e-01, 1.5723e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.6328e-01, 8.8379e-02, 4.8828e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.7734e-01, 3.1281e-03, 1.4160e-02,  ..., 1.1719e-01,
           0.0000e+00, 0.0000e+00],
          [7.5000e-01, 1.5182e-03, 1.9836e-03,  ..., 7.5195e-02,
           1.0547e-01, 0.0000e+00],
          [6.5234e-01, 5.7373e-03, 3.5400e-03,  ..., 4.0771e-02,
           2.1362e-02, 2.5781e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.8555e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.7090e-02, 6.6833e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [6.3281e-01, 2.8839e-03, 1.0132e-02,  ..., 2.0020e-01,
           0.0000e+00, 0.0000e+00],
          [7.5391e-01, 1.8387e-03, 2.5482e-03,  ..., 7.0312e-02,
           8.3984e-02, 0.0000e+00],
          [6.5625e-01, 1.6235e-02, 1.2024e-02,  ..., 1.5918e-01,
           3.4668e-02, 6.6895e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.3245e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.2054e-03, 1.4954e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.1016e-01, 4.7302e-04, 3.0365e-03,  ..., 8.2031e-02,
           0.0000e+00, 0.0000e+00],
          [8.6719e-01, 2.5558e-04, 2.4719e-03,  ..., 3.2959e-02,
           6.5430e-02, 0.0000e+00],
          [9.2578e-01, 2.9449e-03, 7.5073e-03,  ..., 1.4801e-03,
           3.2997e-04, 5.5176e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 4.2725e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 2.0386e-02, 3.2471e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.3203e-01, 2.0752e-02, 1.2512e-03,  ..., 8.8867e-02,
           0.0000e+00, 0.0000e+00],
          [7.7344e-01, 1.8311e-02, 1.4877e-03,  ..., 4.8096e-02,
           7.5195e-02, 0.0000e+00],
          [8.0859e-01, 2.5513e-02, 6.2561e-03,  ..., 4.5898e-02,
           2.5330e-03, 5.4688e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.3359e-01, 6.5430e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.9453e-01, 5.6885e-02, 4.7119e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [4.8633e-01, 4.1748e-02, 2.6611e-02,  ..., 1.5820e-01,
           0.0000e+00, 0.0000e+00],
          [3.8281e-01, 2.0264e-02, 1.3062e-02,  ..., 3.6523e-01,
           4.1016e-02, 0.0000e+00],
          [4.8633e-01, 3.6865e-02, 2.6001e-02,  ..., 2.4805e-01,
           2.3071e-02, 1.0205e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.2949e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 1.9455e-03, 7.6294e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6875e-01, 1.7624e-03, 4.3392e-05,  ..., 2.3071e-02,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 7.2479e-04, 2.6464e-05,  ..., 3.9673e-03,
           1.8433e-02, 0.0000e+00],
          [9.6094e-01, 1.9455e-03, 3.8910e-04,  ..., 3.4637e-03,
           2.2125e-04, 2.8687e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[11]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9688, 0.0293, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9648, 0.0317, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7891, 0.0625, 0.0374,  ..., 0.0537, 0.0000, 0.0000],
          [0.5312, 0.0337, 0.0430,  ..., 0.2246, 0.0352, 0.0000],
          [0.7461, 0.0457, 0.0388,  ..., 0.0542, 0.0164, 0.0454]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9648, 0.0337, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9844, 0.0096, 0.0052,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8906, 0.0413, 0.0153,  ..., 0.0066, 0.0000, 0.0000],
          [0.8164, 0.0226, 0.0214,  ..., 0.0276, 0.0075, 0.0000],
          [0.7266, 0.0388, 0.0354,  ..., 0.0737, 0.0248, 0.0513]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0237, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0194, 0.0054,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8203, 0.0522, 0.0221,  ..., 0.0197, 0.0000, 0.0000],
          [0.7930, 0.0422, 0.0220,  ..., 0.0299, 0.0188, 0.0000],
          [0.8203, 0.0513, 0.0374,  ..., 0.0171, 0.0164, 0.0184]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9727, 0.0186, 0.0098,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9805, 0.0049, 0.0015,  ..., 0.0067, 0.0000, 0.0000],
          [0.9570, 0.0058, 0.0035,  ..., 0.0134, 0.0095, 0.0000],
          [0.9609, 0.0062, 0.0027,  ..., 0.0079, 0.0040, 0.0151]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9922, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9688, 0.0082, 0.0228,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9102, 0.0094, 0.0070,  ..., 0.0253, 0.0000, 0.0000],
          [0.7539, 0.0062, 0.0098,  ..., 0.0947, 0.0520, 0.0000],
          [0.8359, 0.0228, 0.0139,  ..., 0.0193, 0.0076, 0.0723]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9688, 0.0308, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9219, 0.0669, 0.0129,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7461, 0.0525, 0.1377,  ..., 0.0165, 0.0000, 0.0000],
          [0.7812, 0.0166, 0.0356,  ..., 0.1245, 0.0139, 0.0000],
          [0.7188, 0.0562, 0.0703,  ..., 0.0728, 0.0137, 0.0435]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

here is attentions[12]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9023, 0.0977, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.8555, 0.1167, 0.0295,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7109, 0.0552, 0.0488,  ..., 0.0903, 0.0000, 0.0000],
          [0.5625, 0.0549, 0.0530,  ..., 0.2070, 0.0544, 0.0000],
          [0.6367, 0.0723, 0.0547,  ..., 0.0659, 0.0271, 0.1025]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9727, 0.0282, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9648, 0.0272, 0.0075,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9297, 0.0273, 0.0061,  ..., 0.0243, 0.0000, 0.0000],
          [0.8945, 0.0267, 0.0078,  ..., 0.0322, 0.0172, 0.0000],
          [0.8750, 0.0291, 0.0050,  ..., 0.0315, 0.0079, 0.0427]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9297, 0.0688, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9414, 0.0474, 0.0106,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8398, 0.0287, 0.0155,  ..., 0.0447, 0.0000, 0.0000],
          [0.8008, 0.0156, 0.0056,  ..., 0.0962, 0.0454, 0.0000],
          [0.6523, 0.0170, 0.0121,  ..., 0.1113, 0.0796, 0.0835]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9531, 0.0471, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9219, 0.0698, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7812, 0.0097, 0.0047,  ..., 0.0967, 0.0000, 0.0000],
          [0.8398, 0.0053, 0.0019,  ..., 0.0781, 0.0308, 0.0000],
          [0.8516, 0.0160, 0.0070,  ..., 0.0608, 0.0267, 0.0141]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.7656, 0.2344, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.7031, 0.1826, 0.1138,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7031, 0.0215, 0.0131,  ..., 0.1094, 0.0000, 0.0000],
          [0.6328, 0.0090, 0.0046,  ..., 0.1729, 0.1152, 0.0000],
          [0.6680, 0.0199, 0.0203,  ..., 0.1064, 0.0422, 0.1016]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9414, 0.0571, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9180, 0.0649, 0.0170,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8164, 0.0277, 0.0140,  ..., 0.0479, 0.0000, 0.0000],
          [0.6406, 0.0132, 0.0073,  ..., 0.1572, 0.1064, 0.0000],
          [0.6406, 0.0216, 0.0121,  ..., 0.1108, 0.1206, 0.0522]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

here is attentions[13]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9609, 0.0408, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9570, 0.0374, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8555, 0.0361, 0.0167,  ..., 0.0181, 0.0000, 0.0000],
          [0.8320, 0.0240, 0.0112,  ..., 0.0439, 0.0140, 0.0000],
          [0.7930, 0.0427, 0.0156,  ..., 0.0488, 0.0280, 0.0187]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9727, 0.0272, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9570, 0.0233, 0.0209,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8594, 0.0124, 0.0042,  ..., 0.0659, 0.0000, 0.0000],
          [0.8438, 0.0089, 0.0029,  ..., 0.0447, 0.0767, 0.0000],
          [0.7461, 0.0131, 0.0060,  ..., 0.1006, 0.0422, 0.0488]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9258, 0.0728, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9375, 0.0481, 0.0153,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7930, 0.0400, 0.0140,  ..., 0.0579, 0.0000, 0.0000],
          [0.6484, 0.0229, 0.0189,  ..., 0.1338, 0.0464, 0.0000],
          [0.7227, 0.0403, 0.0143,  ..., 0.0762, 0.0344, 0.0520]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.8320, 0.1670, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.8203, 0.1484, 0.0312,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7227, 0.0133, 0.0262,  ..., 0.1177, 0.0000, 0.0000],
          [0.6289, 0.0160, 0.0283,  ..., 0.1543, 0.0752, 0.0000],
          [0.6250, 0.0160, 0.0183,  ..., 0.1123, 0.0884, 0.0962]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9531, 0.0474, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9688, 0.0197, 0.0117,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8398, 0.0150, 0.0068,  ..., 0.0850, 0.0000, 0.0000],
          [0.8242, 0.0245, 0.0051,  ..., 0.0547, 0.0464, 0.0000],
          [0.7500, 0.0080, 0.0049,  ..., 0.0654, 0.0898, 0.0554]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9219, 0.0781, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9219, 0.0530, 0.0254,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.5625, 0.0781, 0.0442,  ..., 0.1982, 0.0000, 0.0000],
          [0.5469, 0.0996, 0.0452,  ..., 0.1562, 0.0515, 0.0000],
          [0.6055, 0.0535, 0.0334,  ..., 0.1328, 0.0442, 0.0718]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

here is attentions[14]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.0254e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.2207e-02, 3.2806e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.5938e-01, 1.7456e-02, 8.6060e-03,  ..., 5.0781e-02,
           0.0000e+00, 0.0000e+00],
          [8.6328e-01, 1.4160e-02, 7.4768e-03,  ..., 3.6621e-02,
           3.8574e-02, 0.0000e+00],
          [9.0625e-01, 7.3547e-03, 2.3193e-03,  ..., 1.9165e-02,
           1.8311e-02, 3.3203e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 9.3384e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 9.5215e-03, 3.1433e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.2031e-01, 1.9287e-02, 5.4321e-03,  ..., 6.3965e-02,
           0.0000e+00, 0.0000e+00],
          [8.7500e-01, 7.3242e-03, 4.9744e-03,  ..., 4.3213e-02,
           4.1016e-02, 0.0000e+00],
          [7.5781e-01, 7.3853e-03, 7.3853e-03,  ..., 7.2754e-02,
           7.9102e-02, 3.2471e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.3245e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 7.3853e-03, 2.7313e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.3984e-01, 2.7100e-02, 9.5825e-03,  ..., 3.7842e-02,
           0.0000e+00, 0.0000e+00],
          [8.4375e-01, 1.5259e-02, 7.9346e-03,  ..., 3.0884e-02,
           4.9072e-02, 0.0000e+00],
          [8.0469e-01, 6.4392e-03, 4.0283e-03,  ..., 4.5166e-02,
           7.8613e-02, 3.7354e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.3359e-01, 6.6406e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.0234e-01, 6.4453e-02, 3.2471e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.4375e-01, 1.3123e-02, 1.2512e-03,  ..., 8.9844e-02,
           0.0000e+00, 0.0000e+00],
          [8.8281e-01, 1.7212e-02, 3.5095e-03,  ..., 3.8086e-02,
           1.8311e-02, 0.0000e+00],
          [8.3984e-01, 2.7100e-02, 3.5400e-03,  ..., 4.3945e-02,
           7.2632e-03, 4.4678e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 2.6978e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 3.2471e-02, 3.8147e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.3359e-01, 1.4282e-02, 2.6703e-03,  ..., 2.2339e-02,
           0.0000e+00, 0.0000e+00],
          [9.0625e-01, 1.4771e-02, 3.3722e-03,  ..., 2.5391e-02,
           2.4048e-02, 0.0000e+00],
          [8.6328e-01, 2.8687e-02, 5.5542e-03,  ..., 3.0518e-02,
           1.9409e-02, 2.4902e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.2461e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 8.9722e-03, 1.6327e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.1406e-01, 9.6436e-03, 9.6130e-04,  ..., 5.8105e-02,
           0.0000e+00, 0.0000e+00],
          [8.3594e-01, 7.8125e-03, 2.0447e-03,  ..., 5.3223e-02,
           7.7148e-02, 0.0000e+00],
          [7.2656e-01, 1.3855e-02, 6.0120e-03,  ..., 6.0791e-02,
           7.7637e-02, 9.1797e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[15]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 4.2969e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.7500e-01, 8.3984e-02, 4.0771e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [5.6641e-01, 8.7891e-03, 1.6479e-02,  ..., 1.4746e-01,
           0.0000e+00, 0.0000e+00],
          [5.5469e-01, 1.8799e-02, 3.9062e-02,  ..., 2.2363e-01,
           7.7637e-02, 0.0000e+00],
          [5.2344e-01, 1.9653e-02, 4.3457e-02,  ..., 9.3750e-02,
           5.5908e-02, 2.0898e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 9.8419e-04, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 3.6812e-04, 7.5531e-04,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8438e-01, 1.4687e-04, 8.1062e-05,  ..., 1.5015e-02,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 4.3154e-05, 9.8944e-06,  ..., 2.5482e-03,
           2.6703e-03, 0.0000e+00],
          [9.9219e-01, 1.5736e-04, 6.7711e-05,  ..., 1.0910e-03,
           5.4359e-05, 5.7068e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 6.1035e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 1.1063e-03, 5.7373e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.5703e-01, 1.7090e-03, 2.8534e-03,  ..., 2.5757e-02,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 5.1498e-04, 1.1215e-03,  ..., 2.8198e-02,
           7.7209e-03, 0.0000e+00],
          [9.8438e-01, 8.5449e-04, 2.2583e-03,  ..., 2.6855e-03,
           5.1117e-04, 8.2397e-03]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.5991e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.2573e-02, 9.8267e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.1484e-01, 2.7008e-03, 4.1199e-03,  ..., 2.4316e-01,
           0.0000e+00, 0.0000e+00],
          [8.6328e-01, 4.2725e-03, 3.3875e-03,  ..., 7.8613e-02,
           4.0283e-02, 0.0000e+00],
          [7.3047e-01, 1.4954e-02, 2.0752e-02,  ..., 6.7871e-02,
           3.3203e-02, 5.8594e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.8387e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 3.4027e-03, 5.9814e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7656e-01, 1.3275e-03, 5.5313e-04,  ..., 1.7456e-02,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 3.9482e-04, 5.8651e-05,  ..., 2.0264e-02,
           5.2795e-03, 0.0000e+00],
          [9.9219e-01, 2.5635e-03, 1.1969e-04,  ..., 3.9291e-04,
           6.3896e-05, 4.6387e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.9844e-01, 1.0156e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.8906e-01, 1.5137e-01, 6.0303e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [6.9922e-01, 5.2734e-02, 6.9336e-02,  ..., 3.5889e-02,
           0.0000e+00, 0.0000e+00],
          [3.8672e-01, 1.7480e-01, 2.2754e-01,  ..., 7.9590e-02,
           1.8188e-02, 0.0000e+00],
          [6.3281e-01, 7.3242e-02, 8.0078e-02,  ..., 1.4587e-02,
           1.9775e-02, 1.3965e-01]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[16]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.0193e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 2.0874e-02, 5.9814e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6094e-01, 3.3569e-03, 1.5564e-03,  ..., 1.7334e-02,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 3.7689e-03, 4.1504e-03,  ..., 2.5757e-02,
           1.3367e-02, 0.0000e+00],
          [9.5312e-01, 6.5308e-03, 3.0365e-03,  ..., 7.0496e-03,
           3.7689e-03, 1.8921e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.9455e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 5.9204e-03, 4.0894e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7656e-01, 9.1934e-04, 5.9128e-04,  ..., 1.5015e-02,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 4.3945e-03, 4.1809e-03,  ..., 2.4292e-02,
           1.3489e-02, 0.0000e+00],
          [9.1016e-01, 1.4343e-02, 2.3499e-03,  ..., 4.1199e-03,
           1.8845e-03, 5.5908e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.3547e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 1.3245e-02, 4.2725e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.2188e-01, 1.9684e-03, 3.0518e-03,  ..., 4.3457e-02,
           0.0000e+00, 0.0000e+00],
          [8.9844e-01, 2.7924e-03, 8.6060e-03,  ..., 6.1279e-02,
           1.9409e-02, 0.0000e+00],
          [7.8125e-01, 2.5269e-02, 1.1414e-02,  ..., 2.1484e-02,
           7.4768e-03, 1.2793e-01]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.9062e-01, 1.0938e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.0312e-01, 2.5586e-01, 3.9795e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [5.7812e-01, 2.1729e-02, 3.0396e-02,  ..., 2.0020e-01,
           0.0000e+00, 0.0000e+00],
          [3.5547e-01, 2.4048e-02, 3.9307e-02,  ..., 3.5938e-01,
           9.0820e-02, 0.0000e+00],
          [5.5469e-01, 3.4912e-02, 1.6724e-02,  ..., 4.9316e-02,
           1.1572e-01, 2.0312e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [5.0781e-01, 4.9023e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [5.0391e-01, 6.3965e-02, 4.3164e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [4.0820e-01, 9.9487e-03, 4.5776e-03,  ..., 5.3906e-01,
           0.0000e+00, 0.0000e+00],
          [3.1641e-01, 1.2939e-02, 5.6458e-03,  ..., 5.7617e-02,
           6.0156e-01, 0.0000e+00],
          [3.5547e-01, 1.4587e-02, 7.9956e-03,  ..., 1.4893e-02,
           5.1758e-02, 5.4688e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.8281e-01, 1.1621e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.3203e-01, 9.7656e-02, 6.9336e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [6.1719e-01, 6.0547e-02, 3.8086e-02,  ..., 1.7188e-01,
           0.0000e+00, 0.0000e+00],
          [3.3789e-01, 9.6680e-02, 6.1035e-02,  ..., 2.9688e-01,
           1.1426e-01, 0.0000e+00],
          [2.7539e-01, 1.5918e-01, 6.5430e-02,  ..., 2.1680e-01,
           8.8867e-02, 1.3672e-01]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[17]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 9.2773e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.0071e-02, 8.7280e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.5703e-01, 1.7319e-03, 4.5204e-04,  ..., 3.3691e-02,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.3199e-03, 2.1172e-04,  ..., 1.1963e-02,
           5.7373e-03, 0.0000e+00],
          [9.6484e-01, 2.2430e-03, 2.0599e-04,  ..., 1.1353e-02,
           8.6670e-03, 9.7046e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.9897e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 1.4404e-02, 1.5747e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6875e-01, 3.7231e-03, 2.1515e-03,  ..., 1.3367e-02,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 5.4932e-03, 3.4943e-03,  ..., 1.4465e-02,
           1.0925e-02, 0.0000e+00],
          [9.6094e-01, 9.2773e-03, 3.4637e-03,  ..., 5.9814e-03,
           4.3030e-03, 8.7280e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 2.7832e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.0078e-01, 1.1133e-01, 8.6914e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.3047e-01, 1.2939e-02, 1.1902e-02,  ..., 1.4844e-01,
           0.0000e+00, 0.0000e+00],
          [7.7734e-01, 1.9897e-02, 1.2329e-02,  ..., 8.9355e-02,
           2.8442e-02, 0.0000e+00],
          [8.6719e-01, 1.6602e-02, 2.9297e-03,  ..., 2.9907e-02,
           1.5137e-02, 4.6143e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.6245e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 6.7444e-03, 8.1177e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8438e-01, 6.7520e-04, 2.8229e-04,  ..., 1.0620e-02,
           0.0000e+00, 0.0000e+00],
          [8.3984e-01, 2.2125e-03, 1.0452e-03,  ..., 6.4941e-02,
           6.6406e-02, 0.0000e+00],
          [7.8516e-01, 2.1667e-03, 1.6174e-03,  ..., 8.8379e-02,
           6.4941e-02, 3.9551e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.2634e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 2.5513e-02, 2.5269e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.6953e-01, 1.1841e-02, 1.1035e-01,  ..., 4.6387e-02,
           0.0000e+00, 0.0000e+00],
          [3.2031e-01, 3.1982e-02, 4.1797e-01,  ..., 8.2520e-02,
           1.1963e-02, 0.0000e+00],
          [8.1641e-01, 1.6357e-02, 6.3477e-02,  ..., 1.4587e-02,
           8.1177e-03, 4.8584e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.2451e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 1.8433e-02, 1.4771e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.1406e-01, 3.3417e-03, 6.5613e-03,  ..., 5.4688e-02,
           0.0000e+00, 0.0000e+00],
          [7.6562e-01, 7.3242e-03, 9.7656e-03,  ..., 1.1426e-01,
           7.4219e-02, 0.0000e+00],
          [7.8125e-01, 1.6968e-02, 1.8311e-02,  ..., 4.2725e-02,
           3.5889e-02, 8.8379e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[18]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0171, 0.0052,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9648, 0.0013, 0.0030,  ..., 0.0199, 0.0000, 0.0000],
          [0.7812, 0.0016, 0.0022,  ..., 0.1060, 0.0674, 0.0000],
          [0.7266, 0.0062, 0.0078,  ..., 0.0732, 0.0491, 0.0967]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9102, 0.0903, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.8555, 0.0308, 0.1133,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9492, 0.0061, 0.0041,  ..., 0.0132, 0.0000, 0.0000],
          [0.9375, 0.0109, 0.0060,  ..., 0.0093, 0.0042, 0.0000],
          [0.9648, 0.0052, 0.0021,  ..., 0.0048, 0.0028, 0.0070]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9727, 0.0261, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.8281, 0.0781, 0.0957,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9062, 0.0095, 0.0052,  ..., 0.0306, 0.0000, 0.0000],
          [0.9258, 0.0112, 0.0153,  ..., 0.0164, 0.0058, 0.0000],
          [0.9453, 0.0081, 0.0066,  ..., 0.0085, 0.0044, 0.0148]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0248, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9648, 0.0128, 0.0214,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8555, 0.0079, 0.0090,  ..., 0.0713, 0.0000, 0.0000],
          [0.8242, 0.0107, 0.0131,  ..., 0.0879, 0.0284, 0.0000],
          [0.7227, 0.0162, 0.0162,  ..., 0.0569, 0.0742, 0.0811]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9961, 0.0025, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0054, 0.0079,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.5195, 0.0100, 0.0060,  ..., 0.1709, 0.0000, 0.0000],
          [0.6680, 0.0033, 0.0100,  ..., 0.2158, 0.0884, 0.0000],
          [0.6836, 0.0249, 0.0166,  ..., 0.0540, 0.1523, 0.0356]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9961, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9922, 0.0036, 0.0038,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8008, 0.0037, 0.0059,  ..., 0.1167, 0.0000, 0.0000],
          [0.9141, 0.0017, 0.0026,  ..., 0.0513, 0.0165, 0.0000],
          [0.8320, 0.0049, 0.0045,  ..., 0.0292, 0.0588, 0.0154]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

here is attentions[19]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9961, 0.0028, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0084, 0.0157,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8164, 0.0032, 0.0022,  ..., 0.0593, 0.0000, 0.0000],
          [0.9023, 0.0015, 0.0018,  ..., 0.0583, 0.0229, 0.0000],
          [0.8906, 0.0079, 0.0034,  ..., 0.0288, 0.0226, 0.0217]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9961, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9727, 0.0072, 0.0186,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8789, 0.0118, 0.0039,  ..., 0.0325, 0.0000, 0.0000],
          [0.8438, 0.0273, 0.0208,  ..., 0.0569, 0.0201, 0.0000],
          [0.7969, 0.0571, 0.0400,  ..., 0.0378, 0.0130, 0.0315]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9805, 0.0060, 0.0153,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8594, 0.0121, 0.0036,  ..., 0.0332, 0.0000, 0.0000],
          [0.9023, 0.0125, 0.0135,  ..., 0.0435, 0.0058, 0.0000],
          [0.9102, 0.0269, 0.0130,  ..., 0.0160, 0.0057, 0.0150]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9531, 0.0486, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9648, 0.0172, 0.0179,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8750, 0.0161, 0.0161,  ..., 0.0216, 0.0000, 0.0000],
          [0.9180, 0.0130, 0.0139,  ..., 0.0194, 0.0130, 0.0000],
          [0.9023, 0.0164, 0.0070,  ..., 0.0166, 0.0132, 0.0322]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9805, 0.0186, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9258, 0.0474, 0.0251,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9062, 0.0024, 0.0056,  ..., 0.0405, 0.0000, 0.0000],
          [0.8125, 0.0136, 0.0178,  ..., 0.0635, 0.0354, 0.0000],
          [0.8008, 0.0203, 0.0104,  ..., 0.0510, 0.0383, 0.0461]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9531, 0.0481, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.7852, 0.0552, 0.1602,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9102, 0.0023, 0.0054,  ..., 0.0405, 0.0000, 0.0000],
          [0.8633, 0.0045, 0.0168,  ..., 0.0291, 0.0334, 0.0000],
          [0.9414, 0.0038, 0.0130,  ..., 0.0087, 0.0081, 0.0153]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

here is attentions[20]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.8008, 0.2002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.7500, 0.1196, 0.1299,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7148, 0.0073, 0.0157,  ..., 0.1709, 0.0000, 0.0000],
          [0.6836, 0.0045, 0.0157,  ..., 0.0708, 0.1777, 0.0000],
          [0.5938, 0.0242, 0.0640,  ..., 0.0635, 0.0474, 0.1357]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0226, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9805, 0.0025, 0.0151,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8164, 0.0044, 0.0082,  ..., 0.0962, 0.0000, 0.0000],
          [0.8555, 0.0023, 0.0075,  ..., 0.0767, 0.0231, 0.0000],
          [0.8516, 0.0048, 0.0104,  ..., 0.0347, 0.0151, 0.0420]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9922, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9961, 0.0013, 0.0036,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7656, 0.0022, 0.0061,  ..., 0.0703, 0.0000, 0.0000],
          [0.8828, 0.0024, 0.0052,  ..., 0.0294, 0.0400, 0.0000],
          [0.8359, 0.0042, 0.0069,  ..., 0.0415, 0.0430, 0.0192]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9219, 0.0659, 0.0137,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8711, 0.0036, 0.0079,  ..., 0.0659, 0.0000, 0.0000],
          [0.6719, 0.0086, 0.0195,  ..., 0.0742, 0.1670, 0.0000],
          [0.8125, 0.0066, 0.0063,  ..., 0.0117, 0.0708, 0.0815]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9922, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0041, 0.0072,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9453, 0.0021, 0.0065,  ..., 0.0299, 0.0000, 0.0000],
          [0.8477, 0.0032, 0.0231,  ..., 0.0243, 0.0527, 0.0000],
          [0.7227, 0.0151, 0.0464,  ..., 0.0101, 0.0359, 0.1250]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9922, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9531, 0.0220, 0.0238,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8203, 0.0203, 0.0449,  ..., 0.0186, 0.0000, 0.0000],
          [0.8008, 0.0305, 0.0508,  ..., 0.0280, 0.0232, 0.0000],
          [0.9336, 0.0190, 0.0079,  ..., 0.0064, 0.0117, 0.0127]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

here is attentions[21]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.5625e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.1230e-02, 9.0332e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9844e-01, 1.7578e-02, 1.9531e-02,  ..., 2.6489e-02,
           0.0000e+00, 0.0000e+00],
          [6.8750e-01, 4.0039e-02, 7.7148e-02,  ..., 6.7871e-02,
           5.3955e-02, 0.0000e+00],
          [8.0859e-01, 3.7354e-02, 2.8564e-02,  ..., 2.4658e-02,
           4.7119e-02, 3.0151e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 8.8501e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 8.1177e-03, 9.0332e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.3594e-01, 3.2654e-03, 3.1128e-03,  ..., 8.7402e-02,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 8.7357e-04, 5.1117e-04,  ..., 2.6398e-03,
           7.4158e-03, 0.0000e+00],
          [9.1406e-01, 2.1606e-02, 9.8267e-03,  ..., 9.0942e-03,
           1.0437e-02, 2.3926e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 2.6978e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4141e-01, 1.0010e-02, 4.8584e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.6719e-01, 2.5177e-03, 1.1719e-02,  ..., 6.7871e-02,
           0.0000e+00, 0.0000e+00],
          [9.4141e-01, 9.5749e-04, 1.0834e-03,  ..., 3.9062e-03,
           4.1260e-02, 0.0000e+00],
          [9.2578e-01, 5.4932e-03, 5.3406e-03,  ..., 2.5635e-03,
           5.4016e-03, 4.7607e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.7014e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 8.0109e-04, 5.5542e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7656e-01, 1.1597e-03, 9.6130e-04,  ..., 8.3008e-03,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 2.8610e-04, 1.2436e-03,  ..., 8.4229e-03,
           2.2125e-03, 0.0000e+00],
          [9.6875e-01, 1.4114e-03, 2.9907e-03,  ..., 4.5471e-03,
           2.8992e-03, 2.1210e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.3101e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 2.4261e-03, 5.2185e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.9219e-01, 1.2360e-03, 9.3460e-04,  ..., 2.0447e-03,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 4.3297e-04, 2.2736e-03,  ..., 1.7700e-03,
           7.6294e-04, 0.0000e+00],
          [9.9219e-01, 1.2131e-03, 1.8005e-03,  ..., 9.1934e-04,
           3.8338e-04, 1.0757e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.3193e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 5.9128e-04, 9.2163e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7266e-01, 1.6861e-03, 2.8229e-03,  ..., 8.2397e-03,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 3.7956e-04, 3.9673e-03,  ..., 5.0049e-03,
           3.8300e-03, 0.0000e+00],
          [9.3359e-01, 2.0142e-03, 1.3672e-02,  ..., 8.6060e-03,
           8.8501e-03, 6.4087e-03]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[22]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.5177e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 1.0376e-03, 7.2327e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8047e-01, 7.6675e-04, 1.3199e-03,  ..., 5.5542e-03,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 2.1553e-04, 1.8387e-03,  ..., 8.1177e-03,
           1.6174e-03, 0.0000e+00],
          [9.6875e-01, 2.1515e-03, 4.9133e-03,  ..., 4.2114e-03,
           1.0452e-03, 9.2773e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 3.1738e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 1.3580e-03, 5.0659e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6875e-01, 4.0894e-03, 2.1820e-03,  ..., 1.0376e-02,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 6.5231e-04, 1.8311e-03,  ..., 9.2163e-03,
           2.9755e-03, 0.0000e+00],
          [9.3359e-01, 6.6833e-03, 9.1553e-03,  ..., 5.2795e-03,
           3.1586e-03, 2.8564e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.7700e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 9.5825e-03, 5.3406e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9844e-01, 6.1523e-02, 4.9438e-03,  ..., 1.8066e-02,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 6.0730e-03, 5.7068e-03,  ..., 2.0508e-02,
           3.7384e-03, 0.0000e+00],
          [9.2578e-01, 2.3682e-02, 7.4158e-03,  ..., 4.7913e-03,
           1.5793e-03, 2.8564e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 9.2773e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 3.0060e-03, 6.5613e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9062e-01, 9.5825e-03, 1.0986e-02,  ..., 2.8198e-02,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 1.1902e-03, 3.2501e-03,  ..., 8.9722e-03,
           5.7068e-03, 0.0000e+00],
          [9.2969e-01, 8.4229e-03, 7.5378e-03,  ..., 7.2021e-03,
           7.6599e-03, 1.9409e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 1.9531e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 5.2795e-03, 6.9885e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6875e-01, 2.1515e-03, 1.7548e-03,  ..., 1.1536e-02,
           0.0000e+00, 0.0000e+00],
          [9.2578e-01, 2.3651e-03, 3.6621e-03,  ..., 3.0640e-02,
           1.7456e-02, 0.0000e+00],
          [8.9453e-01, 1.0071e-02, 5.3101e-03,  ..., 1.2268e-02,
           2.0508e-02, 3.0762e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 4.7607e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 2.3193e-03, 3.9978e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7266e-01, 3.9062e-03, 2.8229e-03,  ..., 5.9814e-03,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 3.9673e-04, 8.1253e-04,  ..., 6.5231e-04,
           3.1853e-04, 0.0000e+00],
          [9.6875e-01, 3.0823e-03, 4.0283e-03,  ..., 8.2779e-04,
           4.1771e-04, 1.7456e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[23]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 3.5889e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 3.6621e-02, 1.7090e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.2188e-01, 1.5869e-02, 2.1973e-02,  ..., 1.5991e-02,
           0.0000e+00, 0.0000e+00],
          [9.3359e-01, 1.9043e-02, 1.4771e-02,  ..., 6.7139e-03,
           2.1820e-03, 0.0000e+00],
          [9.2969e-01, 2.0142e-02, 1.8555e-02,  ..., 4.1809e-03,
           7.0801e-03, 1.2146e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [6.8750e-01, 3.1055e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.0312e-01, 5.8105e-02, 2.3730e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6094e-01, 1.2573e-02, 3.6316e-03,  ..., 1.6846e-02,
           0.0000e+00, 0.0000e+00],
          [9.4141e-01, 2.4780e-02, 5.0354e-03,  ..., 8.3008e-03,
           1.3428e-02, 0.0000e+00],
          [8.9844e-01, 1.4404e-02, 3.4943e-03,  ..., 7.7515e-03,
           1.5076e-02, 5.8838e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.2578e-01, 7.3242e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.3750e-01, 2.3071e-02, 4.0039e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9844e-01, 2.1729e-02, 2.6489e-02,  ..., 2.3682e-02,
           0.0000e+00, 0.0000e+00],
          [8.4375e-01, 2.7344e-02, 2.1973e-02,  ..., 3.9062e-02,
           1.8311e-02, 0.0000e+00],
          [8.9062e-01, 2.8931e-02, 1.3977e-02,  ..., 1.0193e-02,
           1.0193e-02, 3.8574e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.2578e-01, 7.5195e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.0625e-01, 6.4941e-02, 2.6733e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4922e-01, 6.9427e-04, 1.1444e-03,  ..., 2.1851e-02,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 7.8583e-04, 1.0910e-03,  ..., 2.2949e-02,
           2.0020e-02, 0.0000e+00],
          [9.6875e-01, 6.4468e-04, 5.5313e-04,  ..., 2.4414e-03,
           6.8359e-03, 2.1362e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 3.0884e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [3.2812e-01, 6.4062e-01, 3.1494e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.3984e-01, 2.1515e-03, 7.2632e-03,  ..., 1.8799e-02,
           0.0000e+00, 0.0000e+00],
          [9.1406e-01, 3.5706e-03, 6.4697e-03,  ..., 1.9531e-02,
           1.7944e-02, 0.0000e+00],
          [9.4141e-01, 1.7624e-03, 3.8719e-04,  ..., 3.1433e-03,
           1.5137e-02, 3.6621e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.1658e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.6113e-02, 4.6082e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6094e-01, 1.1597e-03, 2.1973e-03,  ..., 1.8311e-02,
           0.0000e+00, 0.0000e+00],
          [8.7500e-01, 1.1826e-03, 7.5073e-03,  ..., 7.2266e-02,
           3.8574e-02, 0.0000e+00],
          [9.2969e-01, 8.3542e-04, 1.1978e-03,  ..., 3.6774e-03,
           1.8433e-02, 4.4678e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[24]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.1797e-01, 8.0566e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 1.7212e-02, 1.3489e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.5312e-01, 1.0498e-02, 5.5847e-03,  ..., 1.5625e-02,
           0.0000e+00, 0.0000e+00],
          [9.2188e-01, 2.8198e-02, 1.0254e-02,  ..., 1.5747e-02,
           8.9111e-03, 0.0000e+00],
          [8.9453e-01, 3.0518e-02, 8.1177e-03,  ..., 1.1719e-02,
           6.9580e-03, 3.3447e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.5991e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 1.4282e-02, 3.9062e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6875e-01, 3.4943e-03, 3.6621e-03,  ..., 1.6724e-02,
           0.0000e+00, 0.0000e+00],
          [8.7891e-01, 1.3977e-02, 2.1240e-02,  ..., 4.7852e-02,
           1.4771e-02, 0.0000e+00],
          [9.1016e-01, 1.3428e-02, 9.3384e-03,  ..., 1.9531e-02,
           7.5378e-03, 3.1494e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [6.9531e-01, 3.0664e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.6719e-01, 7.0801e-02, 6.1768e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9453e-01, 2.5024e-02, 8.6670e-03,  ..., 5.1270e-02,
           0.0000e+00, 0.0000e+00],
          [9.3750e-01, 2.2949e-02, 1.0132e-02,  ..., 1.1902e-02,
           7.9956e-03, 0.0000e+00],
          [8.8672e-01, 4.1992e-02, 1.6357e-02,  ..., 9.1553e-03,
           6.2866e-03, 2.3438e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.1016e-01, 8.8867e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.0469e-01, 6.9336e-02, 1.2598e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.5781e-01, 3.0975e-03, 5.6152e-03,  ..., 1.7578e-01,
           0.0000e+00, 0.0000e+00],
          [8.0078e-01, 7.5073e-03, 1.2634e-02,  ..., 4.8828e-02,
           7.7637e-02, 0.0000e+00],
          [8.7500e-01, 1.0254e-02, 7.3547e-03,  ..., 1.3000e-02,
           2.6001e-02, 5.6641e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.2327e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [6.5625e-01, 3.1445e-01, 2.9541e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.5703e-01, 2.4872e-03, 2.5635e-03,  ..., 2.0996e-02,
           0.0000e+00, 0.0000e+00],
          [8.9453e-01, 1.2589e-03, 8.1253e-04,  ..., 7.7637e-02,
           1.7456e-02, 0.0000e+00],
          [9.3750e-01, 3.8910e-03, 7.2098e-04,  ..., 6.6223e-03,
           1.5625e-02, 3.2471e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.9346e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.0925e-02, 3.2349e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.5703e-01, 1.1658e-02, 7.6599e-03,  ..., 5.2490e-03,
           0.0000e+00, 0.0000e+00],
          [9.1406e-01, 4.2969e-02, 1.2634e-02,  ..., 3.0060e-03,
           5.4321e-03, 0.0000e+00],
          [9.4531e-01, 2.4780e-02, 1.9989e-03,  ..., 6.7139e-04,
           1.6403e-03, 1.8555e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[25]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9727, 0.0146, 0.0110,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9609, 0.0044, 0.0055,  ..., 0.0220, 0.0000, 0.0000],
          [0.9648, 0.0064, 0.0048,  ..., 0.0117, 0.0066, 0.0000],
          [0.9727, 0.0038, 0.0020,  ..., 0.0040, 0.0020, 0.0114]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0104, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8906, 0.0146, 0.0222,  ..., 0.0413, 0.0000, 0.0000],
          [0.6758, 0.0952, 0.0598,  ..., 0.0618, 0.0481, 0.0000],
          [0.7266, 0.0623, 0.0381,  ..., 0.0293, 0.0496, 0.0684]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9961, 0.0035, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9844, 0.0125, 0.0042,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9258, 0.0145, 0.0084,  ..., 0.0378, 0.0000, 0.0000],
          [0.8984, 0.0113, 0.0122,  ..., 0.0374, 0.0197, 0.0000],
          [0.9531, 0.0065, 0.0033,  ..., 0.0052, 0.0042, 0.0190]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [1.0000, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9844, 0.0073, 0.0084,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9062, 0.0021, 0.0135,  ..., 0.0420, 0.0000, 0.0000],
          [0.5312, 0.0022, 0.0129,  ..., 0.3906, 0.0215, 0.0000],
          [0.9688, 0.0042, 0.0078,  ..., 0.0024, 0.0069, 0.0096]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9961, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9922, 0.0059, 0.0037,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9219, 0.0015, 0.0024,  ..., 0.0383, 0.0000, 0.0000],
          [0.5547, 0.0026, 0.0017,  ..., 0.3555, 0.0530, 0.0000],
          [0.9336, 0.0013, 0.0012,  ..., 0.0074, 0.0153, 0.0369]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9922, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9414, 0.0312, 0.0262,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8555, 0.0046, 0.0132,  ..., 0.0535, 0.0000, 0.0000],
          [0.4102, 0.0018, 0.0056,  ..., 0.4512, 0.0253, 0.0000],
          [0.9492, 0.0017, 0.0016,  ..., 0.0053, 0.0099, 0.0311]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

here is attentions[26]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.5156e-01, 1.4746e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.7734e-01, 8.4839e-03, 2.1582e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [5.8984e-01, 2.0905e-03, 7.9956e-03,  ..., 3.0273e-01,
           0.0000e+00, 0.0000e+00],
          [5.8594e-01, 5.5542e-03, 9.3994e-03,  ..., 5.0049e-02,
           2.5000e-01, 0.0000e+00],
          [6.6406e-01, 6.5002e-03, 1.5137e-02,  ..., 1.6235e-02,
           2.4292e-02, 2.5977e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.6172e-01, 2.3926e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [5.1172e-01, 6.9824e-02, 4.1797e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.2656e-01, 1.3809e-03, 4.9744e-03,  ..., 2.0605e-01,
           0.0000e+00, 0.0000e+00],
          [6.0938e-01, 1.2131e-03, 3.1891e-03,  ..., 7.7637e-02,
           2.3438e-01, 0.0000e+00],
          [5.1172e-01, 9.2316e-04, 3.3875e-03,  ..., 1.9775e-02,
           3.4180e-02, 4.2188e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.9165e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 8.4839e-03, 1.6846e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6094e-01, 8.1253e-04, 2.3041e-03,  ..., 1.4404e-02,
           0.0000e+00, 0.0000e+00],
          [8.6328e-01, 8.9722e-03, 2.3193e-02,  ..., 4.0527e-02,
           2.0874e-02, 0.0000e+00],
          [7.1875e-01, 9.3994e-03, 1.5869e-02,  ..., 3.7109e-02,
           4.5898e-02, 1.3379e-01]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 6.1035e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 3.8147e-03, 3.9978e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6875e-01, 2.7313e-03, 2.3346e-03,  ..., 6.1340e-03,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 1.3046e-03, 3.1891e-03,  ..., 1.9165e-02,
           4.6997e-03, 0.0000e+00],
          [9.2188e-01, 8.9111e-03, 8.6060e-03,  ..., 1.2024e-02,
           3.3722e-03, 1.7700e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.1106e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.0620e-02, 5.7678e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9844e-01, 6.9580e-03, 1.8921e-02,  ..., 2.1240e-02,
           0.0000e+00, 0.0000e+00],
          [6.0938e-01, 3.6133e-02, 1.3379e-01,  ..., 7.2266e-02,
           5.9082e-02, 0.0000e+00],
          [6.6406e-01, 2.9907e-02, 8.6426e-02,  ..., 2.8564e-02,
           4.7852e-02, 9.8633e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.7014e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 1.6708e-03, 1.8387e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8047e-01, 1.0452e-03, 1.4267e-03,  ..., 7.0190e-03,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 5.7220e-04, 1.8768e-03,  ..., 1.4954e-02,
           2.2583e-03, 0.0000e+00],
          [9.6484e-01, 3.8910e-03, 3.3264e-03,  ..., 7.5989e-03,
           1.5182e-03, 5.9204e-03]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[27]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.3560e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 7.6599e-03, 5.7678e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8438e-01, 2.5482e-03, 1.0452e-03,  ..., 6.4087e-03,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 2.7618e-03, 2.5635e-03,  ..., 5.4932e-03,
           4.4861e-03, 0.0000e+00],
          [9.8828e-01, 1.6098e-03, 1.5106e-03,  ..., 2.0905e-03,
           1.3962e-03, 2.6550e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.1106e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 5.4321e-03, 5.9814e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6094e-01, 3.4180e-03, 5.9814e-03,  ..., 7.3547e-03,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 2.8534e-03, 7.9956e-03,  ..., 3.1281e-03,
           2.5635e-03, 0.0000e+00],
          [9.6094e-01, 4.5776e-03, 9.8267e-03,  ..., 1.4420e-03,
           1.8768e-03, 5.0354e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.3611e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.0681e-02, 1.1108e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7656e-01, 3.9978e-03, 2.7466e-03,  ..., 6.7749e-03,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 2.8839e-03, 3.5858e-03,  ..., 3.6926e-03,
           5.0659e-03, 0.0000e+00],
          [9.6094e-01, 2.7008e-03, 6.3782e-03,  ..., 4.9744e-03,
           5.7068e-03, 1.0620e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.2827e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.9062e-01, 7.8125e-02, 3.2959e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [5.3906e-01, 2.5787e-03, 2.9297e-03,  ..., 9.3262e-02,
           0.0000e+00, 0.0000e+00],
          [9.1406e-01, 7.0190e-04, 7.0190e-04,  ..., 5.7617e-02,
           1.1169e-02, 0.0000e+00],
          [5.7812e-01, 4.9133e-03, 2.6398e-03,  ..., 1.2085e-02,
           2.9688e-01, 1.0400e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.3359e-01, 6.7871e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.5938e-01, 5.3955e-02, 8.4961e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.7344e-01, 1.1780e-02, 1.8799e-02,  ..., 9.8633e-02,
           0.0000e+00, 0.0000e+00],
          [7.1484e-01, 1.2024e-02, 2.0020e-02,  ..., 3.4912e-02,
           1.3965e-01, 0.0000e+00],
          [7.8516e-01, 1.1719e-02, 2.1240e-02,  ..., 1.0498e-02,
           6.1523e-02, 9.5703e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 3.1128e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 2.1484e-02, 1.8677e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.3438e-01, 3.3417e-03, 8.1787e-03,  ..., 4.9072e-02,
           0.0000e+00, 0.0000e+00],
          [8.7500e-01, 8.1787e-03, 1.2817e-02,  ..., 4.2969e-02,
           5.2002e-02, 0.0000e+00],
          [9.0234e-01, 7.3547e-03, 3.9978e-03,  ..., 1.1414e-02,
           1.8188e-02, 4.7363e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[28]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.3977e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 9.7046e-03, 3.9551e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.5781e-01, 2.9449e-03, 1.6968e-02,  ..., 7.9102e-02,
           0.0000e+00, 0.0000e+00],
          [4.8438e-01, 1.3000e-02, 5.2002e-02,  ..., 8.0078e-02,
           1.4746e-01, 0.0000e+00],
          [5.7812e-01, 2.6733e-02, 2.8809e-02,  ..., 3.2959e-02,
           8.1055e-02, 1.8457e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.4780e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.3594e-01, 9.4727e-02, 6.9336e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8438e-01, 1.9302e-03, 8.4305e-04,  ..., 1.0010e-02,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 9.7656e-04, 1.5335e-03,  ..., 9.9487e-03,
           9.7046e-03, 0.0000e+00],
          [9.6484e-01, 2.7161e-03, 1.7471e-03,  ..., 5.6458e-03,
           5.0659e-03, 1.6235e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.2451e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.1597e-03, 8.7891e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6094e-01, 6.0272e-04, 1.6098e-03,  ..., 1.9653e-02,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 4.0627e-04, 7.7057e-04,  ..., 1.2634e-02,
           4.5776e-03, 0.0000e+00],
          [9.6094e-01, 1.4420e-03, 1.7395e-03,  ..., 7.1106e-03,
           8.0566e-03, 1.5991e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.8516e-01, 2.1484e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [5.9375e-01, 5.0537e-02, 3.5547e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9062e-01, 1.0223e-03, 8.1787e-03,  ..., 8.1543e-02,
           0.0000e+00, 0.0000e+00],
          [7.3828e-01, 2.6550e-03, 1.0132e-02,  ..., 4.6143e-02,
           1.7090e-01, 0.0000e+00],
          [7.4609e-01, 5.5237e-03, 9.7046e-03,  ..., 3.8330e-02,
           1.6846e-02, 1.7969e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.3750e-01, 6.1523e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [5.5859e-01, 1.0059e-01, 3.3984e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [4.5117e-01, 5.2185e-03, 2.0264e-02,  ..., 3.0078e-01,
           0.0000e+00, 0.0000e+00],
          [2.4023e-01, 7.9346e-03, 4.6143e-02,  ..., 5.0293e-02,
           4.9219e-01, 0.0000e+00],
          [4.4922e-01, 2.3071e-02, 7.3730e-02,  ..., 2.1118e-02,
           9.3750e-02, 2.9492e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.7678e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 2.4048e-02, 1.6724e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9844e-01, 6.1340e-03, 1.1108e-02,  ..., 4.7363e-02,
           0.0000e+00, 0.0000e+00],
          [7.9297e-01, 9.1797e-02, 4.1016e-02,  ..., 1.7090e-02,
           2.8076e-02, 0.0000e+00],
          [5.0781e-01, 1.3672e-01, 4.6631e-02,  ..., 3.0884e-02,
           7.1777e-02, 1.8359e-01]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[29]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9219, 0.0767, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.2930, 0.6406, 0.0674,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9648, 0.0028, 0.0081,  ..., 0.0098, 0.0000, 0.0000],
          [0.8281, 0.0417, 0.0228,  ..., 0.0168, 0.0552, 0.0000],
          [0.8789, 0.0179, 0.0361,  ..., 0.0042, 0.0181, 0.0330]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [1.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0095, 0.0130,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9766, 0.0022, 0.0042,  ..., 0.0099, 0.0000, 0.0000],
          [0.9297, 0.0046, 0.0212,  ..., 0.0179, 0.0166, 0.0000],
          [0.9336, 0.0067, 0.0103,  ..., 0.0137, 0.0092, 0.0194]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9922, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9961, 0.0017, 0.0037,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9375, 0.0107, 0.0240,  ..., 0.0099, 0.0000, 0.0000],
          [0.9531, 0.0047, 0.0221,  ..., 0.0032, 0.0082, 0.0000],
          [0.8711, 0.0228, 0.0378,  ..., 0.0047, 0.0126, 0.0425]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9609, 0.0378, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9453, 0.0138, 0.0398,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7812, 0.0059, 0.0161,  ..., 0.0640, 0.0000, 0.0000],
          [0.8398, 0.0046, 0.0055,  ..., 0.0160, 0.0344, 0.0000],
          [0.8672, 0.0031, 0.0031,  ..., 0.0070, 0.0074, 0.0586]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.8945, 0.1040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.7070, 0.0620, 0.2295,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.3242, 0.0065, 0.0172,  ..., 0.3672, 0.0000, 0.0000],
          [0.4785, 0.0116, 0.0135,  ..., 0.0771, 0.2734, 0.0000],
          [0.1963, 0.0136, 0.0133,  ..., 0.0244, 0.0654, 0.6133]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9062, 0.0952, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9219, 0.0208, 0.0571,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.4004, 0.0084, 0.0391,  ..., 0.2168, 0.0000, 0.0000],
          [0.3145, 0.0187, 0.0752,  ..., 0.0933, 0.1455, 0.0000],
          [0.5586, 0.0182, 0.0121,  ..., 0.0620, 0.0835, 0.1572]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

here is attentions[30]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [6.8359e-01, 3.1641e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [2.3633e-01, 2.9663e-02, 7.3438e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.3828e-01, 8.1787e-03, 8.1177e-03,  ..., 1.9727e-01,
           0.0000e+00, 0.0000e+00],
          [5.6250e-01, 2.0630e-02, 1.8311e-02,  ..., 8.2031e-02,
           2.7344e-01, 0.0000e+00],
          [3.9258e-01, 1.7822e-02, 2.2095e-02,  ..., 2.4902e-02,
           3.3203e-02, 4.8047e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.2188e-01, 7.7148e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.6562e-01, 1.9287e-02, 2.1582e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [6.9922e-01, 5.4321e-03, 8.2397e-03,  ..., 1.5820e-01,
           0.0000e+00, 0.0000e+00],
          [6.4453e-01, 1.4587e-02, 6.1035e-02,  ..., 3.2715e-02,
           1.1426e-01, 0.0000e+00],
          [5.1953e-01, 1.5137e-02, 2.9419e-02,  ..., 8.7280e-03,
           8.7891e-02, 2.4902e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.2949e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 3.0823e-03, 1.2573e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.8516e-01, 3.3569e-03, 7.8125e-03,  ..., 8.7402e-02,
           0.0000e+00, 0.0000e+00],
          [5.5078e-01, 7.5073e-03, 4.1260e-02,  ..., 8.4473e-02,
           1.6309e-01, 0.0000e+00],
          [5.5078e-01, 1.5015e-02, 5.5908e-02,  ..., 3.7109e-02,
           7.1289e-02, 2.0801e-01]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.8082e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 9.6893e-04, 2.4719e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6875e-01, 4.5013e-04, 9.8419e-04,  ..., 8.6670e-03,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 5.2643e-04, 3.8910e-03,  ..., 2.3193e-02,
           6.1340e-03, 0.0000e+00],
          [9.4531e-01, 4.7913e-03, 8.0566e-03,  ..., 6.8665e-03,
           3.0060e-03, 9.5215e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.3651e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 1.8845e-03, 5.6458e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.0625e-01, 7.4005e-04, 2.1820e-03,  ..., 5.5420e-02,
           0.0000e+00, 0.0000e+00],
          [7.5781e-01, 5.2261e-04, 4.8828e-03,  ..., 1.8066e-01,
           3.3203e-02, 0.0000e+00],
          [8.6719e-01, 3.7079e-03, 8.8501e-03,  ..., 5.2246e-02,
           1.3184e-02, 1.2573e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 4.8218e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 6.5918e-03, 1.4832e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.3750e-01, 1.2207e-03, 5.5847e-03,  ..., 2.4414e-02,
           0.0000e+00, 0.0000e+00],
          [8.4766e-01, 1.4877e-03, 1.2451e-02,  ..., 9.6680e-02,
           2.2705e-02, 0.0000e+00],
          [7.9688e-01, 1.2329e-02, 2.7100e-02,  ..., 4.2236e-02,
           3.1982e-02, 3.3447e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[31]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.6094, 0.3906, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.2832, 0.0630, 0.6523,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.2256, 0.0105, 0.0297,  ..., 0.3105, 0.0000, 0.0000],
          [0.2754, 0.0184, 0.0228,  ..., 0.0454, 0.4883, 0.0000],
          [0.3301, 0.0201, 0.0199,  ..., 0.0101, 0.2412, 0.3066]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9805, 0.0210, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9570, 0.0154, 0.0288,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.2832, 0.0020, 0.0082,  ..., 0.5898, 0.0000, 0.0000],
          [0.5781, 0.0027, 0.0098,  ..., 0.1680, 0.1260, 0.0000],
          [0.5703, 0.0045, 0.0097,  ..., 0.0364, 0.1523, 0.1904]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.6016, 0.4004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.4824, 0.0859, 0.4316,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.5312, 0.0032, 0.0046,  ..., 0.3848, 0.0000, 0.0000],
          [0.2637, 0.0087, 0.0179,  ..., 0.0962, 0.5469, 0.0000],
          [0.2314, 0.0076, 0.0065,  ..., 0.0386, 0.1602, 0.5391]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0251, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0033, 0.0086,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8477, 0.0105, 0.0137,  ..., 0.0605, 0.0000, 0.0000],
          [0.7344, 0.0222, 0.0254,  ..., 0.0947, 0.0309, 0.0000],
          [0.8320, 0.0334, 0.0166,  ..., 0.0195, 0.0093, 0.0732]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9492, 0.0527, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0135, 0.0106,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7227, 0.1099, 0.0374,  ..., 0.0850, 0.0000, 0.0000],
          [0.8438, 0.0564, 0.0282,  ..., 0.0374, 0.0208, 0.0000],
          [0.8945, 0.0398, 0.0227,  ..., 0.0059, 0.0031, 0.0215]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9531, 0.0102, 0.0383,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9180, 0.0017, 0.0025,  ..., 0.0496, 0.0000, 0.0000],
          [0.8867, 0.0059, 0.0071,  ..., 0.0364, 0.0255, 0.0000],
          [0.9141, 0.0020, 0.0055,  ..., 0.0242, 0.0132, 0.0173]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Here is generated_ids: torch.Size([1, 20])
Output Text: <|begin_of_text|>What is 1+1=? - Mathematics
Let’s take an example to understand the concept
no issues, arrived at the end of program
