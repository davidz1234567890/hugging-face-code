here is input text: What is the capital of France?
Tokenized Input: ['<|begin_of_text|>', 'What', 'Ġis', 'Ġthe', 'Ġcapital', 'Ġof', 'ĠFrance', '?']
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.72it/s]
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
Hidden States Shape (Last Layer): torch.Size([1, 8, 4096])
here is hidden_states[0]: tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,
           6.3419e-05,  1.1902e-03],
         [ 2.0752e-02, -1.2894e-03,  2.8229e-03,  ...,  2.1973e-02,
           3.1128e-03,  1.0681e-02],
         [-2.6093e-03,  7.7057e-04,  2.6131e-04,  ...,  1.1902e-02,
           4.6387e-03,  9.1553e-03],
         ...,
         [ 1.2817e-03,  9.1171e-04,  2.0905e-03,  ...,  1.6251e-03,
           4.0894e-03, -4.0283e-03],
         [-4.3335e-03, -2.4414e-03,  4.0283e-03,  ...,  3.3875e-03,
          -1.2390e-02, -3.7231e-03],
         [-4.8523e-03, -1.8005e-03,  7.2937e-03,  ...,  2.3956e-03,
          -1.3657e-03, -5.4932e-03]]], dtype=torch.bfloat16,
       grad_fn=<EmbeddingBackward0>)

here is hidden_states[1]: tensor([[[ 0.0022,  0.0041, -0.0007,  ...,  0.0190, -0.0042, -0.0025],
         [ 0.0493,  0.0287,  0.0214,  ...,  0.0098,  0.0114,  0.0237],
         [ 0.0171,  0.0104,  0.0003,  ...,  0.0181, -0.0066,  0.0101],
         ...,
         [-0.0037, -0.0188,  0.0200,  ...,  0.0117,  0.0064, -0.0106],
         [-0.0023, -0.0162,  0.0007,  ..., -0.0065, -0.0166, -0.0042],
         [-0.0009, -0.0064, -0.0014,  ...,  0.0036,  0.0064, -0.0115]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[2]: tensor([[[-0.1040,  0.0718, -0.0723,  ...,  0.8398,  0.1826,  0.1167],
         [ 0.0569,  0.0366,  0.0175,  ..., -0.0042,  0.0090,  0.0352],
         [ 0.0364,  0.0144, -0.0022,  ..., -0.0194, -0.0464,  0.0190],
         ...,
         [-0.0272, -0.0427,  0.0010,  ..., -0.0161, -0.0022, -0.0244],
         [ 0.0147, -0.0212,  0.0016,  ..., -0.0162, -0.0151, -0.0046],
         [ 0.0112, -0.0104, -0.0129,  ..., -0.0143, -0.0143, -0.0051]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[3]: tensor([[[-0.1108,  0.0767, -0.0669,  ...,  0.8555,  0.1963,  0.1123],
         [ 0.0525,  0.0295,  0.0366,  ..., -0.0048,  0.0092,  0.0503],
         [ 0.0237,  0.0366, -0.0247,  ..., -0.0206, -0.0557,  0.0359],
         ...,
         [-0.0232, -0.0352, -0.0308,  ...,  0.0649, -0.0242, -0.0635],
         [ 0.0079,  0.0293,  0.0291,  ...,  0.0488,  0.0215, -0.0190],
         [ 0.0093, -0.0011, -0.0203,  ...,  0.0124, -0.0023, -0.0282]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[4]: tensor([[[-9.0332e-02,  8.9844e-02, -4.7119e-02,  ...,  9.0625e-01,
           1.9531e-01,  1.1328e-01],
         [ 6.9336e-02,  7.4219e-02,  3.5400e-02,  ..., -4.6387e-03,
           1.2207e-04,  3.9551e-02],
         [ 7.5684e-02,  3.1494e-02, -1.1963e-02,  ..., -3.1982e-02,
          -1.3672e-02,  6.4453e-02],
         ...,
         [ 4.7363e-02, -1.2500e-01, -4.1748e-02,  ...,  4.6631e-02,
          -2.0630e-02, -1.4893e-02],
         [ 6.1035e-02,  2.5635e-02,  1.3184e-02,  ...,  5.5420e-02,
           1.0254e-02, -2.1240e-02],
         [ 2.3071e-02,  9.5367e-04, -1.5015e-02,  ...,  4.1016e-02,
           2.6611e-02,  3.6621e-03]]], dtype=torch.bfloat16,
       grad_fn=<AddBackward0>)

here is hidden_states[5]: tensor([[[-0.1138,  0.0898, -0.0457,  ...,  0.8828,  0.2295,  0.1235],
         [ 0.0825,  0.1113,  0.0236,  ...,  0.0240,  0.0171,  0.0708],
         [ 0.0791,  0.1436, -0.0325,  ..., -0.0101, -0.0094,  0.1182],
         ...,
         [ 0.0112, -0.1240, -0.0613,  ...,  0.0762, -0.0254, -0.0273],
         [ 0.0244,  0.0212,  0.0056,  ...,  0.0391,  0.0012,  0.0026],
         [ 0.0094,  0.0015, -0.0295,  ...,  0.0054,  0.0200,  0.0081]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[6]: tensor([[[-0.1069,  0.0962, -0.0312,  ...,  0.8750,  0.2461,  0.1309],
         [ 0.1523,  0.0938,  0.0273,  ..., -0.0217, -0.0045,  0.1289],
         [ 0.1475,  0.2051, -0.0654,  ...,  0.0090,  0.0728,  0.1221],
         ...,
         [ 0.0674, -0.0347, -0.0776,  ...,  0.0156,  0.0422, -0.0488],
         [-0.0173,  0.1035,  0.0078,  ..., -0.0151,  0.0430, -0.0289],
         [ 0.0166,  0.0168, -0.0234,  ..., -0.0679,  0.0034, -0.0072]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[7]: tensor([[[-0.1133,  0.1157, -0.0223,  ...,  0.8320,  0.2559,  0.1270],
         [ 0.1475,  0.1177, -0.0214,  ..., -0.0500, -0.0034,  0.1660],
         [ 0.1270,  0.0894, -0.0972,  ..., -0.0068, -0.0283,  0.0613],
         ...,
         [ 0.0806,  0.0205, -0.0137,  ...,  0.0605,  0.0593, -0.0466],
         [-0.0261,  0.0723, -0.0110,  ..., -0.0535,  0.0444, -0.0376],
         [ 0.0127,  0.0649, -0.0153,  ...,  0.0190, -0.0356,  0.0854]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[8]: tensor([[[-0.0981,  0.1729, -0.0469,  ...,  0.7695,  0.2490,  0.1543],
         [ 0.2109,  0.1055, -0.0752,  ..., -0.1396, -0.1006,  0.1338],
         [ 0.1865,  0.1035, -0.0977,  ..., -0.0569, -0.0366,  0.0581],
         ...,
         [ 0.0669,  0.0562, -0.0771,  ...,  0.1484,  0.0776, -0.0220],
         [-0.0153,  0.1123, -0.0251,  ...,  0.0145,  0.0195,  0.0386],
         [ 0.0786,  0.0220, -0.0284,  ...,  0.0688,  0.0518,  0.0503]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[9]: tensor([[[-0.1064,  0.2051, -0.0322,  ...,  0.7031,  0.2578,  0.1592],
         [ 0.1992,  0.1592, -0.0869,  ..., -0.1455, -0.1152,  0.1387],
         [ 0.1992,  0.1221, -0.0195,  ..., -0.0481, -0.0649, -0.0664],
         ...,
         [ 0.0444,  0.0315, -0.0281,  ...,  0.1162,  0.0410, -0.0762],
         [-0.0410,  0.0898, -0.0366,  ...,  0.0557,  0.0178, -0.0293],
         [ 0.0776,  0.0234, -0.0312,  ...,  0.0654,  0.0962,  0.0571]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[10]: tensor([[[-9.3750e-02,  2.2949e-01, -6.1035e-02,  ...,  6.5234e-01,
           2.7734e-01,  1.8945e-01],
         [ 1.5527e-01,  1.2256e-01, -5.7129e-02,  ..., -1.3477e-01,
          -4.6875e-02,  1.7676e-01],
         [ 1.7480e-01,  4.8340e-02, -1.8677e-02,  ..., -5.4688e-02,
          -4.0039e-02, -6.0303e-02],
         ...,
         [ 8.5449e-02,  9.1309e-02, -3.6133e-02,  ...,  1.0156e-01,
           1.7188e-01, -2.4414e-04],
         [ 9.2773e-03,  5.5420e-02, -3.6133e-02,  ...,  6.6406e-02,
           2.4414e-03,  7.3242e-02],
         [ 1.0205e-01,  1.5039e-01, -4.5654e-02,  ...,  7.3242e-02,
           1.5723e-01,  1.4941e-01]]], dtype=torch.bfloat16,
       grad_fn=<AddBackward0>)

here is hidden_states[11]: tensor([[[-0.0286,  0.3125, -0.0479,  ...,  0.5312,  0.3086,  0.2070],
         [ 0.1738,  0.0449, -0.0189,  ..., -0.1484, -0.0405,  0.2070],
         [ 0.1963,  0.0066,  0.0366,  ..., -0.0723, -0.0542,  0.0352],
         ...,
         [ 0.1060,  0.0420,  0.0305,  ...,  0.0674,  0.1147, -0.0269],
         [-0.0369,  0.0137, -0.0869,  ...,  0.0288, -0.0027,  0.0918],
         [ 0.1338,  0.0381,  0.0408,  ...,  0.0757,  0.1318,  0.1699]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[12]: tensor([[[-0.0391,  0.3398, -0.0894,  ...,  0.5078,  0.3203,  0.2061],
         [ 0.1895,  0.0581,  0.0245,  ..., -0.1787, -0.0248,  0.1738],
         [ 0.1104,  0.0364, -0.0254,  ..., -0.0791, -0.0164, -0.1064],
         ...,
         [ 0.0977,  0.0198,  0.1011,  ...,  0.0630,  0.1680,  0.0330],
         [ 0.0093,  0.0483, -0.0659,  ...,  0.0669,  0.0449,  0.1094],
         [ 0.0107,  0.0083,  0.0063,  ...,  0.0698,  0.1367,  0.1133]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[13]: tensor([[[-0.0265,  0.3594, -0.0737,  ...,  0.4375,  0.3047,  0.1963],
         [ 0.1807,  0.0117,  0.0879,  ..., -0.2207, -0.0415,  0.2930],
         [ 0.1484, -0.0266, -0.0094,  ..., -0.1475, -0.0354, -0.0718],
         ...,
         [ 0.1377, -0.0457,  0.1904,  ...,  0.0483,  0.0708,  0.0239],
         [ 0.0156, -0.0322,  0.0405,  ...,  0.1270,  0.0220,  0.0435],
         [ 0.0850, -0.1016,  0.0430,  ...,  0.1118,  0.0996,  0.1167]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[14]: tensor([[[-0.0300,  0.3398, -0.1167,  ...,  0.3652,  0.2930,  0.2051],
         [ 0.0776, -0.0228,  0.0425,  ..., -0.2246, -0.0106,  0.2949],
         [ 0.0425, -0.0444,  0.0024,  ..., -0.1523,  0.0742, -0.0913],
         ...,
         [ 0.0352, -0.0024,  0.2637,  ...,  0.0503,  0.1152, -0.0571],
         [ 0.0117, -0.0737,  0.0356,  ...,  0.1680, -0.0033,  0.0317],
         [ 0.0596, -0.0010,  0.1309,  ...,  0.1270,  0.2100,  0.1963]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[15]: tensor([[[-0.0569,  0.2988, -0.1055,  ...,  0.3672,  0.3320,  0.1221],
         [ 0.0464, -0.0566,  0.0752,  ..., -0.2412,  0.0391,  0.2656],
         [ 0.0322, -0.1240,  0.0537,  ..., -0.2324,  0.0928, -0.0908],
         ...,
         [ 0.0649,  0.0688,  0.2471,  ...,  0.0630,  0.1758, -0.1279],
         [-0.0520, -0.0518,  0.0483,  ...,  0.2324, -0.0417, -0.0205],
         [ 0.1406, -0.0014,  0.0505,  ...,  0.1465,  0.1934,  0.1108]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[16]: tensor([[[-0.1104,  0.1650, -0.0396,  ...,  0.2871,  0.3711,  0.0840],
         [-0.0591, -0.0332,  0.0508,  ..., -0.2812,  0.1172,  0.3242],
         [ 0.0588, -0.1631, -0.1543,  ..., -0.2930,  0.1079,  0.0151],
         ...,
         [ 0.1660,  0.0718,  0.0908,  ...,  0.0481,  0.1064,  0.0806],
         [-0.1562, -0.1094,  0.1113,  ...,  0.1348, -0.0165,  0.0444],
         [ 0.1494, -0.0391,  0.1855,  ...,  0.0947,  0.0303,  0.1089]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[17]: tensor([[[-0.1230,  0.1641, -0.0303,  ...,  0.2734,  0.3926,  0.0688],
         [ 0.0776, -0.1455,  0.0688,  ..., -0.2012,  0.0186,  0.4043],
         [ 0.0129, -0.1865, -0.0908,  ..., -0.3281,  0.0933, -0.0232],
         ...,
         [ 0.0723, -0.0330,  0.1475,  ...,  0.1079,  0.0918, -0.0042],
         [ 0.0005, -0.2129,  0.1533,  ...,  0.1924, -0.0820,  0.0200],
         [ 0.2578, -0.0386,  0.3047,  ...,  0.1973,  0.0312,  0.0466]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[18]: tensor([[[-0.1543,  0.1250, -0.0259,  ...,  0.2637,  0.4863,  0.0469],
         [-0.0115, -0.1445,  0.1777,  ..., -0.2031, -0.0259,  0.4414],
         [ 0.0386, -0.1699, -0.0024,  ..., -0.3320,  0.1631, -0.1245],
         ...,
         [ 0.0347,  0.0835,  0.0068,  ...,  0.0427,  0.0576,  0.0938],
         [-0.1562, -0.1641,  0.0645,  ...,  0.1592, -0.0178,  0.0204],
         [ 0.1270, -0.0262,  0.3066,  ...,  0.1914, -0.0415,  0.1533]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[19]: tensor([[[-0.1729,  0.1196, -0.0119,  ...,  0.2773,  0.4238,  0.0206],
         [-0.0527, -0.2773,  0.1201,  ..., -0.2207,  0.1006,  0.5078],
         [ 0.0703, -0.2344, -0.2773,  ..., -0.2188,  0.2891, -0.1035],
         ...,
         [ 0.0029,  0.0879, -0.0835,  ...,  0.0520, -0.0093,  0.1387],
         [-0.1953, -0.1689,  0.0342,  ...,  0.2031,  0.0552,  0.0742],
         [ 0.0605, -0.0269,  0.3359,  ...,  0.1641,  0.0938,  0.1338]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[20]: tensor([[[-0.1807,  0.1250,  0.0145,  ...,  0.2832,  0.4199,  0.0723],
         [ 0.0059, -0.3984,  0.2246,  ..., -0.3809,  0.1602,  0.5625],
         [ 0.2207, -0.2344, -0.3086,  ..., -0.2832,  0.1523, -0.1108],
         ...,
         [-0.0566,  0.0688, -0.0889,  ...,  0.1553,  0.0542,  0.1602],
         [-0.1445, -0.2236,  0.0713,  ...,  0.3164,  0.1719,  0.0520],
         [ 0.2197, -0.0205,  0.3145,  ...,  0.2051,  0.2930,  0.1328]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[21]: tensor([[[-0.1738,  0.1030,  0.0120,  ...,  0.2695,  0.3711,  0.0850],
         [-0.1309, -0.4883,  0.2637,  ..., -0.4102,  0.2441,  0.6211],
         [ 0.0830, -0.4785, -0.4141,  ..., -0.2852,  0.2480, -0.0947],
         ...,
         [-0.0442,  0.0300, -0.0859,  ...,  0.1836,  0.0430,  0.2119],
         [-0.0728, -0.1328,  0.0032,  ...,  0.4102,  0.1660,  0.0596],
         [ 0.1855,  0.0645,  0.3164,  ...,  0.3203,  0.2363,  0.1709]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[22]: tensor([[[-0.1582,  0.0889,  0.0371,  ...,  0.2969,  0.3711,  0.0962],
         [-0.1699, -0.5234,  0.3125,  ..., -0.6562,  0.1279,  0.7109],
         [-0.1973, -0.4688, -0.4707,  ..., -0.2988,  0.1211, -0.2441],
         ...,
         [-0.1416, -0.0056,  0.0010,  ...,  0.2275,  0.1216,  0.1123],
         [-0.0854, -0.1162,  0.0986,  ...,  0.4492,  0.0635,  0.1338],
         [ 0.1318, -0.0022,  0.3555,  ...,  0.3242,  0.2090,  0.1187]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[23]: tensor([[[-0.1279,  0.0742,  0.0332,  ...,  0.3105,  0.3770,  0.0923],
         [-0.3438, -0.5938,  0.2930,  ..., -0.6367,  0.1689,  0.7734],
         [-0.1943, -0.3867, -0.5117,  ..., -0.4297,  0.1123, -0.2969],
         ...,
         [-0.3457,  0.0549,  0.0522,  ...,  0.2949,  0.0010,  0.0520],
         [-0.2178, -0.1099,  0.2070,  ...,  0.4199,  0.0713,  0.1660],
         [ 0.0598,  0.2715,  0.4941,  ...,  0.4570,  0.0801,  0.1523]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[24]: tensor([[[-0.1348,  0.0591, -0.0088,  ...,  0.2930,  0.3340,  0.0723],
         [-0.4141, -0.7383,  0.3027,  ..., -0.6680,  0.0659,  0.5781],
         [-0.1235, -0.4531, -0.3633,  ..., -0.4961,  0.0581, -0.3184],
         ...,
         [-0.2100,  0.1602,  0.1758,  ...,  0.0967, -0.0187,  0.3398],
         [-0.2256, -0.2090,  0.2871,  ...,  0.3477,  0.1338,  0.0273],
         [ 0.0186,  0.2773,  0.4414,  ...,  0.3906, -0.0088,  0.0581]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[25]: tensor([[[-0.1235,  0.0508, -0.0026,  ...,  0.2871,  0.3242,  0.0728],
         [-0.4609, -0.5078,  0.3184,  ..., -0.6680,  0.1240,  0.5820],
         [-0.1797, -0.5430, -0.4375,  ..., -0.4863,  0.0718, -0.3789],
         ...,
         [-0.1455, -0.0820,  0.0713,  ...,  0.0708,  0.0688,  0.4375],
         [-0.2432, -0.1846,  0.3125,  ...,  0.3262,  0.2500, -0.0050],
         [-0.0352,  0.2910,  0.5000,  ...,  0.5703,  0.0933,  0.0011]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[26]: tensor([[[-0.1270,  0.0280, -0.0356,  ...,  0.3008,  0.3086,  0.0767],
         [-0.5312, -0.5586,  0.3086,  ..., -0.6797,  0.3262,  0.6914],
         [-0.2832, -0.6680, -0.5195,  ..., -0.4785,  0.1992, -0.3359],
         ...,
         [-0.1621, -0.0654,  0.1396,  ...,  0.0107, -0.1348,  0.4336],
         [-0.3574, -0.2227,  0.3809,  ...,  0.4590,  0.3535, -0.0913],
         [-0.1758,  0.5078,  0.5117,  ...,  0.6680,  0.2236,  0.0146]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[27]: tensor([[[-0.1113,  0.0415, -0.0674,  ...,  0.2754,  0.3477,  0.0596],
         [-0.5547, -0.4590,  0.1279,  ..., -0.6875,  0.3223,  0.7539],
         [-0.2656, -0.7695, -0.5664,  ..., -0.4434,  0.3711, -0.3887],
         ...,
         [-0.1309,  0.0767,  0.1147,  ...,  0.1348,  0.0537,  0.3730],
         [-0.5039, -0.3379,  0.3359,  ...,  0.5781,  0.2773,  0.0234],
         [-0.3223,  0.3125,  0.4961,  ...,  0.7266,  0.1152,  0.1016]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[28]: tensor([[[-0.1050,  0.0532, -0.0096,  ...,  0.3066,  0.3887,  0.0898],
         [-0.7734, -0.5859,  0.0625,  ..., -0.6953,  0.1689,  0.9844],
         [-0.2871, -0.8398, -0.5508,  ..., -0.4746,  0.3008, -0.5078],
         ...,
         [ 0.0947, -0.0068,  0.0571,  ...,  0.0068,  0.0967,  0.3301],
         [-0.6016, -0.5156,  0.4551,  ...,  0.5039,  0.1953,  0.1152],
         [-0.5078,  0.3125,  0.7109,  ...,  0.7617, -0.1650,  0.0308]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[29]: tensor([[[-0.0674,  0.1104, -0.0156,  ...,  0.3164,  0.3828,  0.1367],
         [-0.5898, -0.6289,  0.0669,  ..., -0.6094,  0.1328,  1.1562],
         [-0.5078, -0.7227, -0.5430,  ..., -0.7188,  0.2266, -0.2500],
         ...,
         [-0.0967,  0.1738, -0.0181,  ..., -0.0396,  0.2363,  0.3848],
         [-0.5312, -0.6836,  0.5273,  ...,  0.4648,  0.3516,  0.0635],
         [-0.4258,  0.0195,  0.8828,  ...,  0.8281,  0.1328, -0.0449]]],
       dtype=torch.bfloat16, grad_fn=<AddBackward0>)

here is hidden_states[30]: tensor([[[ 5.2490e-02,  1.6992e-01,  9.8145e-02,  ...,  2.4609e-01,
           2.3438e-01,  1.0840e-01],
         [-9.4531e-01, -3.5742e-01, -1.0449e-01,  ..., -5.4297e-01,
          -5.3223e-02,  1.0469e+00],
         [-6.4062e-01, -7.9688e-01, -6.9531e-01,  ..., -7.3438e-01,
           9.7656e-04, -1.2500e-01],
         ...,
         [ 1.2402e-01,  0.0000e+00, -1.1816e-01,  ...,  6.3477e-02,
           6.2500e-02,  2.6758e-01],
         [-4.5703e-01, -1.0000e+00,  5.0391e-01,  ...,  5.6250e-01,
           5.6250e-01,  1.3184e-01],
         [-4.9219e-01, -2.7100e-02,  9.4922e-01,  ...,  9.4531e-01,
          -2.1484e-01, -4.3945e-02]]], dtype=torch.bfloat16,
       grad_fn=<AddBackward0>)

here is hidden_states[31]: tensor([[[ 1.5820e-01,  4.1406e-01,  1.6992e-01,  ...,  2.8711e-01,
          -9.6680e-02,  2.7930e-01],
         [-7.8516e-01, -8.1250e-01,  9.0820e-02,  ..., -2.8516e-01,
          -9.8633e-02,  8.2812e-01],
         [-6.6016e-01, -8.0469e-01, -8.8672e-01,  ..., -1.4844e+00,
          -4.2480e-02, -5.3516e-01],
         ...,
         [ 2.9785e-02,  4.8828e-04, -2.9688e-01,  ...,  1.0059e-01,
           3.3203e-01, -1.4258e-01],
         [-4.8047e-01, -1.3125e+00,  5.3516e-01,  ...,  4.3164e-01,
           6.7578e-01, -3.0273e-01],
         [-7.1875e-01, -6.7578e-01,  8.1250e-01,  ...,  5.7422e-01,
          -2.2559e-01, -4.2188e-01]]], dtype=torch.bfloat16,
       grad_fn=<AddBackward0>)

here is hidden_states[32]: tensor([[[ 1.3203,  2.7656,  1.6484,  ..., -1.9531,  2.7656,  1.8906],
         [-0.0486, -3.2188,  0.8750,  ..., -0.8789,  0.5391,  1.8672],
         [ 0.2969,  0.2139, -3.7344,  ..., -2.7969, -0.1729,  1.3672],
         ...,
         [-1.6875,  0.0654,  0.0273,  ...,  0.4746, -0.4102, -1.1641],
         [-1.2812, -3.9062,  1.7969,  ...,  0.7344, -0.1807, -0.9258],
         [-1.0000, -2.7500,  2.9688,  ...,  3.5938, -0.2578, -1.6641]]],
       dtype=torch.bfloat16, grad_fn=<MulBackward0>)

Attention Shape (First Layer): torch.Size([1, 32, 8, 8])
here is attentions[0]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 3.9551e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.4375e-01, 1.4258e-01, 1.3672e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [3.0078e-01, 2.4121e-01, 1.8848e-01,  ..., 2.3193e-02,
           0.0000e+00, 0.0000e+00],
          [4.1406e-01, 4.6387e-02, 6.3477e-02,  ..., 9.2285e-02,
           5.9570e-02, 0.0000e+00],
          [3.1055e-01, 1.2988e-01, 1.0400e-01,  ..., 1.0742e-01,
           4.9316e-02, 1.3379e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.4048e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [6.2500e-02, 8.9062e-01, 4.7119e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.9102e-02, 9.8228e-05, 2.4872e-03,  ..., 1.7773e-01,
           0.0000e+00, 0.0000e+00],
          [8.7500e-01, 5.7983e-03, 7.0190e-03,  ..., 3.0884e-02,
           3.1982e-02, 0.0000e+00],
          [6.6406e-01, 1.2024e-02, 1.6602e-02,  ..., 4.5166e-02,
           7.7148e-02, 1.3477e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4141e-01, 5.8350e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.4551e-01, 6.7188e-01, 1.8164e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [2.5635e-02, 4.8523e-03, 4.7119e-02,  ..., 1.0791e-01,
           0.0000e+00, 0.0000e+00],
          [2.2339e-02, 2.4557e-05, 1.9932e-04,  ..., 5.9375e-01,
           1.8750e-01, 0.0000e+00],
          [6.8848e-02, 1.0824e-04, 7.3433e-05,  ..., 7.6660e-02,
           2.8516e-01, 5.6641e-01]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 6.8188e-05, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.2398e-04, 3.3617e-05,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.9219e-01, 8.6427e-06, 4.6349e-04,  ..., 5.3644e-05,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 4.6253e-05, 5.1498e-04,  ..., 1.2131e-03,
           1.8477e-05, 0.0000e+00],
          [9.8047e-01, 1.3173e-05, 8.2016e-05,  ..., 9.3460e-05,
           5.7220e-05, 2.2769e-05]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.3359e-01, 6.5430e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.8281e-01, 5.7068e-03, 1.1230e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9453e-01, 3.5248e-03, 7.5378e-03,  ..., 5.8105e-02,
           0.0000e+00, 0.0000e+00],
          [9.0234e-01, 2.9449e-03, 1.2390e-02,  ..., 1.6113e-02,
           3.6133e-02, 0.0000e+00],
          [9.2969e-01, 6.4850e-04, 1.6785e-03,  ..., 1.8158e-03,
           6.0654e-04, 5.6641e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 2.6716e-11, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.2932e-12, 4.4529e-09,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [1.0000e+00, 5.7980e-12, 7.9162e-09,  ..., 2.0955e-09,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 3.7517e-12, 1.9354e-09,  ..., 3.8708e-09,
           2.2471e-13, 0.0000e+00],
          [1.0000e+00, 3.2287e-11, 1.0303e-08,  ..., 1.5061e-09,
           5.4001e-13, 3.1479e-07]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[1]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9805, 0.0181, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9336, 0.0214, 0.0461,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8008, 0.0211, 0.1021,  ..., 0.0231, 0.0000, 0.0000],
          [0.5078, 0.0205, 0.0251,  ..., 0.2295, 0.0210, 0.0000],
          [0.6914, 0.0087, 0.0486,  ..., 0.1094, 0.0039, 0.0298]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0221, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.8828, 0.0742, 0.0437,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7656, 0.0136, 0.0527,  ..., 0.0339, 0.0000, 0.0000],
          [0.8672, 0.0038, 0.0062,  ..., 0.0593, 0.0149, 0.0000],
          [0.9062, 0.0035, 0.0124,  ..., 0.0237, 0.0034, 0.0292]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9805, 0.0178, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9492, 0.0203, 0.0305,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9297, 0.0079, 0.0116,  ..., 0.0200, 0.0000, 0.0000],
          [0.9219, 0.0028, 0.0027,  ..., 0.0082, 0.0520, 0.0000],
          [0.8359, 0.0084, 0.0144,  ..., 0.0125, 0.0164, 0.0815]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9805, 0.0098, 0.0111,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8633, 0.0330, 0.0403,  ..., 0.0208, 0.0000, 0.0000],
          [0.7891, 0.0302, 0.0613,  ..., 0.0559, 0.0132, 0.0000],
          [0.8867, 0.0102, 0.0132,  ..., 0.0081, 0.0045, 0.0398]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0128, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9297, 0.0508, 0.0190,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7031, 0.0118, 0.0383,  ..., 0.0337, 0.0000, 0.0000],
          [0.8359, 0.0041, 0.0055,  ..., 0.0400, 0.0244, 0.0000],
          [0.6172, 0.0223, 0.0254,  ..., 0.0708, 0.1064, 0.0559]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9180, 0.0811, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9297, 0.0327, 0.0364,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8750, 0.0166, 0.0157,  ..., 0.0483, 0.0000, 0.0000],
          [0.8672, 0.0043, 0.0044,  ..., 0.0165, 0.0762, 0.0000],
          [0.7812, 0.0251, 0.0209,  ..., 0.0270, 0.0182, 0.0869]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

here is attentions[2]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 6.5002e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.2878e-02, 2.3041e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9062e-01, 5.9082e-02, 3.8330e-02,  ..., 2.2125e-03,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 3.3722e-03, 2.2888e-03,  ..., 4.2114e-03,
           2.1515e-03, 0.0000e+00],
          [8.8281e-01, 3.9307e-02, 3.4668e-02,  ..., 4.1809e-03,
           3.8605e-03, 1.1902e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 4.6082e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 1.0529e-03, 2.2278e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8047e-01, 7.2098e-04, 1.1520e-03,  ..., 2.6703e-03,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 4.9591e-04, 9.5749e-04,  ..., 1.8463e-03,
           2.0142e-02, 0.0000e+00],
          [9.4531e-01, 8.8501e-04, 1.9302e-03,  ..., 6.8359e-03,
           3.7842e-03, 2.7222e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.2888e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 6.7749e-03, 1.8005e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.2188e-01, 1.7944e-02, 1.5869e-02,  ..., 8.7280e-03,
           0.0000e+00, 0.0000e+00],
          [8.6719e-01, 1.6632e-03, 3.5706e-03,  ..., 9.8633e-02,
           3.5706e-03, 0.0000e+00],
          [8.7109e-01, 7.0801e-03, 1.0925e-02,  ..., 5.2246e-02,
           5.4321e-03, 1.9531e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.1963e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 7.3547e-03, 7.4768e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.9219e-01, 1.4801e-03, 6.3705e-04,  ..., 2.1515e-03,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 3.6049e-04, 4.4823e-04,  ..., 1.9455e-03,
           8.1787e-03, 0.0000e+00],
          [9.0234e-01, 2.8809e-02, 1.0071e-02,  ..., 1.2207e-02,
           3.1738e-03, 1.8921e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.1057e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 1.4877e-03, 2.6855e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4922e-01, 2.3956e-03, 1.3123e-02,  ..., 5.3101e-03,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 5.1880e-04, 7.2632e-03,  ..., 1.8799e-02,
           9.6512e-04, 0.0000e+00],
          [7.6562e-01, 1.0681e-02, 7.1289e-02,  ..., 5.0049e-02,
           6.1646e-03, 1.9043e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 2.0630e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 2.1118e-02, 1.0254e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [6.6797e-01, 1.7456e-02, 1.9531e-01,  ..., 1.7822e-02,
           0.0000e+00, 0.0000e+00],
          [8.1250e-01, 4.5471e-03, 1.1414e-02,  ..., 1.1182e-01,
           1.2573e-02, 0.0000e+00],
          [7.8906e-01, 2.8931e-02, 6.6895e-02,  ..., 3.8818e-02,
           2.5635e-03, 2.8931e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[3]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9961, 0.0028, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0054, 0.0167,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9766, 0.0026, 0.0041,  ..., 0.0095, 0.0000, 0.0000],
          [0.9414, 0.0024, 0.0067,  ..., 0.0168, 0.0129, 0.0000],
          [0.9336, 0.0040, 0.0061,  ..., 0.0133, 0.0063, 0.0251]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9922, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9688, 0.0143, 0.0176,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9648, 0.0047, 0.0087,  ..., 0.0090, 0.0000, 0.0000],
          [0.9609, 0.0058, 0.0054,  ..., 0.0061, 0.0080, 0.0000],
          [0.9531, 0.0021, 0.0046,  ..., 0.0072, 0.0065, 0.0164]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9922, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9805, 0.0090, 0.0103,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9102, 0.0211, 0.0247,  ..., 0.0077, 0.0000, 0.0000],
          [0.8789, 0.0066, 0.0244,  ..., 0.0303, 0.0206, 0.0000],
          [0.8867, 0.0123, 0.0208,  ..., 0.0189, 0.0215, 0.0086]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9805, 0.0183, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9844, 0.0078, 0.0062,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9609, 0.0050, 0.0047,  ..., 0.0142, 0.0000, 0.0000],
          [0.8789, 0.0035, 0.0062,  ..., 0.0215, 0.0645, 0.0000],
          [0.7891, 0.0038, 0.0087,  ..., 0.0347, 0.0874, 0.0439]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9922, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0036, 0.0071,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9297, 0.0023, 0.0070,  ..., 0.0234, 0.0000, 0.0000],
          [0.8945, 0.0015, 0.0043,  ..., 0.0267, 0.0503, 0.0000],
          [0.4492, 0.0035, 0.0206,  ..., 0.2402, 0.1206, 0.0332]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9922, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9570, 0.0273, 0.0146,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9570, 0.0049, 0.0083,  ..., 0.0100, 0.0000, 0.0000],
          [0.9258, 0.0037, 0.0057,  ..., 0.0115, 0.0381, 0.0000],
          [0.7852, 0.0115, 0.0186,  ..., 0.0391, 0.0806, 0.0156]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

here is attentions[4]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.5073e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 4.3640e-03, 7.8735e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8047e-01, 5.6763e-03, 3.8757e-03,  ..., 5.0964e-03,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 3.7994e-03, 1.2436e-03,  ..., 2.1210e-03,
           2.0142e-02, 0.0000e+00],
          [9.6484e-01, 3.6469e-03, 3.9978e-03,  ..., 3.6469e-03,
           4.6997e-03, 1.5076e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.4114e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 5.4016e-03, 1.0620e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4141e-01, 4.6387e-03, 9.1553e-03,  ..., 1.6724e-02,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 3.9978e-03, 8.4229e-03,  ..., 1.0437e-02,
           5.7678e-03, 0.0000e+00],
          [9.3750e-01, 2.7924e-03, 3.8910e-03,  ..., 1.0742e-02,
           1.2573e-02, 2.3926e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.8381e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 7.4768e-03, 2.8198e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.5703e-01, 1.9989e-03, 1.4587e-02,  ..., 6.2866e-03,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 1.0681e-03, 3.4027e-03,  ..., 5.3406e-03,
           8.4839e-03, 0.0000e+00],
          [8.3984e-01, 3.3264e-03, 2.6245e-02,  ..., 2.5757e-02,
           5.4199e-02, 1.0925e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 6.5918e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 1.2741e-03, 5.1270e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8828e-01, 3.0136e-04, 1.7090e-03,  ..., 4.1504e-03,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 4.4823e-04, 8.3542e-04,  ..., 1.6632e-03,
           1.7700e-03, 0.0000e+00],
          [9.2578e-01, 1.8158e-03, 4.3640e-03,  ..., 6.5002e-03,
           1.3504e-03, 5.4443e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.9346e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 2.4170e-02, 9.5215e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8438e-01, 1.9989e-03, 3.2349e-03,  ..., 2.8076e-03,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 4.1771e-04, 1.6556e-03,  ..., 5.4932e-03,
           1.4572e-03, 0.0000e+00],
          [9.6484e-01, 6.5002e-03, 3.6469e-03,  ..., 2.6245e-03,
           1.0605e-03, 1.2939e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.8289e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.4893e-02, 5.1575e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4531e-01, 1.6632e-03, 4.2725e-03,  ..., 3.6316e-03,
           0.0000e+00, 0.0000e+00],
          [9.1016e-01, 5.7220e-04, 2.9449e-03,  ..., 4.6875e-02,
           5.6152e-03, 0.0000e+00],
          [7.3438e-01, 6.2256e-03, 2.6978e-02,  ..., 3.9551e-02,
           4.8340e-02, 1.2390e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[5]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.2268e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 1.5564e-02, 2.0264e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7266e-01, 8.9722e-03, 7.7820e-03,  ..., 2.7771e-03,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 4.3030e-03, 6.9580e-03,  ..., 1.1292e-02,
           3.1891e-03, 0.0000e+00],
          [9.4922e-01, 1.5869e-02, 8.4229e-03,  ..., 5.5542e-03,
           2.7618e-03, 6.1035e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.6602e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.6113e-02, 7.2327e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6875e-01, 1.3428e-02, 1.3504e-03,  ..., 3.9062e-03,
           0.0000e+00, 0.0000e+00],
          [8.8672e-01, 9.9487e-03, 4.0817e-04,  ..., 3.8330e-02,
           2.6367e-02, 0.0000e+00],
          [9.6875e-01, 1.0498e-02, 1.2283e-03,  ..., 9.8419e-04,
           3.9673e-03, 9.5215e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 6.8970e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 9.3384e-03, 5.4321e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.1797e-01, 2.8687e-02, 2.5391e-02,  ..., 5.7068e-03,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 4.9438e-03, 1.0193e-02,  ..., 1.4038e-02,
           2.5635e-03, 0.0000e+00],
          [9.1016e-01, 3.0640e-02, 1.8311e-02,  ..., 1.3611e-02,
           5.6763e-03, 6.3171e-03]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 5.9204e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 1.6357e-02, 1.1169e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.1250e-01, 1.3123e-02, 2.5146e-02,  ..., 5.9082e-02,
           0.0000e+00, 0.0000e+00],
          [5.5859e-01, 6.6528e-03, 8.4229e-03,  ..., 2.6367e-01,
           9.1553e-03, 0.0000e+00],
          [8.2812e-01, 1.8555e-02, 3.3203e-02,  ..., 4.8584e-02,
           1.1353e-02, 2.6733e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.3101e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 5.6763e-03, 9.9487e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4141e-01, 6.2561e-03, 2.3315e-02,  ..., 2.4414e-03,
           0.0000e+00, 0.0000e+00],
          [8.5938e-01, 3.9978e-03, 1.8433e-02,  ..., 4.3457e-02,
           8.4839e-03, 0.0000e+00],
          [8.9062e-01, 1.2451e-02, 2.1240e-02,  ..., 1.5076e-02,
           1.3977e-02, 1.0132e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 9.2773e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 5.0354e-03, 6.7749e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.4375e-01, 2.5177e-03, 2.8534e-03,  ..., 6.2988e-02,
           0.0000e+00, 0.0000e+00],
          [6.6406e-01, 1.2665e-03, 3.7231e-03,  ..., 2.1582e-01,
           1.6235e-02, 0.0000e+00],
          [3.9453e-01, 8.0566e-03, 1.8311e-02,  ..., 2.8906e-01,
           7.8613e-02, 4.1748e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[6]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.7373e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 5.0354e-03, 5.7068e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.3359e-01, 4.0588e-03, 1.7822e-02,  ..., 1.1475e-02,
           0.0000e+00, 0.0000e+00],
          [9.2188e-01, 1.8082e-03, 5.2490e-03,  ..., 3.2959e-02,
           1.0437e-02, 0.0000e+00],
          [7.2266e-01, 1.7456e-02, 2.8442e-02,  ..., 7.9102e-02,
           4.7119e-02, 2.0630e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.5625e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 3.2959e-02, 9.8267e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9844e-01, 3.2715e-02, 4.2969e-02,  ..., 2.5635e-03,
           0.0000e+00, 0.0000e+00],
          [7.8516e-01, 1.3489e-02, 2.2827e-02,  ..., 1.1377e-01,
           5.7068e-03, 0.0000e+00],
          [8.1250e-01, 3.5156e-02, 5.1025e-02,  ..., 1.9287e-02,
           3.3264e-03, 1.4404e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 3.7689e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 4.5166e-03, 3.6316e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4141e-01, 4.1504e-03, 3.3417e-03,  ..., 1.9043e-02,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 2.2736e-03, 1.8311e-03,  ..., 8.6670e-03,
           6.5308e-03, 0.0000e+00],
          [9.6484e-01, 3.0212e-03, 1.1826e-03,  ..., 5.3101e-03,
           4.2419e-03, 7.7209e-03]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 4.5471e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 4.6387e-03, 6.2561e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4922e-01, 3.9673e-04, 1.0147e-03,  ..., 8.2397e-03,
           0.0000e+00, 0.0000e+00],
          [8.9844e-01, 2.0409e-04, 7.9346e-04,  ..., 8.2397e-03,
           8.3496e-02, 0.0000e+00],
          [1.3965e-01, 4.0283e-03, 1.4099e-02,  ..., 3.2422e-01,
           5.8594e-02, 2.0996e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.2339e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.0071e-02, 1.5015e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.4766e-01, 1.4526e-02, 7.4707e-02,  ..., 1.3794e-02,
           0.0000e+00, 0.0000e+00],
          [6.9141e-01, 3.8605e-03, 1.4832e-02,  ..., 2.0410e-01,
           3.3112e-03, 0.0000e+00],
          [6.1328e-01, 3.2959e-02, 1.0547e-01,  ..., 1.0303e-01,
           1.1658e-02, 3.0396e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 3.9368e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 1.6632e-03, 4.6692e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.9219e-01, 4.5586e-04, 2.0885e-04,  ..., 3.8147e-03,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 6.8283e-04, 7.5150e-04,  ..., 1.9455e-03,
           3.1738e-02, 0.0000e+00],
          [7.8906e-01, 6.9809e-04, 1.6251e-03,  ..., 3.4180e-02,
           1.4062e-01, 1.9897e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[7]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [6.4062e-01, 3.5742e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [5.8594e-01, 1.2109e-01, 2.9297e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [4.8438e-01, 3.0518e-02, 6.5430e-02,  ..., 2.4121e-01,
           0.0000e+00, 0.0000e+00],
          [6.7969e-01, 3.8300e-03, 8.4839e-03,  ..., 8.2520e-02,
           1.6699e-01, 0.0000e+00],
          [4.2773e-01, 1.8433e-02, 5.0049e-02,  ..., 1.3184e-01,
           1.5198e-02, 2.3047e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.5024e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4141e-01, 4.3945e-02, 1.5442e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.9297e-01, 5.8594e-03, 1.7334e-02,  ..., 3.0396e-02,
           0.0000e+00, 0.0000e+00],
          [7.6172e-01, 6.4850e-04, 1.5106e-03,  ..., 5.9326e-02,
           1.1279e-01, 0.0000e+00],
          [3.2422e-01, 6.7139e-03, 2.0874e-02,  ..., 1.6895e-01,
           2.6367e-01, 1.3367e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 4.1992e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.7734e-01, 1.8359e-01, 3.8086e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [6.4062e-01, 6.9824e-02, 1.6504e-01,  ..., 1.9653e-02,
           0.0000e+00, 0.0000e+00],
          [5.3125e-01, 6.5918e-03, 1.4526e-02,  ..., 2.1191e-01,
           4.3213e-02, 0.0000e+00],
          [6.7188e-01, 7.4707e-02, 7.3242e-02,  ..., 5.6152e-02,
           1.7090e-02, 1.8921e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.4626e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 1.6235e-02, 1.2817e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.0625e-01, 8.9111e-03, 1.3977e-02,  ..., 2.2217e-02,
           0.0000e+00, 0.0000e+00],
          [7.6953e-01, 3.0975e-03, 3.6774e-03,  ..., 3.8086e-02,
           1.2012e-01, 0.0000e+00],
          [4.2578e-01, 6.6528e-03, 1.0010e-02,  ..., 9.1797e-02,
           2.4609e-01, 1.5747e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 4.5166e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.4587e-02, 7.6904e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.2969e-01, 1.4160e-02, 1.7456e-02,  ..., 1.4160e-02,
           0.0000e+00, 0.0000e+00],
          [8.2812e-01, 7.1716e-03, 1.0071e-02,  ..., 3.1494e-02,
           8.4961e-02, 0.0000e+00],
          [6.1719e-01, 1.9653e-02, 2.6245e-02,  ..., 3.8086e-02,
           2.2949e-01, 1.7212e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.2939e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 2.2461e-02, 2.4414e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.3359e-01, 9.5825e-03, 1.5076e-02,  ..., 1.7334e-02,
           0.0000e+00, 0.0000e+00],
          [7.5781e-01, 1.0986e-02, 8.8501e-03,  ..., 7.9102e-02,
           1.2451e-02, 0.0000e+00],
          [8.9453e-01, 1.4465e-02, 1.2573e-02,  ..., 1.9775e-02,
           2.5635e-02, 1.2024e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[8]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.3711e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 6.1646e-03, 4.8828e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.2578e-01, 9.2163e-03, 2.1210e-03,  ..., 4.7607e-02,
           0.0000e+00, 0.0000e+00],
          [8.6719e-01, 1.2451e-02, 2.0447e-03,  ..., 2.3804e-02,
           7.3730e-02, 0.0000e+00],
          [9.1797e-01, 1.3367e-02, 1.5640e-03,  ..., 1.7456e-02,
           1.0193e-02, 2.7832e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 4.0588e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.3956e-03, 7.0953e-04,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.9609e-01, 5.9509e-04, 9.0599e-06,  ..., 1.4114e-03,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 6.0272e-04, 5.1022e-05,  ..., 3.3569e-03,
           1.8188e-02, 0.0000e+00],
          [9.8828e-01, 7.4768e-04, 2.2888e-05,  ..., 1.1902e-03,
           5.5237e-03, 3.2959e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 3.7354e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 3.3936e-02, 1.6846e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.0234e-01, 1.5747e-02, 5.0354e-03,  ..., 5.2979e-02,
           0.0000e+00, 0.0000e+00],
          [8.0078e-01, 1.3977e-02, 7.3853e-03,  ..., 2.7588e-02,
           1.2402e-01, 0.0000e+00],
          [8.9844e-01, 1.4526e-02, 4.2114e-03,  ..., 2.5391e-02,
           7.5073e-03, 3.1982e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.4414e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 3.1250e-02, 1.7822e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4531e-01, 1.5503e-02, 6.1646e-03,  ..., 1.5503e-02,
           0.0000e+00, 0.0000e+00],
          [8.7500e-01, 6.4697e-03, 4.4556e-03,  ..., 1.1047e-02,
           8.7891e-02, 0.0000e+00],
          [5.5859e-01, 1.5564e-02, 2.8229e-03,  ..., 5.8899e-03,
           3.9062e-01, 1.7700e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.2578e-01, 7.2754e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.8672e-01, 3.0518e-02, 8.1055e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.8906e-01, 1.2756e-02, 7.5684e-03,  ..., 8.4473e-02,
           0.0000e+00, 0.0000e+00],
          [7.1094e-01, 7.4158e-03, 5.0354e-03,  ..., 5.3955e-02,
           1.3086e-01, 0.0000e+00],
          [4.8828e-01, 1.6724e-02, 5.4626e-03,  ..., 8.9355e-02,
           2.4512e-01, 9.8145e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 3.5156e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.9688e-01, 5.7617e-02, 1.4746e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [6.6406e-01, 1.1475e-02, 4.9133e-03,  ..., 1.5234e-01,
           0.0000e+00, 0.0000e+00],
          [7.1484e-01, 6.0120e-03, 5.3101e-03,  ..., 9.5215e-02,
           7.4707e-02, 0.0000e+00],
          [2.6367e-01, 1.1353e-02, 4.9744e-03,  ..., 2.1875e-01,
           8.1543e-02, 2.3633e-01]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[9]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.1406e-01, 8.6914e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.0234e-01, 6.2012e-02, 3.6865e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.7500e-01, 1.3000e-02, 8.0566e-03,  ..., 4.7607e-02,
           0.0000e+00, 0.0000e+00],
          [6.6797e-01, 1.2146e-02, 8.7280e-03,  ..., 6.5918e-02,
           1.3477e-01, 0.0000e+00],
          [6.0938e-01, 3.3447e-02, 8.1787e-03,  ..., 1.3574e-01,
           3.6377e-02, 1.0156e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 2.0752e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.1016e-01, 4.0771e-02, 4.9072e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.8672e-01, 2.5177e-03, 9.1934e-04,  ..., 5.8594e-02,
           0.0000e+00, 0.0000e+00],
          [5.6250e-01, 8.3618e-03, 4.4250e-03,  ..., 1.3281e-01,
           1.1768e-01, 0.0000e+00],
          [6.2891e-01, 6.8054e-03, 1.0452e-03,  ..., 9.0332e-02,
           1.0938e-01, 5.4199e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 4.8340e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 2.3682e-02, 6.1340e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.1406e-01, 1.4587e-02, 1.9379e-03,  ..., 3.4912e-02,
           0.0000e+00, 0.0000e+00],
          [5.8594e-01, 5.0354e-03, 5.2795e-03,  ..., 1.8750e-01,
           6.4941e-02, 0.0000e+00],
          [5.2344e-01, 3.1494e-02, 4.2419e-03,  ..., 2.8516e-01,
           2.2217e-02, 4.7119e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 2.5513e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 4.9561e-02, 3.6926e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.5703e-01, 1.8555e-02, 1.4343e-03,  ..., 1.3855e-02,
           0.0000e+00, 0.0000e+00],
          [7.6953e-01, 1.8188e-02, 3.7384e-03,  ..., 6.0547e-02,
           1.1182e-01, 0.0000e+00],
          [8.6328e-01, 5.0781e-02, 2.5024e-03,  ..., 2.2095e-02,
           1.5320e-02, 2.7344e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 2.6245e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 3.7598e-02, 9.8877e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.2188e-01, 1.4038e-02, 1.2390e-02,  ..., 2.2461e-02,
           0.0000e+00, 0.0000e+00],
          [7.5391e-01, 1.7822e-02, 8.7891e-03,  ..., 8.7891e-02,
           4.2480e-02, 0.0000e+00],
          [8.3594e-01, 1.8066e-02, 6.1951e-03,  ..., 4.3701e-02,
           3.2959e-02, 1.7090e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 5.0293e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 1.4709e-02, 2.2339e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6875e-01, 3.4637e-03, 6.0654e-04,  ..., 1.6968e-02,
           0.0000e+00, 0.0000e+00],
          [8.6719e-01, 7.0496e-03, 1.3275e-03,  ..., 4.4556e-03,
           1.0693e-01, 0.0000e+00],
          [5.7031e-01, 6.3477e-03, 1.9379e-03,  ..., 1.4746e-01,
           1.2256e-01, 5.0537e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[10]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.4375e-01, 1.5723e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.6328e-01, 8.8379e-02, 4.8828e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.0234e-01, 2.2736e-03, 6.3782e-03,  ..., 2.5024e-02,
           0.0000e+00, 0.0000e+00],
          [7.3828e-01, 2.5787e-03, 2.9602e-03,  ..., 1.9653e-02,
           1.9141e-01, 0.0000e+00],
          [6.3672e-01, 9.2773e-03, 6.4392e-03,  ..., 6.3477e-02,
           6.0547e-02, 1.7969e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.8555e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.7090e-02, 6.6833e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.2188e-01, 1.7776e-03, 1.8082e-03,  ..., 2.3926e-02,
           0.0000e+00, 0.0000e+00],
          [8.5547e-01, 9.2697e-04, 4.5204e-04,  ..., 7.7057e-04,
           1.3574e-01, 0.0000e+00],
          [6.6016e-01, 1.0132e-02, 9.5215e-03,  ..., 7.3730e-02,
           1.0596e-01, 2.1851e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.3245e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.2054e-03, 1.4954e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8047e-01, 5.0545e-05, 1.1730e-04,  ..., 1.0071e-02,
           0.0000e+00, 0.0000e+00],
          [9.2188e-01, 3.4523e-04, 2.6894e-04,  ..., 7.4863e-05,
           7.5684e-02, 0.0000e+00],
          [9.6094e-01, 1.7395e-03, 2.7313e-03,  ..., 8.1787e-03,
           9.7046e-03, 9.0942e-03]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 4.2725e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 2.0386e-02, 3.2471e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.3359e-01, 1.1658e-02, 1.2589e-03,  ..., 3.6133e-02,
           0.0000e+00, 0.0000e+00],
          [7.7344e-01, 1.0925e-02, 1.6861e-03,  ..., 3.6926e-03,
           1.8945e-01, 0.0000e+00],
          [8.9062e-01, 1.6602e-02, 2.0447e-03,  ..., 2.6001e-02,
           5.3406e-03, 2.2583e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.3359e-01, 6.5430e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.9453e-01, 5.6885e-02, 4.7119e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [6.7969e-01, 2.6611e-02, 2.0752e-02,  ..., 1.7480e-01,
           0.0000e+00, 0.0000e+00],
          [3.3008e-01, 6.2561e-03, 4.2419e-03,  ..., 2.2070e-01,
           1.7969e-01, 0.0000e+00],
          [5.4297e-01, 3.6133e-02, 1.1780e-02,  ..., 1.8750e-01,
           1.7822e-02, 9.8145e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.2949e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 1.9455e-03, 7.6294e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8438e-01, 1.2817e-03, 7.1526e-05,  ..., 1.3916e-02,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 6.9046e-04, 2.5868e-05,  ..., 1.4877e-04,
           4.3945e-02, 0.0000e+00],
          [9.7656e-01, 1.9455e-03, 1.6689e-04,  ..., 7.9346e-03,
           9.7656e-04, 9.3384e-03]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[11]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9688, 0.0293, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9648, 0.0317, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7891, 0.1162, 0.0356,  ..., 0.0208, 0.0000, 0.0000],
          [0.6445, 0.0361, 0.0197,  ..., 0.2168, 0.0134, 0.0000],
          [0.7695, 0.1079, 0.0101,  ..., 0.0240, 0.0098, 0.0564]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9648, 0.0337, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9844, 0.0096, 0.0052,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8086, 0.0221, 0.0221,  ..., 0.0645, 0.0000, 0.0000],
          [0.8828, 0.0085, 0.0058,  ..., 0.0251, 0.0302, 0.0000],
          [0.6914, 0.0282, 0.0184,  ..., 0.0688, 0.0649, 0.0356]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0237, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0194, 0.0054,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8594, 0.0508, 0.0189,  ..., 0.0295, 0.0000, 0.0000],
          [0.6758, 0.0535, 0.0211,  ..., 0.1357, 0.0214, 0.0000],
          [0.7578, 0.1001, 0.0173,  ..., 0.0374, 0.0164, 0.0289]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9727, 0.0186, 0.0098,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9766, 0.0092, 0.0039,  ..., 0.0038, 0.0000, 0.0000],
          [0.9336, 0.0137, 0.0069,  ..., 0.0097, 0.0229, 0.0000],
          [0.9492, 0.0116, 0.0048,  ..., 0.0068, 0.0130, 0.0070]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9922, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9688, 0.0082, 0.0228,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9062, 0.0386, 0.0272,  ..., 0.0134, 0.0000, 0.0000],
          [0.7852, 0.0332, 0.0393,  ..., 0.0250, 0.0537, 0.0000],
          [0.9180, 0.0272, 0.0105,  ..., 0.0059, 0.0038, 0.0220]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9688, 0.0308, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9219, 0.0669, 0.0129,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8516, 0.0640, 0.0276,  ..., 0.0189, 0.0000, 0.0000],
          [0.6172, 0.0103, 0.0126,  ..., 0.2734, 0.0273, 0.0000],
          [0.6250, 0.1240, 0.0825,  ..., 0.0825, 0.0081, 0.0184]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

here is attentions[12]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9023, 0.0977, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.8555, 0.1167, 0.0295,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.6523, 0.0630, 0.0258,  ..., 0.1221, 0.0000, 0.0000],
          [0.4785, 0.1035, 0.0320,  ..., 0.2129, 0.0569, 0.0000],
          [0.7109, 0.0620, 0.0112,  ..., 0.0625, 0.0444, 0.0613]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9727, 0.0282, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9648, 0.0272, 0.0075,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8984, 0.0393, 0.0072,  ..., 0.0293, 0.0000, 0.0000],
          [0.8164, 0.0630, 0.0146,  ..., 0.0405, 0.0198, 0.0000],
          [0.8633, 0.0347, 0.0047,  ..., 0.0255, 0.0189, 0.0181]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9297, 0.0688, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9414, 0.0474, 0.0106,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7578, 0.0306, 0.0167,  ..., 0.0601, 0.0000, 0.0000],
          [0.7266, 0.0149, 0.0038,  ..., 0.0698, 0.1016, 0.0000],
          [0.5117, 0.0276, 0.0157,  ..., 0.1162, 0.1943, 0.0396]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9531, 0.0471, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9219, 0.0698, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9531, 0.0121, 0.0040,  ..., 0.0157, 0.0000, 0.0000],
          [0.8984, 0.0098, 0.0035,  ..., 0.0220, 0.0469, 0.0000],
          [0.7812, 0.0183, 0.0131,  ..., 0.0635, 0.0566, 0.0211]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.7656, 0.2344, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.7031, 0.1826, 0.1138,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.6289, 0.0228, 0.0408,  ..., 0.1328, 0.0000, 0.0000],
          [0.6484, 0.0096, 0.0048,  ..., 0.0654, 0.1787, 0.0000],
          [0.5586, 0.0454, 0.0289,  ..., 0.0859, 0.0659, 0.1416]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9414, 0.0571, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9180, 0.0649, 0.0170,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8203, 0.0269, 0.0153,  ..., 0.0728, 0.0000, 0.0000],
          [0.6211, 0.0093, 0.0085,  ..., 0.0938, 0.1357, 0.0000],
          [0.4746, 0.0244, 0.0107,  ..., 0.0815, 0.2656, 0.0635]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

here is attentions[13]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9609, 0.0408, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9570, 0.0374, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8750, 0.0464, 0.0139,  ..., 0.0117, 0.0000, 0.0000],
          [0.7422, 0.0248, 0.0085,  ..., 0.0510, 0.0928, 0.0000],
          [0.7539, 0.0312, 0.0083,  ..., 0.0283, 0.1157, 0.0160]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9727, 0.0272, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9570, 0.0233, 0.0209,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9141, 0.0039, 0.0054,  ..., 0.0320, 0.0000, 0.0000],
          [0.6484, 0.0046, 0.0046,  ..., 0.0322, 0.2236, 0.0000],
          [0.7656, 0.0089, 0.0029,  ..., 0.0238, 0.1328, 0.0405]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9258, 0.0728, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9375, 0.0481, 0.0153,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8281, 0.0630, 0.0090,  ..., 0.0569, 0.0000, 0.0000],
          [0.6250, 0.0243, 0.0084,  ..., 0.0420, 0.2559, 0.0000],
          [0.8281, 0.0437, 0.0074,  ..., 0.0520, 0.0312, 0.0226]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.8320, 0.1670, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.8203, 0.1484, 0.0312,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7930, 0.0179, 0.0148,  ..., 0.0586, 0.0000, 0.0000],
          [0.7031, 0.0077, 0.0079,  ..., 0.0923, 0.1221, 0.0000],
          [0.5312, 0.0435, 0.0280,  ..., 0.1699, 0.1035, 0.0398]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9531, 0.0474, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9688, 0.0197, 0.0117,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9023, 0.0071, 0.0025,  ..., 0.0304, 0.0000, 0.0000],
          [0.8594, 0.0069, 0.0018,  ..., 0.0101, 0.0898, 0.0000],
          [0.6211, 0.0089, 0.0058,  ..., 0.0449, 0.2139, 0.0388]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9219, 0.0781, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9219, 0.0530, 0.0254,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.7383, 0.0493, 0.0239,  ..., 0.0977, 0.0000, 0.0000],
          [0.5000, 0.0801, 0.0342,  ..., 0.0825, 0.2090, 0.0000],
          [0.5820, 0.0825, 0.0259,  ..., 0.0684, 0.0923, 0.0835]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

here is attentions[14]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.0254e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.2207e-02, 3.2806e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.3984e-01, 1.6846e-02, 1.4954e-02,  ..., 4.9561e-02,
           0.0000e+00, 0.0000e+00],
          [8.1250e-01, 4.6082e-03, 2.7466e-03,  ..., 1.9653e-02,
           1.2061e-01, 0.0000e+00],
          [7.8125e-01, 1.3062e-02, 3.2501e-03,  ..., 1.9775e-02,
           1.3867e-01, 1.9775e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 9.3384e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 9.5215e-03, 3.1433e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.3359e-01, 1.1719e-02, 3.0670e-03,  ..., 1.6357e-02,
           0.0000e+00, 0.0000e+00],
          [8.0078e-01, 5.4016e-03, 1.8311e-03,  ..., 1.6357e-02,
           1.3281e-01, 0.0000e+00],
          [6.7969e-01, 7.8735e-03, 3.5553e-03,  ..., 2.0874e-02,
           2.3047e-01, 2.6123e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.3245e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 7.3853e-03, 2.7313e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.3359e-01, 1.7334e-02, 5.0659e-03,  ..., 1.3672e-02,
           0.0000e+00, 0.0000e+00],
          [8.0078e-01, 7.2632e-03, 3.7079e-03,  ..., 1.4465e-02,
           1.3770e-01, 0.0000e+00],
          [6.8750e-01, 7.8125e-03, 2.8229e-03,  ..., 1.2817e-02,
           2.5195e-01, 1.1841e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.3359e-01, 6.6406e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.0234e-01, 6.4453e-02, 3.2471e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.0859e-01, 1.7578e-02, 4.7302e-03,  ..., 1.1328e-01,
           0.0000e+00, 0.0000e+00],
          [5.7422e-01, 1.8188e-02, 8.1787e-03,  ..., 1.4258e-01,
           1.4355e-01, 0.0000e+00],
          [6.7969e-01, 4.5654e-02, 3.9673e-03,  ..., 1.1865e-01,
           5.5908e-02, 4.2969e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 2.6978e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 3.2471e-02, 3.8147e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.5703e-01, 1.6113e-02, 1.5793e-03,  ..., 6.1646e-03,
           0.0000e+00, 0.0000e+00],
          [8.5156e-01, 1.8066e-02, 3.2654e-03,  ..., 1.2634e-02,
           7.7637e-02, 0.0000e+00],
          [8.4766e-01, 2.6611e-02, 3.3569e-03,  ..., 1.8066e-02,
           6.5430e-02, 1.0376e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.2461e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 8.9722e-03, 1.6327e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.5703e-01, 5.2490e-03, 1.2894e-03,  ..., 1.7944e-02,
           0.0000e+00, 0.0000e+00],
          [8.9062e-01, 4.8218e-03, 8.5068e-04,  ..., 6.5002e-03,
           7.3730e-02, 0.0000e+00],
          [7.4219e-01, 7.0496e-03, 1.6479e-03,  ..., 4.7607e-02,
           1.4355e-01, 1.8433e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[15]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 4.2969e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.7500e-01, 8.3984e-02, 4.0771e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [6.0156e-01, 2.2705e-02, 2.3682e-02,  ..., 1.2891e-01,
           0.0000e+00, 0.0000e+00],
          [5.5078e-01, 1.4648e-02, 2.0020e-02,  ..., 1.6211e-01,
           8.9844e-02, 0.0000e+00],
          [3.7891e-01, 3.4668e-02, 5.8838e-02,  ..., 2.0898e-01,
           6.2500e-02, 5.8105e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 9.8419e-04, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 3.6812e-04, 7.5531e-04,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [1.0000e+00, 2.1100e-05, 4.5598e-06,  ..., 7.8964e-04,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 5.3406e-05, 4.7207e-05,  ..., 9.9182e-04,
           7.1716e-03, 0.0000e+00],
          [9.9609e-01, 3.0327e-04, 2.1553e-04,  ..., 4.1580e-04,
           5.4932e-04, 2.2430e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 6.1035e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 1.1063e-03, 5.7373e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8047e-01, 3.9673e-04, 1.1826e-03,  ..., 1.0681e-02,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 9.6130e-04, 3.4180e-03,  ..., 7.9346e-03,
           2.7222e-02, 0.0000e+00],
          [9.8047e-01, 1.1673e-03, 4.3335e-03,  ..., 2.2430e-03,
           1.5717e-03, 5.7068e-03]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.5991e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.2573e-02, 9.8267e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8438e-01, 1.5717e-03, 4.0894e-03,  ..., 5.2490e-03,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 2.5787e-03, 3.0212e-03,  ..., 4.3945e-03,
           1.8188e-02, 0.0000e+00],
          [8.7109e-01, 6.1951e-03, 1.5747e-02,  ..., 1.2512e-02,
           2.1240e-02, 2.4902e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.8387e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 3.4027e-03, 5.9814e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.9609e-01, 1.8406e-04, 1.9222e-06,  ..., 4.6082e-03,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.2793e-04, 1.9550e-04,  ..., 3.8910e-04,
           9.6130e-04, 0.0000e+00],
          [9.9219e-01, 2.4719e-03, 9.2983e-05,  ..., 1.0538e-04,
           1.0681e-03, 3.2806e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.9844e-01, 1.0156e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.8906e-01, 1.5137e-01, 6.0303e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [5.6250e-01, 1.2695e-01, 1.4062e-01,  ..., 2.1362e-02,
           0.0000e+00, 0.0000e+00],
          [5.2734e-01, 1.0986e-01, 8.9844e-02,  ..., 8.8867e-02,
           3.0762e-02, 0.0000e+00],
          [5.8984e-01, 7.5684e-02, 8.0078e-02,  ..., 6.3477e-02,
           1.5625e-02, 9.9121e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[16]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.0193e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 2.0874e-02, 5.9814e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.9219e-01, 2.0447e-03, 6.8665e-04,  ..., 1.4572e-03,
           0.0000e+00, 0.0000e+00],
          [9.3359e-01, 9.0942e-03, 6.1646e-03,  ..., 1.0437e-02,
           2.1851e-02, 0.0000e+00],
          [9.4922e-01, 1.0254e-02, 2.3956e-03,  ..., 3.0670e-03,
           1.8555e-02, 6.4087e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.9455e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 5.9204e-03, 4.0894e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.9609e-01, 8.7738e-04, 2.5177e-04,  ..., 1.3199e-03,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 5.3101e-03, 3.4332e-03,  ..., 9.4604e-03,
           2.0752e-02, 0.0000e+00],
          [9.1797e-01, 2.8809e-02, 2.1210e-03,  ..., 5.0049e-03,
           1.6357e-02, 1.1108e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.3547e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 1.3245e-02, 4.2725e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6094e-01, 2.5024e-03, 2.1973e-03,  ..., 1.0864e-02,
           0.0000e+00, 0.0000e+00],
          [8.9453e-01, 7.5989e-03, 7.3853e-03,  ..., 1.7456e-02,
           4.7363e-02, 0.0000e+00],
          [7.9297e-01, 5.5420e-02, 1.1475e-02,  ..., 1.4343e-02,
           3.8086e-02, 4.7363e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.9062e-01, 1.0938e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.0312e-01, 2.5586e-01, 3.9795e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [5.9375e-01, 1.9531e-02, 1.9653e-02,  ..., 5.7373e-02,
           0.0000e+00, 0.0000e+00],
          [4.4336e-01, 3.0151e-02, 1.4221e-02,  ..., 1.6016e-01,
           2.0312e-01, 0.0000e+00],
          [6.7188e-01, 3.2715e-02, 6.1035e-03,  ..., 2.0386e-02,
           1.3184e-01, 9.8145e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [5.0781e-01, 4.9023e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [5.0391e-01, 6.3965e-02, 4.3164e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [3.8477e-01, 5.1575e-03, 2.3956e-03,  ..., 5.5469e-01,
           0.0000e+00, 0.0000e+00],
          [3.0273e-01, 1.6479e-02, 5.0049e-03,  ..., 2.8320e-02,
           6.0156e-01, 0.0000e+00],
          [4.1406e-01, 1.0254e-02, 6.6528e-03,  ..., 9.0942e-03,
           7.7637e-02, 4.6875e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.8281e-01, 1.1621e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.3203e-01, 9.7656e-02, 6.9336e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.0312e-01, 5.1025e-02, 2.4658e-02,  ..., 1.0156e-01,
           0.0000e+00, 0.0000e+00],
          [2.9297e-01, 6.6895e-02, 3.5889e-02,  ..., 2.5586e-01,
           1.6211e-01, 0.0000e+00],
          [5.3906e-01, 9.3750e-02, 2.0630e-02,  ..., 5.3223e-02,
           1.1133e-01, 8.6426e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[17]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 9.2773e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.0071e-02, 8.7280e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7266e-01, 5.0964e-03, 9.2697e-04,  ..., 9.9487e-03,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 6.7520e-04, 1.0395e-04,  ..., 6.9885e-03,
           3.0151e-02, 0.0000e+00],
          [9.2969e-01, 7.2479e-04, 5.2452e-05,  ..., 3.8605e-03,
           4.4678e-02, 1.0742e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.9897e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 1.4404e-02, 1.5747e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9062e-01, 4.0283e-02, 9.2773e-03,  ..., 2.8809e-02,
           0.0000e+00, 0.0000e+00],
          [9.3750e-01, 7.3853e-03, 2.4719e-03,  ..., 7.9956e-03,
           3.2227e-02, 0.0000e+00],
          [9.0234e-01, 7.8125e-03, 1.8845e-03,  ..., 7.4463e-03,
           5.2734e-02, 1.5198e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 2.7832e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.0078e-01, 1.1133e-01, 8.6914e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.3438e-01, 6.9336e-02, 2.1118e-02,  ..., 9.8633e-02,
           0.0000e+00, 0.0000e+00],
          [8.7500e-01, 1.0986e-02, 2.4872e-03,  ..., 4.2725e-02,
           3.1006e-02, 0.0000e+00],
          [6.7969e-01, 1.4709e-02, 3.6774e-03,  ..., 6.2988e-02,
           8.3008e-02, 8.8379e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.6245e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 6.7444e-03, 8.1177e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8438e-01, 3.0518e-04, 3.4714e-04,  ..., 4.0894e-03,
           0.0000e+00, 0.0000e+00],
          [8.6719e-01, 2.6703e-03, 2.5940e-03,  ..., 3.0762e-02,
           5.7373e-02, 0.0000e+00],
          [9.2188e-01, 1.2436e-03, 1.5717e-03,  ..., 6.6528e-03,
           2.2949e-02, 7.1411e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.2634e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 2.5513e-02, 2.5269e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.1094e-01, 2.1484e-02, 5.6396e-02,  ..., 1.1865e-01,
           0.0000e+00, 0.0000e+00],
          [5.3906e-01, 2.6978e-02, 1.8262e-01,  ..., 9.6680e-02,
           4.2236e-02, 0.0000e+00],
          [8.8281e-01, 9.0332e-03, 2.9541e-02,  ..., 2.1851e-02,
           1.4832e-02, 1.4038e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.2451e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 1.8433e-02, 1.4771e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4531e-01, 1.6357e-02, 8.2397e-03,  ..., 8.1177e-03,
           0.0000e+00, 0.0000e+00],
          [8.0078e-01, 2.8687e-02, 3.7354e-02,  ..., 4.4434e-02,
           4.3213e-02, 0.0000e+00],
          [7.4609e-01, 3.6133e-02, 3.9062e-02,  ..., 1.1658e-02,
           2.4170e-02, 1.0840e-01]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[18]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.1536e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.7090e-02, 5.2185e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.1641e-01, 3.2806e-03, 6.7444e-03,  ..., 8.1055e-02,
           0.0000e+00, 0.0000e+00],
          [6.6797e-01, 2.4719e-03, 5.6763e-03,  ..., 1.1768e-01,
           7.2754e-02, 0.0000e+00],
          [8.0078e-01, 2.5024e-03, 2.3956e-03,  ..., 2.6611e-02,
           6.4453e-02, 7.2754e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.1016e-01, 9.0332e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.5547e-01, 3.0762e-02, 1.1328e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.0234e-01, 1.2268e-02, 1.0742e-02,  ..., 4.0771e-02,
           0.0000e+00, 0.0000e+00],
          [9.2969e-01, 1.2573e-02, 1.0498e-02,  ..., 1.3611e-02,
           1.0498e-02, 0.0000e+00],
          [9.4141e-01, 1.1414e-02, 4.4250e-03,  ..., 5.4932e-03,
           6.5308e-03, 2.2339e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 2.6123e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.2812e-01, 7.8125e-02, 9.5703e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.9688e-01, 5.3711e-03, 7.8125e-03,  ..., 9.1309e-02,
           0.0000e+00, 0.0000e+00],
          [9.1797e-01, 3.8605e-03, 4.8828e-03,  ..., 2.8320e-02,
           2.3804e-02, 0.0000e+00],
          [8.3594e-01, 3.3722e-03, 3.8757e-03,  ..., 3.7354e-02,
           4.9805e-02, 4.0771e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.4780e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 1.2756e-02, 2.1362e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6484e-01, 3.7689e-03, 3.7689e-03,  ..., 1.5137e-02,
           0.0000e+00, 0.0000e+00],
          [7.7344e-01, 7.9956e-03, 8.9111e-03,  ..., 4.2480e-02,
           8.7402e-02, 0.0000e+00],
          [6.0156e-01, 1.3611e-02, 1.0254e-02,  ..., 2.9907e-02,
           1.2598e-01, 4.7119e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.5482e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 5.4321e-03, 7.8735e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.9219e-01, 2.4567e-03, 1.7700e-03,  ..., 2.7771e-03,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 2.9297e-03, 3.4180e-03,  ..., 4.0588e-03,
           3.6865e-02, 0.0000e+00],
          [9.1016e-01, 5.0964e-03, 3.1891e-03,  ..., 3.7231e-03,
           4.3701e-02, 2.6855e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 3.9368e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 3.6316e-03, 3.7537e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.9609e-01, 9.9945e-04, 6.6376e-04,  ..., 6.8665e-04,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 2.1057e-03, 2.3804e-03,  ..., 5.7068e-03,
           1.9897e-02, 0.0000e+00],
          [9.3750e-01, 1.3580e-03, 1.5411e-03,  ..., 5.4626e-03,
           3.9551e-02, 3.9368e-03]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[19]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.7618e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 8.4229e-03, 1.5747e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.9609e-01, 2.9564e-04, 1.0300e-03,  ..., 8.5449e-04,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 4.1809e-03, 3.3569e-03,  ..., 6.6376e-04,
           1.2573e-02, 0.0000e+00],
          [9.8047e-01, 2.9755e-03, 1.1139e-03,  ..., 5.6076e-04,
           1.0376e-02, 4.1199e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.5542e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 7.2021e-03, 1.8555e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6094e-01, 6.3782e-03, 7.0190e-03,  ..., 1.0864e-02,
           0.0000e+00, 0.0000e+00],
          [9.0234e-01, 1.6724e-02, 1.2817e-02,  ..., 9.8267e-03,
           5.0293e-02, 0.0000e+00],
          [8.0078e-01, 6.0303e-02, 1.5747e-02,  ..., 9.9487e-03,
           6.0547e-02, 2.9907e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 9.8877e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 6.0120e-03, 1.5320e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8438e-01, 2.0905e-03, 6.2256e-03,  ..., 2.1210e-03,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 6.0730e-03, 6.4697e-03,  ..., 2.1973e-03,
           2.0386e-02, 0.0000e+00],
          [9.1406e-01, 2.0874e-02, 6.6528e-03,  ..., 7.7820e-03,
           2.0020e-02, 1.1719e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 4.8584e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 1.7212e-02, 1.7944e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [6.6016e-01, 2.5269e-02, 3.1128e-02,  ..., 1.4551e-01,
           0.0000e+00, 0.0000e+00],
          [8.7500e-01, 1.6357e-02, 1.0010e-02,  ..., 3.4424e-02,
           2.7222e-02, 0.0000e+00],
          [8.2031e-01, 1.9897e-02, 8.4229e-03,  ..., 3.1494e-02,
           2.6855e-02, 5.6641e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.8555e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.2578e-01, 4.7363e-02, 2.5146e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4531e-01, 9.5825e-03, 1.0376e-02,  ..., 1.1108e-02,
           0.0000e+00, 0.0000e+00],
          [8.1250e-01, 1.9897e-02, 1.8433e-02,  ..., 3.7354e-02,
           7.2754e-02, 0.0000e+00],
          [8.7109e-01, 2.0386e-02, 6.4697e-03,  ..., 7.0801e-03,
           3.9551e-02, 2.9053e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 4.8096e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.8516e-01, 5.5176e-02, 1.6016e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.4609e-01, 2.7954e-02, 2.0264e-02,  ..., 1.0303e-01,
           0.0000e+00, 0.0000e+00],
          [9.0625e-01, 3.8757e-03, 8.4839e-03,  ..., 2.9663e-02,
           3.2471e-02, 0.0000e+00],
          [8.7109e-01, 6.4697e-03, 1.2146e-02,  ..., 1.7944e-02,
           2.2217e-02, 3.2959e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[20]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.0078e-01, 2.0020e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.5000e-01, 1.1963e-01, 1.2988e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.0859e-01, 6.1951e-03, 1.5198e-02,  ..., 1.0303e-01,
           0.0000e+00, 0.0000e+00],
          [7.1875e-01, 1.1230e-02, 1.6479e-02,  ..., 4.3457e-02,
           1.7383e-01, 0.0000e+00],
          [6.7969e-01, 1.4221e-02, 1.1902e-02,  ..., 2.0996e-02,
           8.7891e-02, 1.3672e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.2583e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 2.4719e-03, 1.5137e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4141e-01, 5.1880e-04, 2.8076e-03,  ..., 4.1748e-02,
           0.0000e+00, 0.0000e+00],
          [9.0625e-01, 3.9978e-03, 5.5542e-03,  ..., 1.9531e-02,
           4.4922e-02, 0.0000e+00],
          [9.2188e-01, 3.6011e-03, 5.9204e-03,  ..., 1.2939e-02,
           1.6724e-02, 1.5381e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 9.7046e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 1.2817e-03, 3.5858e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8828e-01, 3.2616e-04, 1.9073e-03,  ..., 3.0975e-03,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 7.2861e-04, 1.4038e-03,  ..., 1.1047e-02,
           1.9775e-02, 0.0000e+00],
          [9.3359e-01, 2.3499e-03, 3.6469e-03,  ..., 4.8218e-03,
           1.9165e-02, 8.4839e-03]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.1169e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.2188e-01, 6.5918e-02, 1.3672e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.5000e-01, 2.4261e-03, 1.0071e-02,  ..., 6.5430e-02,
           0.0000e+00, 0.0000e+00],
          [5.3906e-01, 4.3030e-03, 1.0559e-02,  ..., 1.8457e-01,
           8.3496e-02, 0.0000e+00],
          [9.1797e-01, 4.9744e-03, 2.2430e-03,  ..., 6.5002e-03,
           1.6602e-02, 4.1748e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.8125e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 4.0894e-03, 7.2021e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6094e-01, 1.5182e-03, 2.4261e-03,  ..., 1.7822e-02,
           0.0000e+00, 0.0000e+00],
          [8.8672e-01, 1.2878e-02, 1.9043e-02,  ..., 1.1292e-02,
           2.2461e-02, 0.0000e+00],
          [5.7422e-01, 2.3438e-02, 6.5430e-02,  ..., 2.1118e-02,
           6.2256e-02, 6.4941e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 8.7280e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.5312e-01, 2.1973e-02, 2.3804e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.3750e-01, 2.3438e-02, 6.4697e-03,  ..., 8.7891e-03,
           0.0000e+00, 0.0000e+00],
          [9.4141e-01, 2.0020e-02, 1.1353e-02,  ..., 4.4250e-03,
           9.7656e-03, 0.0000e+00],
          [9.6094e-01, 1.1536e-02, 3.0060e-03,  ..., 1.5411e-03,
           5.7068e-03, 1.3062e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[21]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.5625e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.1230e-02, 9.0332e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8438e-01, 3.1738e-03, 1.6785e-03,  ..., 6.3171e-03,
           0.0000e+00, 0.0000e+00],
          [6.8359e-01, 1.7090e-02, 1.0107e-01,  ..., 5.4443e-02,
           7.6172e-02, 0.0000e+00],
          [9.1406e-01, 5.5237e-03, 1.1536e-02,  ..., 5.8899e-03,
           1.8555e-02, 2.7832e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 8.8501e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 8.1177e-03, 9.0332e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.5312e-01, 4.7913e-03, 2.5177e-03,  ..., 1.3000e-02,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 2.9755e-03, 1.4954e-03,  ..., 2.4261e-03,
           2.2583e-02, 0.0000e+00],
          [8.8281e-01, 1.1108e-02, 2.1210e-03,  ..., 3.4943e-03,
           2.5269e-02, 3.1738e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 2.6978e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4141e-01, 1.0010e-02, 4.8584e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.7500e-01, 3.3112e-03, 2.1362e-02,  ..., 7.4219e-02,
           0.0000e+00, 0.0000e+00],
          [9.1016e-01, 5.6763e-03, 6.1340e-03,  ..., 3.5553e-03,
           6.7871e-02, 0.0000e+00],
          [8.8672e-01, 3.7384e-03, 7.6599e-03,  ..., 3.0518e-03,
           1.1353e-02, 8.3984e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.7014e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 8.0109e-04, 5.5542e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.9609e-01, 6.1417e-04, 6.7520e-04,  ..., 2.0142e-03,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.4932e-04, 1.2817e-03,  ..., 1.3809e-03,
           6.7520e-04, 0.0000e+00],
          [9.9219e-01, 1.1597e-03, 1.6937e-03,  ..., 9.3460e-04,
           4.6921e-04, 1.0910e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.3101e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 2.4261e-03, 5.2185e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7266e-01, 3.0518e-03, 1.9684e-03,  ..., 1.1230e-02,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 3.3112e-03, 3.4180e-03,  ..., 4.9744e-03,
           3.7994e-03, 0.0000e+00],
          [9.7656e-01, 2.7008e-03, 2.0142e-03,  ..., 2.7008e-03,
           7.0190e-03, 9.3460e-04]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.3193e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 5.9128e-04, 9.2163e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.9219e-01, 3.0327e-04, 6.6376e-04,  ..., 2.8381e-03,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 1.0071e-03, 8.4229e-03,  ..., 6.2561e-03,
           1.2360e-03, 0.0000e+00],
          [9.2969e-01, 4.1199e-03, 2.2583e-02,  ..., 1.1353e-02,
           2.3041e-03, 3.2043e-03]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[22]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.5177e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 1.0376e-03, 7.2327e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8828e-01, 4.3297e-04, 9.1553e-04,  ..., 3.3569e-03,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 3.8528e-04, 3.6011e-03,  ..., 5.8289e-03,
           3.0365e-03, 0.0000e+00],
          [9.7656e-01, 3.0975e-03, 5.5237e-03,  ..., 1.3580e-03,
           2.0599e-03, 2.4109e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 3.1738e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 1.3580e-03, 5.0659e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8438e-01, 1.1902e-03, 1.5030e-03,  ..., 3.8452e-03,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.2665e-03, 3.3875e-03,  ..., 1.8463e-03,
           1.3962e-03, 0.0000e+00],
          [9.5703e-01, 4.3640e-03, 5.6763e-03,  ..., 1.9989e-03,
           1.9379e-03, 1.5198e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.7700e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 9.5825e-03, 5.3406e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8047e-01, 7.7209e-03, 1.5488e-03,  ..., 4.6082e-03,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 5.7068e-03, 5.6458e-03,  ..., 3.6926e-03,
           3.4180e-03, 0.0000e+00],
          [9.5703e-01, 1.0071e-02, 6.5613e-03,  ..., 1.6022e-03,
           2.4872e-03, 1.3428e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 9.2773e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 3.0060e-03, 6.5613e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6875e-01, 9.8419e-04, 1.1520e-03,  ..., 1.8921e-02,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 1.8616e-03, 3.6926e-03,  ..., 6.2866e-03,
           1.3977e-02, 0.0000e+00],
          [8.9062e-01, 1.1536e-02, 5.6152e-03,  ..., 1.0010e-02,
           2.4658e-02, 3.6865e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 1.9531e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 5.2795e-03, 6.9885e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4531e-01, 1.1597e-03, 1.4267e-03,  ..., 3.4180e-02,
           0.0000e+00, 0.0000e+00],
          [8.3203e-01, 4.3640e-03, 4.7913e-03,  ..., 3.8330e-02,
           7.4707e-02, 0.0000e+00],
          [7.2656e-01, 1.0498e-02, 4.8218e-03,  ..., 1.4160e-02,
           2.0117e-01, 1.9043e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 4.7607e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 2.3193e-03, 3.9978e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7656e-01, 6.7139e-04, 4.9210e-04,  ..., 1.5747e-02,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.7014e-03, 1.7319e-03,  ..., 3.6011e-03,
           2.0599e-03, 0.0000e+00],
          [9.6484e-01, 3.1128e-03, 2.3499e-03,  ..., 3.4790e-03,
           2.3956e-03, 1.4282e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[23]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6484e-01, 3.5889e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 3.6621e-02, 1.7090e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.7500e-01, 2.4658e-02, 2.9907e-02,  ..., 2.2583e-02,
           0.0000e+00, 0.0000e+00],
          [9.3750e-01, 9.9487e-03, 5.9204e-03,  ..., 1.2085e-02,
           1.9775e-02, 0.0000e+00],
          [8.2812e-01, 2.7832e-02, 2.3804e-02,  ..., 1.1597e-02,
           1.9775e-02, 6.9336e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [6.8750e-01, 3.1055e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.0312e-01, 5.8105e-02, 2.3730e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.4766e-01, 4.2480e-02, 2.2339e-02,  ..., 5.0049e-02,
           0.0000e+00, 0.0000e+00],
          [8.5156e-01, 3.0640e-02, 1.3184e-02,  ..., 3.5400e-02,
           3.5400e-02, 0.0000e+00],
          [8.7109e-01, 1.3977e-02, 7.5378e-03,  ..., 2.4658e-02,
           1.4160e-02, 5.7373e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.2578e-01, 7.3242e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.3750e-01, 2.3071e-02, 4.0039e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.7734e-01, 2.2705e-02, 6.7383e-02,  ..., 6.8359e-02,
           0.0000e+00, 0.0000e+00],
          [8.7109e-01, 1.5320e-02, 1.2817e-02,  ..., 3.2959e-02,
           3.2959e-02, 0.0000e+00],
          [8.2031e-01, 2.5513e-02, 2.5513e-02,  ..., 1.7700e-02,
           2.1484e-02, 6.2988e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.2578e-01, 7.5195e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.0625e-01, 6.4941e-02, 2.6733e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4141e-01, 1.6556e-03, 1.7929e-03,  ..., 1.1108e-02,
           0.0000e+00, 0.0000e+00],
          [9.4141e-01, 9.0027e-04, 4.0054e-04,  ..., 1.8433e-02,
           2.4292e-02, 0.0000e+00],
          [9.1797e-01, 2.9602e-03, 1.5106e-03,  ..., 6.7749e-03,
           2.5146e-02, 4.1260e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 3.0884e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [3.2812e-01, 6.4062e-01, 3.1494e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4922e-01, 5.0735e-04, 1.8845e-03,  ..., 1.1047e-02,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 6.9046e-04, 4.7493e-04,  ..., 1.5381e-02,
           2.7588e-02, 0.0000e+00],
          [8.9062e-01, 4.1199e-03, 4.0054e-04,  ..., 5.6152e-03,
           3.2959e-02, 6.2988e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.1658e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.6113e-02, 4.6082e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6484e-01, 1.2207e-03, 1.7242e-03,  ..., 9.3994e-03,
           0.0000e+00, 0.0000e+00],
          [4.6289e-01, 7.5989e-03, 2.5391e-02,  ..., 1.3672e-01,
           4.2725e-02, 0.0000e+00],
          [9.1016e-01, 2.4719e-03, 7.3242e-04,  ..., 4.7607e-03,
           2.1362e-02, 5.0781e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[24]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.1797e-01, 8.0566e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 1.7212e-02, 1.3489e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4531e-01, 1.9165e-02, 1.0864e-02,  ..., 8.9722e-03,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 1.1414e-02, 5.1880e-03,  ..., 1.8234e-03,
           4.4250e-03, 0.0000e+00],
          [9.2188e-01, 9.4604e-03, 5.2185e-03,  ..., 3.4332e-03,
           1.1169e-02, 1.3550e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.5991e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 1.4282e-02, 3.9062e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4922e-01, 6.9275e-03, 8.9111e-03,  ..., 2.1484e-02,
           0.0000e+00, 0.0000e+00],
          [8.3594e-01, 2.6489e-02, 1.6724e-02,  ..., 1.7212e-02,
           1.3367e-02, 0.0000e+00],
          [7.3047e-01, 8.9722e-03, 4.9133e-03,  ..., 4.1260e-02,
           1.2305e-01, 4.6143e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [6.9531e-01, 3.0664e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.6719e-01, 7.0801e-02, 6.1768e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [5.5859e-01, 2.6978e-02, 2.7344e-02,  ..., 2.6758e-01,
           0.0000e+00, 0.0000e+00],
          [9.0625e-01, 2.2705e-02, 9.1553e-03,  ..., 5.2185e-03,
           1.5564e-02, 0.0000e+00],
          [7.8906e-01, 3.0029e-02, 1.5869e-02,  ..., 4.1016e-02,
           2.5391e-02, 2.5146e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.1016e-01, 8.8867e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.0469e-01, 6.9336e-02, 1.2598e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.8125e-01, 1.0376e-03, 4.1504e-03,  ..., 1.7383e-01,
           0.0000e+00, 0.0000e+00],
          [8.0469e-01, 1.5442e-02, 1.1780e-02,  ..., 3.6621e-02,
           8.0078e-02, 0.0000e+00],
          [8.9453e-01, 2.7618e-03, 1.6251e-03,  ..., 9.0942e-03,
           3.6865e-02, 4.1992e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.2327e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [6.5625e-01, 3.1445e-01, 2.9541e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [3.7305e-01, 1.9897e-02, 7.5684e-03,  ..., 2.3340e-01,
           0.0000e+00, 0.0000e+00],
          [9.2578e-01, 1.1520e-03, 9.3842e-04,  ..., 3.4912e-02,
           2.0630e-02, 0.0000e+00],
          [9.1406e-01, 3.9673e-03, 5.3024e-04,  ..., 7.7820e-03,
           2.3926e-02, 4.8340e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.9346e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.0925e-02, 3.2349e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6484e-01, 3.9368e-03, 1.6403e-03,  ..., 9.6436e-03,
           0.0000e+00, 0.0000e+00],
          [9.5703e-01, 5.5237e-03, 2.0599e-03,  ..., 4.6387e-03,
           2.0996e-02, 0.0000e+00],
          [9.0234e-01, 5.7983e-03, 9.6130e-04,  ..., 3.1586e-03,
           1.0742e-02, 7.3730e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[25]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.3123e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 1.4587e-02, 1.0986e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4141e-01, 6.1340e-03, 1.2329e-02,  ..., 2.3315e-02,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 9.0332e-03, 5.6458e-03,  ..., 1.0742e-02,
           1.7578e-02, 0.0000e+00],
          [9.4141e-01, 9.4604e-03, 2.9144e-03,  ..., 1.7395e-03,
           1.6327e-03, 4.0039e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.1780e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.0376e-02, 2.3651e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7656e-01, 6.7444e-03, 3.0670e-03,  ..., 4.7607e-03,
           0.0000e+00, 0.0000e+00],
          [7.4219e-01, 6.0547e-02, 2.5757e-02,  ..., 3.7842e-02,
           4.8828e-02, 0.0000e+00],
          [8.0469e-01, 3.8086e-02, 1.5991e-02,  ..., 1.4404e-02,
           3.4912e-02, 5.3711e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 3.5400e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.2451e-02, 4.1504e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8047e-01, 5.0049e-03, 3.7689e-03,  ..., 2.3651e-03,
           0.0000e+00, 0.0000e+00],
          [8.8672e-01, 8.5449e-03, 1.0010e-02,  ..., 1.5991e-02,
           5.9326e-02, 0.0000e+00],
          [9.6875e-01, 4.4250e-03, 1.9836e-03,  ..., 1.8997e-03,
           2.7618e-03, 1.8311e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.8387e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 7.2937e-03, 8.4229e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7656e-01, 1.6098e-03, 9.2163e-03,  ..., 3.7994e-03,
           0.0000e+00, 0.0000e+00],
          [7.5781e-01, 4.1771e-04, 1.6785e-03,  ..., 2.1484e-01,
           1.3306e-02, 0.0000e+00],
          [9.7266e-01, 2.4872e-03, 3.4027e-03,  ..., 5.4321e-03,
           4.1809e-03, 8.9722e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 4.1199e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 5.8899e-03, 3.6926e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7656e-01, 3.5858e-03, 3.5858e-03,  ..., 2.2125e-03,
           0.0000e+00, 0.0000e+00],
          [9.3359e-01, 8.7738e-04, 1.5640e-03,  ..., 3.9062e-02,
           1.1353e-02, 0.0000e+00],
          [9.6484e-01, 1.1444e-03, 1.7776e-03,  ..., 4.2419e-03,
           7.2327e-03, 1.0071e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.0190e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4141e-01, 3.1250e-02, 2.6245e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8438e-01, 3.0136e-04, 9.5749e-04,  ..., 6.7444e-03,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 3.1090e-04, 5.2643e-04,  ..., 3.9062e-03,
           9.1553e-03, 0.0000e+00],
          [9.8438e-01, 6.5613e-04, 4.6539e-04,  ..., 9.2316e-04,
           5.9509e-03, 8.6670e-03]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[26]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.5156e-01, 1.4746e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.7734e-01, 8.4839e-03, 2.1582e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.7109e-01, 5.9128e-04, 1.9073e-03,  ..., 1.2207e-01,
           0.0000e+00, 0.0000e+00],
          [8.3984e-01, 2.1820e-03, 4.6082e-03,  ..., 2.2888e-03,
           1.4746e-01, 0.0000e+00],
          [7.5391e-01, 1.8692e-03, 8.2397e-03,  ..., 1.2268e-02,
           3.3447e-02, 1.8555e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.6172e-01, 2.3926e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [5.1172e-01, 6.9824e-02, 4.1797e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [6.6016e-01, 3.9978e-03, 1.8555e-02,  ..., 1.9238e-01,
           0.0000e+00, 0.0000e+00],
          [6.7578e-01, 2.2430e-03, 2.9297e-03,  ..., 2.4536e-02,
           2.5391e-01, 0.0000e+00],
          [6.3672e-01, 2.1820e-03, 5.1575e-03,  ..., 1.4038e-02,
           6.2988e-02, 1.7773e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 1.9165e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7266e-01, 8.4839e-03, 1.6846e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.3750e-01, 3.6011e-03, 7.3853e-03,  ..., 4.0771e-02,
           0.0000e+00, 0.0000e+00],
          [8.1641e-01, 8.7891e-03, 1.5747e-02,  ..., 2.2095e-02,
           5.4199e-02, 0.0000e+00],
          [8.3984e-01, 5.4016e-03, 5.1575e-03,  ..., 4.9133e-03,
           5.9570e-02, 6.9824e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 6.1035e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 3.8147e-03, 3.9978e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7266e-01, 4.6730e-04, 1.8463e-03,  ..., 2.3804e-02,
           0.0000e+00, 0.0000e+00],
          [9.2578e-01, 7.6294e-03, 8.5449e-03,  ..., 3.0640e-02,
           1.4954e-02, 0.0000e+00],
          [9.4531e-01, 5.1880e-03, 4.9438e-03,  ..., 1.2451e-02,
           7.6599e-03, 1.8066e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.1106e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.0620e-02, 5.7678e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.1406e-01, 3.3569e-03, 2.4048e-02,  ..., 3.8574e-02,
           0.0000e+00, 0.0000e+00],
          [7.7344e-01, 1.4099e-02, 4.3213e-02,  ..., 4.6631e-02,
           4.3213e-02, 0.0000e+00],
          [6.7578e-01, 8.3618e-03, 1.5747e-02,  ..., 4.4922e-02,
           5.4932e-02, 1.4844e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.7014e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 1.6708e-03, 1.8387e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7266e-01, 4.3869e-04, 1.2665e-03,  ..., 2.4658e-02,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 3.0518e-03, 3.7537e-03,  ..., 1.4465e-02,
           6.1646e-03, 0.0000e+00],
          [9.4531e-01, 3.8757e-03, 2.9602e-03,  ..., 5.7983e-03,
           5.2795e-03, 2.6367e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[27]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.3560e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 7.6599e-03, 5.7678e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8828e-01, 1.7090e-03, 6.8283e-04,  ..., 3.5095e-03,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.9150e-03, 2.6245e-03,  ..., 4.6692e-03,
           6.1035e-03, 0.0000e+00],
          [9.6875e-01, 1.8692e-03, 1.6479e-03,  ..., 7.8735e-03,
           6.2256e-03, 8.4839e-03]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 7.1106e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 5.4321e-03, 5.9814e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.4141e-01, 9.6130e-04, 2.0599e-03,  ..., 2.6489e-02,
           0.0000e+00, 0.0000e+00],
          [9.2188e-01, 2.8381e-03, 6.0120e-03,  ..., 2.1729e-02,
           1.7578e-02, 0.0000e+00],
          [8.7500e-01, 2.3041e-03, 6.0730e-03,  ..., 3.3691e-02,
           4.0039e-02, 1.4160e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.3611e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.0681e-02, 1.1108e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7266e-01, 1.9989e-03, 5.4321e-03,  ..., 9.7656e-03,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 3.9368e-03, 5.6458e-03,  ..., 1.1230e-02,
           1.4954e-02, 0.0000e+00],
          [9.5312e-01, 3.0823e-03, 5.0049e-03,  ..., 1.1780e-02,
           7.0496e-03, 1.0742e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.2827e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.9062e-01, 7.8125e-02, 3.2959e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [3.3008e-01, 9.3079e-04, 2.3804e-03,  ..., 3.3203e-02,
           0.0000e+00, 0.0000e+00],
          [9.3359e-01, 1.1826e-03, 1.1444e-03,  ..., 2.1729e-02,
           2.2949e-02, 0.0000e+00],
          [6.5625e-01, 2.9907e-03, 7.7820e-04,  ..., 5.7678e-03,
           3.0469e-01, 2.8076e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.3359e-01, 6.7871e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.5938e-01, 5.3955e-02, 8.4961e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.1641e-01, 4.1504e-03, 1.1414e-02,  ..., 8.3984e-02,
           0.0000e+00, 0.0000e+00],
          [8.3984e-01, 1.3977e-02, 1.1230e-02,  ..., 2.1973e-02,
           6.7871e-02, 0.0000e+00],
          [7.3047e-01, 1.0254e-02, 1.3489e-02,  ..., 3.1128e-02,
           6.0059e-02, 1.1523e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6875e-01, 3.1128e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 2.1484e-02, 1.8677e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.1797e-01, 4.3335e-03, 3.4180e-03,  ..., 1.2939e-02,
           0.0000e+00, 0.0000e+00],
          [8.1250e-01, 1.0559e-02, 5.4626e-03,  ..., 2.0996e-02,
           1.2402e-01, 0.0000e+00],
          [9.2578e-01, 4.6387e-03, 7.5531e-04,  ..., 4.6387e-03,
           3.1738e-02, 1.8066e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[28]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 1.3977e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4922e-01, 9.7046e-03, 3.9551e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.0078e-01, 1.2512e-02, 4.8096e-02,  ..., 8.4961e-02,
           0.0000e+00, 0.0000e+00],
          [6.2500e-01, 1.6479e-02, 1.3855e-02,  ..., 9.7656e-02,
           1.5625e-01, 0.0000e+00],
          [4.8438e-01, 1.9287e-02, 1.0742e-02,  ..., 5.4932e-02,
           2.2559e-01, 1.3867e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.4780e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.3594e-01, 9.4727e-02, 6.9336e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [5.7422e-01, 2.6001e-02, 1.1780e-02,  ..., 1.5234e-01,
           0.0000e+00, 0.0000e+00],
          [9.2969e-01, 2.1667e-03, 3.5095e-03,  ..., 5.9204e-03,
           2.4292e-02, 0.0000e+00],
          [9.1797e-01, 2.5024e-03, 2.5024e-03,  ..., 9.2163e-03,
           2.8442e-02, 2.4658e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.2451e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8828e-01, 1.1597e-03, 8.7891e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.8047e-01, 1.0300e-03, 2.7466e-03,  ..., 9.0942e-03,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 1.0605e-03, 2.5024e-03,  ..., 8.4839e-03,
           6.7749e-03, 0.0000e+00],
          [9.9219e-01, 2.1458e-04, 5.8365e-04,  ..., 1.5640e-03,
           1.5640e-03, 3.3569e-03]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.8516e-01, 2.1484e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [5.9375e-01, 5.0537e-02, 3.5547e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.8281e-01, 1.5793e-03, 1.5198e-02,  ..., 7.7637e-02,
           0.0000e+00, 0.0000e+00],
          [7.7734e-01, 1.1047e-02, 2.2583e-02,  ..., 1.1353e-02,
           8.3984e-02, 0.0000e+00],
          [7.5000e-01, 3.0212e-03, 8.9111e-03,  ..., 7.3547e-03,
           2.2461e-02, 1.1230e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.3750e-01, 6.1523e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [5.5859e-01, 1.0059e-01, 3.3984e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [7.5391e-01, 2.3956e-03, 1.4893e-02,  ..., 1.8750e-01,
           0.0000e+00, 0.0000e+00],
          [2.9883e-01, 8.9111e-03, 4.4189e-02,  ..., 5.1025e-02,
           3.0859e-01, 0.0000e+00],
          [5.9766e-01, 9.8877e-03, 2.9785e-02,  ..., 2.7710e-02,
           1.1865e-01, 1.8164e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 5.7678e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 2.4048e-02, 1.6724e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7266e-01, 1.0529e-03, 1.0681e-03,  ..., 5.3711e-03,
           0.0000e+00, 0.0000e+00],
          [5.5469e-01, 6.0303e-02, 3.7598e-02,  ..., 1.7578e-02,
           8.9844e-02, 0.0000e+00],
          [5.3906e-01, 5.2246e-02, 2.5513e-02,  ..., 2.1606e-02,
           2.4316e-01, 4.7607e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[29]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.2188e-01, 7.6660e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [2.9297e-01, 6.4062e-01, 6.7383e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.1016e-01, 1.0681e-02, 4.8523e-03,  ..., 1.9897e-02,
           0.0000e+00, 0.0000e+00],
          [8.1250e-01, 2.4292e-02, 1.4221e-02,  ..., 7.5989e-03,
           6.9336e-02, 0.0000e+00],
          [8.9062e-01, 1.0559e-02, 1.2024e-02,  ..., 5.5542e-03,
           2.8564e-02, 3.9307e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.4343e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 9.5215e-03, 1.3000e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.5938e-01, 1.7700e-03, 8.3542e-04,  ..., 2.7771e-03,
           0.0000e+00, 0.0000e+00],
          [8.0078e-01, 1.6235e-02, 8.6060e-03,  ..., 9.0942e-03,
           4.0039e-02, 0.0000e+00],
          [9.2969e-01, 4.8218e-03, 3.4637e-03,  ..., 3.2043e-03,
           1.3062e-02, 1.1475e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 8.0566e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 1.7471e-03, 3.7079e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.9219e-01, 1.5411e-03, 2.0447e-03,  ..., 7.1716e-04,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 9.2773e-03, 9.7046e-03,  ..., 6.7749e-03,
           6.6833e-03, 0.0000e+00],
          [8.9453e-01, 3.2959e-02, 2.8687e-02,  ..., 5.0049e-03,
           7.8735e-03, 1.5869e-02]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.6094e-01, 3.7842e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.4531e-01, 1.3794e-02, 3.9795e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.3359e-01, 1.4725e-03, 2.2736e-03,  ..., 3.8330e-02,
           0.0000e+00, 0.0000e+00],
          [9.2188e-01, 4.7607e-03, 3.4790e-03,  ..., 2.7832e-02,
           1.9531e-02, 0.0000e+00],
          [9.7266e-01, 8.8501e-04, 1.1063e-03,  ..., 6.2561e-03,
           4.0283e-03, 1.0864e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [8.9453e-01, 1.0400e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.0703e-01, 6.2012e-02, 2.2949e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.2969e-01, 1.3962e-03, 2.3804e-03,  ..., 4.8096e-02,
           0.0000e+00, 0.0000e+00],
          [5.7422e-01, 9.3994e-03, 3.6926e-03,  ..., 3.4180e-02,
           3.0664e-01, 0.0000e+00],
          [2.7734e-01, 1.5869e-02, 6.3782e-03,  ..., 1.4343e-02,
           3.2422e-01, 2.4512e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.0625e-01, 9.5215e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.2188e-01, 2.0752e-02, 5.7129e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.7891e-01, 2.3956e-03, 1.4893e-02,  ..., 9.1309e-02,
           0.0000e+00, 0.0000e+00],
          [6.8750e-01, 4.3640e-03, 5.6152e-03,  ..., 9.3262e-02,
           9.9121e-02, 0.0000e+00],
          [5.4688e-01, 4.5776e-03, 3.9062e-03,  ..., 2.3071e-02,
           1.3672e-01, 2.0898e-01]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[30]: tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [6.8359e-01, 3.1641e-01, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [2.3633e-01, 2.9663e-02, 7.3438e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [5.5078e-01, 2.2949e-02, 3.4180e-02,  ..., 2.1289e-01,
           0.0000e+00, 0.0000e+00],
          [5.5469e-01, 2.0386e-02, 1.8677e-02,  ..., 4.8584e-02,
           3.0664e-01, 0.0000e+00],
          [5.0391e-01, 1.2207e-02, 8.5449e-03,  ..., 3.5156e-02,
           1.2500e-01, 2.8906e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.2188e-01, 7.7148e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [7.6562e-01, 1.9287e-02, 2.1582e-01,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [6.9922e-01, 4.6692e-03, 9.9487e-03,  ..., 2.5977e-01,
           0.0000e+00, 0.0000e+00],
          [5.1172e-01, 1.7822e-02, 4.6387e-02,  ..., 4.1016e-02,
           1.8262e-01, 0.0000e+00],
          [6.5234e-01, 2.0874e-02, 2.7100e-02,  ..., 7.3547e-03,
           1.5234e-01, 1.0840e-01]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.7656e-01, 2.2949e-02, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8438e-01, 3.0823e-03, 1.2573e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [8.9844e-01, 3.1891e-03, 2.4902e-02,  ..., 5.9570e-02,
           0.0000e+00, 0.0000e+00],
          [6.3672e-01, 1.2329e-02, 3.0029e-02,  ..., 8.4473e-02,
           1.2598e-01, 0.0000e+00],
          [5.5859e-01, 8.6670e-03, 2.3071e-02,  ..., 4.5898e-02,
           1.4258e-01, 1.6992e-01]],

         ...,

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [1.0000e+00, 1.8082e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 9.6893e-04, 2.4719e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6875e-01, 1.4210e-04, 7.7057e-04,  ..., 2.3193e-02,
           0.0000e+00, 0.0000e+00],
          [9.2969e-01, 2.1667e-03, 5.9814e-03,  ..., 1.9409e-02,
           1.8555e-02, 0.0000e+00],
          [8.9453e-01, 4.4861e-03, 6.4087e-03,  ..., 1.2268e-02,
           2.0264e-02, 2.5513e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 2.3651e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9219e-01, 1.8845e-03, 5.6458e-03,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.7656e-01, 5.3787e-04, 2.0599e-03,  ..., 1.6724e-02,
           0.0000e+00, 0.0000e+00],
          [9.2969e-01, 3.6316e-03, 9.7656e-03,  ..., 9.0942e-03,
           3.2715e-02, 0.0000e+00],
          [8.9844e-01, 5.9814e-03, 9.0942e-03,  ..., 4.5776e-03,
           3.9307e-02, 2.5146e-02]],

         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.9609e-01, 4.8218e-03, 0.0000e+00,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          [9.8047e-01, 6.5918e-03, 1.4832e-02,  ..., 0.0000e+00,
           0.0000e+00, 0.0000e+00],
          ...,
          [9.6875e-01, 2.4414e-03, 5.2490e-03,  ..., 9.3994e-03,
           0.0000e+00, 0.0000e+00],
          [8.2422e-01, 3.9978e-03, 1.2756e-02,  ..., 1.1719e-02,
           8.6426e-02, 0.0000e+00],
          [8.3594e-01, 5.3101e-03, 7.7209e-03,  ..., 7.4768e-03,
           7.2266e-02, 3.8086e-02]]]], dtype=torch.bfloat16,
       grad_fn=<ToCopyBackward0>)

here is attentions[31]: tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.6094, 0.3906, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.2832, 0.0630, 0.6523,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.4219, 0.0060, 0.0164,  ..., 0.5000, 0.0000, 0.0000],
          [0.2158, 0.0092, 0.0186,  ..., 0.2334, 0.3398, 0.0000],
          [0.3770, 0.0095, 0.0144,  ..., 0.0334, 0.0776, 0.4551]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9805, 0.0210, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9570, 0.0154, 0.0288,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8125, 0.0033, 0.0103,  ..., 0.1357, 0.0000, 0.0000],
          [0.7812, 0.0043, 0.0107,  ..., 0.0737, 0.0830, 0.0000],
          [0.6914, 0.0042, 0.0070,  ..., 0.0175, 0.0669, 0.2070]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.6016, 0.4004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.4824, 0.0859, 0.4316,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.6797, 0.0068, 0.0073,  ..., 0.2637, 0.0000, 0.0000],
          [0.5156, 0.0082, 0.0054,  ..., 0.0493, 0.3457, 0.0000],
          [0.3105, 0.0051, 0.0068,  ..., 0.0165, 0.0762, 0.5664]],

         ...,

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0251, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0033, 0.0086,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9258, 0.0061, 0.0056,  ..., 0.0464, 0.0000, 0.0000],
          [0.8945, 0.0209, 0.0085,  ..., 0.0253, 0.0186, 0.0000],
          [0.8359, 0.0269, 0.0103,  ..., 0.0114, 0.0127, 0.0659]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9492, 0.0527, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9766, 0.0135, 0.0106,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.9258, 0.0225, 0.0021,  ..., 0.0386, 0.0000, 0.0000],
          [0.7852, 0.0879, 0.0231,  ..., 0.0168, 0.0299, 0.0000],
          [0.6797, 0.0977, 0.0201,  ..., 0.0162, 0.0420, 0.0859]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9883, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.9531, 0.0102, 0.0383,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.8438, 0.0115, 0.0124,  ..., 0.0260, 0.0000, 0.0000],
          [0.7812, 0.0092, 0.0108,  ..., 0.0166, 0.0913, 0.0000],
          [0.8555, 0.0061, 0.0080,  ..., 0.0069, 0.0356, 0.0574]]]],
       dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Here is generated_ids: torch.Size([1, 20])
Output Text: <|begin_of_text|>What is the capital of France? Paris.
What is the capital of France?
Answer: Paris
no issues, arrived at the end of program
